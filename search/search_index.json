{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"90DaysOfDevOps/day01/","title":"#90DaysOfDevOps - Introduction - Day 1","text":""},{"location":"90DaysOfDevOps/day01/#introduction-day-1","title":"Introduction - Day 1","text":"<p>Day 1 of our 90 days and adventure to learn a good foundational understanding of DevOps and tools that help with a DevOps mindset.</p> <p>This learning journey started for me a few years back, but my focus then was around virtualisation platforms and cloud-based technologies, I was looking mostly into Infrastructure as Code and Application configuration management with Terraform and Chef.</p> <p>Fast forward to March 2021, I was given an amazing opportunity to concentrate my efforts around the Cloud Native strategy at Kasten by Veeam. Which was going to be a massive focus on Kubernetes and DevOps and the community surrounding these technologies. I started my learning journey and quickly realised there was a very wide world aside from just learning the fundamentals of Kubernetes and Containerisation and it was then when I started speaking to the community and learning more and more about the DevOps culture, tooling and processes so I started documenting some of the areas I wanted to learn in public.</p> <p>So you want to learn DevOps?</p>"},{"location":"90DaysOfDevOps/day01/#let-the-journey-begin","title":"Let the journey begin","text":"<p>If you read the above blog, you will see this is a high-level contents for my learning journey and I will say at this point I am nowhere near an expert in any of these sections but what I wanted to do was share some resources both FREE and some paid for but an option for both as we all have different circumstances.</p> <p>Over the next 90 days, I want to document these resources and cover those foundational areas. I would love for the community to also get involved. Share your journey and resources so we can learn in public and help each other.</p> <p>You will see from the opening readme in the project repository that I have split things into sections and it is 12 weeks plus 6 days. For the first 6 days, we will explore the fundamentals of DevOps in general before diving into some of the specific areas. By no way is this list exhaustive and again, I would love for the community to assist in making this a useful resource.</p> <p>Another resource I will share at this point and that I think everyone should have a good look at, maybe create your mind map for yourself and your interest and position, is the following:</p> <p>DevOps Roadmap</p> <p>I found this a great resource when I was creating my initial list and blog post on this topic. You can also see other areas go into a lot more detail outside of the 12 topics I have listed here in this repository.</p>"},{"location":"90DaysOfDevOps/day01/#first-steps-what-is-devops","title":"First Steps - What is DevOps?","text":"<p>There are so many blog articles and YouTube videos to list here, but as we start the 90-day challenge and we focus on spending around an hour a day learning something new or about DevOps, I thought it was good to get some of the high level of \"what DevOps is\" down to begin.</p> <p>Firstly, DevOps is not a tool. You cannot buy it, it is not a software SKU or an open source GitHub repository you can download. It is also not a programming language, it is also not some dark art magic either.</p> <p>DevOps is a way to do smarter things in Software Development. - Hold up... But if you are not a software developer should you turn away right now and not dive into this project??? No. Not at all. Stay... Because DevOps brings together a combination of software development and operations. I mentioned earlier that I was more on the VM side and that would generally fall under the Operations side of the house, but within the community, there are people with all different backgrounds where DevOps is 100% going to benefit the individual, Developers, Operations and QA Engineers all can equally learn these best practices by having a better understanding of DevOps.</p> <p>DevOps is a set of practices that help to reach the goal of this movement: reducing the time between the ideation phase of a product and its release in production to the end-user or whomever it could be an internal team or customer.</p> <p>Another area we will dive into in this first week is around The Agile Methodology. DevOps and Agile are widely adopted together to achieve continuous delivery of your Application.</p> <p>The high-level takeaway is that a DevOps mindset or culture is about shrinking the long, drawn out software release process from potentially years to being able to drop smaller releases more frequently. The other key fundamental point to understand here is the responsibility of a DevOps engineer to break down silos between the teams I previously mentioned: Developers, Operations and QA.</p> <p>From a DevOps perspective, Development, Testing and Deployment all land with the DevOps team.</p> <p>The final point I will make is to make this as effective and efficient as possible we must leverage Automation.</p>"},{"location":"90DaysOfDevOps/day01/#resources","title":"Resources","text":"<p>I am always open to adding additional resources to these readme files as it is here as a learning tool.</p> <p>My advice is to watch all of the below and hopefully you have also picked something up from the text and explanations above.</p> <ul> <li>DevOps in 5 Minutes</li> <li>What is DevOps? Easy Way</li> <li>DevOps roadmap 2022 | Success Roadmap 2022</li> <li>From Zero to DevOps Engineer - DevOps Roadmap for YOUR specific background </li> </ul> <p>If you made it this far, then you will know if this is where you want to be or not. See you on Day 2.</p>"},{"location":"90DaysOfDevOps/day02/","title":"#90DaysOfDevOps - Responsibilities of a DevOps Engineer - Day 2","text":""},{"location":"90DaysOfDevOps/day02/#responsibilities-of-a-devops-engineer","title":"Responsibilities of a DevOps Engineer","text":"<p>Hopefully, you are coming into this off the back of going through the resources and posting on Day1 of #90DaysOfDevOps</p> <p>It was briefly touched on in the first post but now we must get deeper into this concept and understand that there are two main parts when creating an application. We have the Development part where software developers program the application and test it. Then we have the Operations part where the application is deployed and maintained on a server.</p>"},{"location":"90DaysOfDevOps/day02/#devops-is-the-link-between-the-two","title":"DevOps is the link between the two","text":"<p>To get to grips with DevOps or the tasks which a DevOps engineer would be carrying out we need to understand the tools or the process and overview of those and how they come together.</p> <p>Everything starts with the application! You will see so much throughout that it is all about the application when it comes to DevOps.</p> <p>Developers will create an application, this can be done with many different technology stacks and let's leave that to the imagination for now as we get into this later. This can also involve many different programming languages, build tools, code repositories etc.</p> <p>As a DevOps engineer you won't be programming the application but having a good understanding of the concepts of how a developer works and the systems, tools and processes they are using is key to success.</p> <p>At a very high level, you are going to need to know how the application is configured to talk to all of its required services or data services and then also sprinkle a requirement of how this can or should be tested.</p> <p>The application will need to be deployed somewhere, let's keep it generally simple here and make this a server, doesn't matter where but a server. This is then expected to be accessed by the customer or end user depending on the application that has been created.</p> <p>This server needs to run somewhere, on-premises, in a public cloud, serverless (Ok I have gone too far, we won't be covering serverless but its an option and more and more enterprises are heading this way) Someone needs to create and configure these servers and get them ready for the application to run. Now, this element might land to you as a DevOps engineer to deploy and configure these servers.</p> <p>These servers run an operating system and generally speaking this is going to be Linux but we have a whole section or week where we cover some of the foundational knowledge you should gain here.</p> <p>It is also likely that we need to communicate with other services in our network or environment, so we also need to have that level of knowledge around networking and configuring that, this might to some degree also land at the feet of the DevOps engineer. Again we will cover this in more detail in a dedicated section talking about all things DNS, DHCP, Load Balancing etc.</p>"},{"location":"90DaysOfDevOps/day02/#jack-of-all-trades-master-of-none","title":"Jack of all trades, Master of none","text":"<p>I will say at this point though, you don't need to be a Network or Infrastructure specialist you need a foundational knowledge of how to get things up and running and talking to each other, much the same as maybe having a foundational knowledge of a programming language but you don't need to be a developer. However, you might be coming into this as a specialist in an area and that is a great footing to adapt to other areas.</p> <p>You will also most likely not take over the management of these servers or the application daily.</p> <p>We have been talking about servers but the likelihood is that your application will be developed to run as containers, Which still runs on a server for the most part but you will also need an understanding of not only virtualisation, Cloud Infrastructure as a Service (IaaS) but also containerisation as well, The focus in these 90 days will be more catered towards containers.</p>"},{"location":"90DaysOfDevOps/day02/#high-level-overview","title":"High-Level Overview","text":"<p>On one side we have our developers creating new features and functionality (as well as bug fixes) for the application.</p> <p>On the other side, we have some sort of environment, infrastructure or servers which are configured and managed to run this application and communicate with all its required services.</p> <p>The big question is how do we get those features and bug fixes into our products and make them available to those end users?</p> <p>How do we release the new application version? This is one of the main tasks for a DevOps engineer, and the important thing here is not to just figure out how to do this once but we need to do this continuously and in an automated, efficient way which also needs to include testing!</p> <p>This is where we are going to end this day of learning, hopefully, this was useful. Over the next few days, we are going to dive a little deeper into some more areas of DevOps and then we will get into the sections that dive deeper into the tooling and processes and the benefits of these.</p>"},{"location":"90DaysOfDevOps/day02/#resources","title":"Resources","text":"<p>I am always open to adding additional resources to these readme files as it is here as a learning tool.</p> <p>My advice is to watch all of the below and hopefully you also picked something up from the text and explanations above.</p> <ul> <li>What is DevOps? - TechWorld with Nana</li> <li>What is DevOps? - GitHub YouTube</li> <li>What is DevOps? - IBM YouTube</li> <li>What is DevOps? - AWS</li> <li>What is DevOps? - Microsoft</li> </ul> <p>If you made it this far then you will know if this is where you want to be or not. See you on Day 3.</p>"},{"location":"90DaysOfDevOps/day03/","title":"#90DaysOfDevOps - Application Focused - Day 3","text":""},{"location":"90DaysOfDevOps/day03/#devops-lifecycle-application-focused","title":"DevOps Lifecycle - Application Focused","text":"<p>As we continue through these next few weeks we are 100% going to come across these titles (Continuous Development, Testing, Deployment, Monitor) over and over again, If you are heading towards the DevOps Engineer role then repeatability will be something you will get used to but constantly enhancing each time is another thing that keeps things interesting.</p> <p>In this hour we are going to take a look at the high-level view of the application from start to finish and then back around again like a constant loop.</p>"},{"location":"90DaysOfDevOps/day03/#development","title":"Development","text":"<p>Let's take a brand new example of an Application, to start with we have nothing created, maybe as a developer, you have to discuss with your client or end user the requirements and come up with some sort of plan or requirements for your Application. We then need to create from the requirements our brand new application.</p> <p>In regards to tooling at this stage, there is no real requirement here other than choosing your IDE and the programming language you wish to use to write your application.</p> <p>As a DevOps engineer, remember you are probably not the one creating this plan or coding the application for the end user, this will be a skilled developer.</p> <p>But it also would not hurt for you to be able to read some of the code so that you can make the best infrastructure decisions moving forward for your application.</p> <p>We previously mentioned that this application can be written in any language. Importantly this should be maintained using a version control system, this is something we will cover also in detail later on and in particular, we will dive into Git.</p> <p>It is also likely that it will not be one developer working on this project although this could be the case even so best practices would require a code repository to store and collaborate on the code, this could be private or public and could be hosted or privately deployed generally speaking you would hear the likes of GitHub or GitLab being used as a code repository. Again we will cover these as part of our section on Git later on.</p>"},{"location":"90DaysOfDevOps/day03/#testing","title":"Testing","text":"<p>At this stage, we have our requirements and we have our application being developed. But we need to make sure we are testing our code in all the different environments that we have available to us or specifically maybe to the programming language chosen.</p> <p>This phase enables QA to test for bugs, more frequently we see containers being used for simulating the test environment which overall can improve on cost overheads of physical or cloud infrastructure.</p> <p>This phase is also likely going to be automated as part of the next area which is Continuous Integration.</p> <p>The ability to automate this testing vs 10s,100s or even 1000s of QA engineers having to do this manually speaks for itself, these engineers can focus on something else within the stack to ensure you are moving faster and developing more functionality vs testing bugs and software which tends to be the hold up on most traditional software releases that use a waterfall methodology.</p>"},{"location":"90DaysOfDevOps/day03/#integration","title":"Integration","text":"<p>Quite importantly Integration is at the middle of the DevOps lifecycle. It is the practice in which developers require to commit changes to the source code more frequently. This could be on a daily or weekly basis.</p> <p>With every commit, your application can go through the automated testing phases and this allows for early detection of issues or bugs before the next phase.</p> <p>Now you might at this stage be saying \"but we don't create applications, we buy it off the shelf from a software vendor\" Don't worry many companies do this and will continue to do this and it will be the software vendor that is concentrating on the above 3 phases but you might want to still adopt the final phase as this will enable for faster and more efficient deployments of your off the shelf deployments.</p> <p>I would also suggest just having this above knowledge is very important as you might buy off the shelf software today, but what about tomorrow or down the line... next job maybe?</p>"},{"location":"90DaysOfDevOps/day03/#deployment","title":"Deployment","text":"<p>Ok so we have our application built and tested against the requirements of our end user and we now need to go ahead and deploy this application into production for our end users to consume.</p> <p>This is the stage where the code is deployed to the production servers, now this is where things get extremely interesting and it is where the rest of our 86 days dives deeper into these areas. Because different applications require different possibly hardware or configurations. This is where Application Configuration Management and Infrastructure as Code could play a key part in your DevOps lifecycle. It might be that your application is Containerised but also available to run on a virtual machine. This then also leads us onto platforms like Kubernetes which would be orchestrating those containers and making sure you have the desired state available to your end users.</p> <p>Of these bold topics, we will go into more detail over the next few weeks to get a better foundational knowledge of what they are and when to use them.</p>"},{"location":"90DaysOfDevOps/day03/#monitoring","title":"Monitoring","text":"<p>Things are moving fast here and we have our Application that we are continuously updating with new features and functionality and we have our testing making sure no gremlins are being found. We have the application running in our environment that can be continually keeping the required configuration and performance.</p> <p>But now we need to be sure that our end users are getting the experience they require. Here we need to make sure that our Application Performance is continuously being monitored, this phase is going to allow your developers to make better decisions about enhancements to the application in future releases to better serve the end users.</p> <p>This section is also where we are going to capture that feedback wheel about the features that have been implemented and how the end users would like to make these better for them.</p> <p>Reliability is a key factor here as well, at the end of the day we want our Application to be available all the time it is required. This then leads to other observability, security and data management areas that should be continuously monitored and feedback can always be used to better enhance, update and release the application continuously.</p> <p>Some input from the community here specifically @_ediri mentioned also part of this continuous process we should also have the FinOps teams involved. Apps &amp; Data are running and stored somewhere you should be monitoring this continuously to make sure if things change from a resources point of view your costs are not causing some major financial pain on your Cloud Bills.</p> <p>I think it is also a good time to bring up the \"DevOps Engineer\" mentioned above, albeit there are many DevOps Engineer positions in the wild that people hold, this is not the ideal way of positioning the process of DevOps. What I mean is from speaking to others in the community the title of DevOps Engineer should not be the goal for anyone because really any position should be adopting DevOps processes and the culture explained here. DevOps should be used in many different positions such as Cloud-Native engineer/architect, virtualisation admin, cloud architect/engineer, and infrastructure admin. This is to name a few but the reason for using DevOps Engineer above was really to highlight the scope of the process used by any of the above positions and more.</p>"},{"location":"90DaysOfDevOps/day03/#resources","title":"Resources","text":"<p>I am always open to adding additional resources to these readme files as it is here as a learning tool.</p> <p>My advice is to watch all of the below and hopefully you also picked something up from the text and explanations above.</p> <ul> <li>Continuous Development I will also add that this is focused on manufacturing but the lean culture can be closely followed with DevOps.</li> <li>Continuous Testing - IBM YouTube</li> <li>Continuous Integration - IBM YouTube</li> <li>Continuous Monitoring</li> <li>The Remote Flow</li> <li>FinOps Foundation - What is FinOps</li> <li>NOT FREE The Phoenix Project: A Novel About IT, DevOps, and Helping Your Business Win</li> </ul> <p>If you made it this far then you will know if this is where you want to be or not. See you on Day 4.</p>"},{"location":"90DaysOfDevOps/day04/","title":"#90DaysOfDevOps - DevOps & Agile - Day 4","text":""},{"location":"90DaysOfDevOps/day04/#devops-agile","title":"DevOps &amp; Agile","text":"<p>Do you know the difference between DevOps and Agile? They were formed as standalone concepts. But now the two terms are getting fused.</p> <p>In this post, we will examine the crucial differences between Agile and DevOps and find out why the two are connected so tightly.</p> <p>I think a good place to start is understanding a little more about a common angle I have seen in learning this area and that is DevOps vs Agile, even though they have similar goals and processes. In this section, I am going to summarise this hopefully.</p> <p>Let's start with definitions.</p>"},{"location":"90DaysOfDevOps/day04/#agile-development","title":"Agile Development","text":"<p>Agile is an approach that focuses on delivering small results faster rather than releasing one big interaction of the product; software is developed in iterations. The team releases a new version every week or month with incremental updates. The final goal of Agile is to deliver an optimal experience to the end-users.</p>"},{"location":"90DaysOfDevOps/day04/#devops","title":"DevOps","text":"<p>We have been covering this for the past few days with a few different ways of describing the end goals of DevOps. DevOps usually describes software development and delivery practices based on cooperation between software developers and operations specialists. The main DevOps benefits are delivering a simplified development process and minimising miscommunication.</p>"},{"location":"90DaysOfDevOps/day04/#what-is-the-difference-between-agile-and-devops","title":"What is the difference between Agile and DevOps","text":"<p>The difference is mainly the preoccupations. Agile and DevOps have different preoccupations but they are helping each other. Agile wants short iteration, which is only possible with the automation that DevOps brings. Agile wants the customer to try a specific version and quickly give feedback which is only possible if DevOps make the creation of a new environment easily.</p>"},{"location":"90DaysOfDevOps/day04/#different-participants","title":"Different participants","text":"<p>Agile focuses on optimising communication between end-users and developers while DevOps targets developers and operation, team members. We could say that agile is outward-oriented toward customers whereas DevOps is a set of internal practices.</p>"},{"location":"90DaysOfDevOps/day04/#team","title":"Team","text":"<p>Agile usually applies to software developers and project managers. The competencies of DevOps engineers lie in the intersection of development, QA (quality assurance) and operations as they are involved in all stages of the product cycle and are part of the Agile team.</p>"},{"location":"90DaysOfDevOps/day04/#applied-frameworks","title":"Applied Frameworks","text":"<p>Agile has a lot of management frameworks to achieve flexibility and transparency: Scrum &gt; Kanban &gt; Lean &gt; Extreme &gt; Crystal &gt; Dynamic &gt; Feature-Driven. DevOps focuses on the development approach in collaboration but doesn't offer specific methodologies. However, DevOps promote practices like Infrastructure as Code, Architecture as Code, Monitoring, Self Healing, end to end test automation ... But per se this is not a framework, but rather practice.</p>"},{"location":"90DaysOfDevOps/day04/#feedback","title":"Feedback","text":"<p>In Agile the main source of feedback is the end-user while in DevOps the feedback from stakeholders and the team itself has a higher priority.</p>"},{"location":"90DaysOfDevOps/day04/#target-areas","title":"Target areas","text":"<p>Agile focuses more on software development than deployment and maintenance. DevOps focuses on software development as well but its values and tools also cover deployment and post-release stages like monitoring, high availability, security and data protection.</p>"},{"location":"90DaysOfDevOps/day04/#documentation","title":"Documentation","text":"<p>Agile prioritises flexibility and tasks at hand over documentation and monitoring. DevOps on the other hand regards project documentation as one of the essential project components.</p>"},{"location":"90DaysOfDevOps/day04/#risks","title":"Risks","text":"<p>Agile risks derive from the flexibility of the methodology. Agile projects are difficult to predict or evaluate as priorities and requirements are continually changing.</p> <p>DevOps risks derive from a misunderstanding of the term and the lack of suitable tools. Some people see DevOps as a collection of software for the deployment and continuous integration failing to change the underlying structure of the development process.</p>"},{"location":"90DaysOfDevOps/day04/#the-tools-used","title":"The Tools Used","text":"<p>Agile tools are focused on management communication collaboration, metrics and feedback processing. The most popular agile tools include JIRA, Trello, Slack, Zoom, SurveyMonkey and others.</p> <p>DevOps uses tools for team communication, software development, deployment and integration like Jenkins, GitHub Actions, BitBucket, etc. Even though agile and DevOps have slightly different focuses and scopes the key values are almost identical, therefore you can combine the two.</p>"},{"location":"90DaysOfDevOps/day04/#bring-it-all-together-good-idea-or-not-discuss","title":"Bring it all together\u2026 good idea or not? Discuss?","text":"<p>The combination of Agile and DevOps brings the following benefits you will get:</p> <ul> <li>Flexible management and powerful technology.</li> <li>Agile practices help DevOps teams to communicate their priorities more efficiently.</li> <li>The automation cost that you have to pay for your DevOps practices is justified by your agile requirement of deploying quickly and frequently.</li> <li>It leads to strengthening: the team adopting agile practices will improve collaboration, increase the team's motivation and decrease employee turnover rates.</li> <li>As a result, you get better product quality.</li> </ul> <p>Agile allows coming back to previous product development stages to fix errors and prevent the accumulation of technical debt. To adopt agile and DevOps simultaneously just follow 7 steps:</p> <ol> <li>Unite the development and operation teams.</li> <li>Create build and run teams, all development and operational concerns are discussed by the entire DevOps team.</li> <li>Change your approach to sprints, and assign priority ratings to offer DevOps tasks that have the same value as development tasks. Encourage development and operations teams to exchange their opinion on other teams' workflow and possible issues.</li> <li>Include QA in all development stages.</li> <li>Choose the right tools.</li> <li>Automate everything you can.</li> <li>Measure and control by using tangible numeric deliverables.</li> </ol> <p>What do you think? Do you have different views? I want to hear from Developers, Operations, QA or anyone that has a better understanding of Agile and DevOps that can pass comments and feedback on this?</p>"},{"location":"90DaysOfDevOps/day04/#resources","title":"Resources","text":"<ul> <li>DevOps for Developers \u2013 Day in the Life: DevOps Engineer in 2021</li> <li>3 Things I wish I knew as a DevOps Engineer</li> <li>How to become a DevOps Engineer feat. Shawn Powers</li> </ul> <p>If you made it this far then you will know if this is where you want to be or not. See you on Day 5.</p>"},{"location":"90DaysOfDevOps/day05/","title":"#90DaysOfDevOps - Plan > Code > Build > Testing > Release > Deploy > Operate > Monitor > - Day 5","text":""},{"location":"90DaysOfDevOps/day05/#plan-code-build-testing-release-deploy-operate-monitor","title":"Plan &gt; Code &gt; Build &gt; Testing &gt; Release &gt; Deploy &gt; Operate &gt; Monitor &gt;","text":"<p>Today we are going to focus on the individual steps from start to finish and the continuous cycle of an Application in a DevOps world.</p> <p></p>"},{"location":"90DaysOfDevOps/day05/#plan","title":"Plan","text":"<p>It all starts with the planning process this is where the development team gets together and figures out what types of features and bug fixes they're going to roll out in their next sprint. This is an opportunity as a DevOps Engineer for you to get involved with that and learn what kinds of things are going to be coming your way that you need to be involved with and also influence their decisions or their path and kind of help them work with the infrastructure that you've built or steer them towards something that's going to work better for them in case they're not on that path and so one key thing to point out here is the developers or software engineering team is your customer as a DevOps engineer so this is your opportunity to work with your customer before they go down a bad path.</p>"},{"location":"90DaysOfDevOps/day05/#code","title":"Code","text":"<p>Now once that planning session's done they're going to go start writing the code you may or may not be involved a whole lot with this one of the places you may get involved with it, is whenever they're writing code you can help them better understand the infrastructure so if they know what services are available and how to best talk with those services so they're going to do that and then once they're done they'll merge that code into the repository</p>"},{"location":"90DaysOfDevOps/day05/#build","title":"Build","text":"<p>This is where we'll kick off the first of our automation processes because we're going to take their code and we're going to build it depending on what language they're using it may be transpiring it or compiling it or it might be creating a docker image from that code either way we're going to go through that process using our ci cd pipeline</p>"},{"location":"90DaysOfDevOps/day05/#testing","title":"Testing","text":"<p>Once we've built it we're going to run some tests on it now the development team usually writes the test you may have some input in what tests get written but we need to run those tests and the testing is a way for us to try and minimise introducing problems out into production, it doesn't guarantee that but we want to get as close to a guarantee as we can that were one not introducing new bugs and two not breaking things that used to work</p>"},{"location":"90DaysOfDevOps/day05/#release","title":"Release","text":"<p>Once those tests pass we're going to do the release process and depending again on what type of application you're working on this may be a non-step. You know the code may just live in the GitHub repo or the git repository or wherever it lives but it may be the process of taking your compiled code or the docker image that you've built and putting it into a registry or a repository where it's accessible by your production servers for the deployment process</p>"},{"location":"90DaysOfDevOps/day05/#deploy","title":"Deploy","text":"<p>which is the thing that we do next because deployment is like the end game of this whole thing because deployments are when we put the code into production and it's not until we do that that our business realizes the value from all the time effort and hard work that you and the software engineering team have put into this product up to this point.</p>"},{"location":"90DaysOfDevOps/day05/#operate","title":"Operate","text":"<p>Once it's deployed we are going to operate it and operate it may involve something like you start getting calls from your customers that they're all annoyed that the site's running slow or their application is running slow right so you need to figure out why that is and then possibly build auto-scaling you know to handle increase the number of servers available during peak periods and decrease the number of servers during off-peak periods either way that's all operational type metrics, another operational thing that you do is include like a feedback loop from production back to your ops team letting you know about key events that happened in production such as a deployment back one step on the deployment thing this may or may not get automated depending on your environment the goal is to always automate it when possible there are some environments where you possibly need to do a few steps before you're ready to do that but ideally you want to deploy automatically as part of your automation process but if you're doing that it might be a good idea to include in your operational steps some type of notification so that your ops team knows that a deployment has happened</p>"},{"location":"90DaysOfDevOps/day05/#monitor","title":"Monitor","text":"<p>All of the above parts lead to the final step because you need to have monitoring, especially around operational issues auto-scaling troubleshooting like you don't know there's a problem if you don't have monitoring in place to tell you that there's a problem so some of the things you might build monitoring for are memory utilization CPU utilization disk space, API endpoint, response time, how quickly that endpoint is responding and a big part of that as well is logs. Logs give developers the ability to see what is happening without having to access production systems.</p>"},{"location":"90DaysOfDevOps/day05/#rinse-repeat","title":"Rinse &amp; Repeat","text":"<p>Once that's in place you go right back to the beginning to the planning stage and go through the whole thing again</p>"},{"location":"90DaysOfDevOps/day05/#continuous","title":"Continuous","text":"<p>Many tools help us achieve the above continuous process, all this code and the ultimate goal of being completely automated, cloud infrastructure or any environment is often described as Continuous Integration/ Continuous Delivery/Continous Deployment or \u201cCI/CD\u201d for short. We will spend a whole week on CI/CD later on in the 90 Days with some examples and walkthroughs to grasp the fundamentals.</p>"},{"location":"90DaysOfDevOps/day05/#continuous-delivery","title":"Continuous Delivery","text":"<p>Continuous Delivery = Plan &gt; Code &gt; Build &gt; Test</p>"},{"location":"90DaysOfDevOps/day05/#continuous-integration","title":"Continuous Integration","text":"<p>This is effectively the outcome of the Continuous Delivery phases above plus the outcome of the Release phase. This is the case for both failure and success but this is fed back into continuous delivery or moved to Continuous Deployment.</p> <p>Continuous Integration = Plan &gt; Code &gt; Build &gt; Test &gt; Release</p>"},{"location":"90DaysOfDevOps/day05/#continuous-deployment","title":"Continuous Deployment","text":"<p>If you have a successful release from your continuous integration then move to Continuous Deployment which brings in the following phases</p> <p>CI Release is Success = Continuous Deployment = Deploy &gt; Operate &gt; Monitor</p> <p>You can see these three Continuous notions above as the simple collection of phases of the DevOps Lifecycle.</p> <p>This last bit was a bit of a recap for me on Day 3 but think this makes things clearer for me.</p>"},{"location":"90DaysOfDevOps/day05/#resources","title":"Resources","text":"<ul> <li>DevOps for Developers \u2013 Software or DevOps Engineer?</li> <li>Techworld with Nana -DevOps Roadmap 2022 - How to become a DevOps Engineer? What is DevOps?</li> <li>How to become a DevOps Engineer in 2021 - DevOps Roadmap</li> </ul> <p>If you made it this far then you will know if this is where you want to be or not.</p> <p>See you on Day 6.</p>"},{"location":"90DaysOfDevOps/day06/","title":"#90DaysOfDevOps - DevOps - The real stories - Day 6","text":""},{"location":"90DaysOfDevOps/day06/#devops-the-real-stories","title":"DevOps - The real stories","text":"<p>DevOps to begin with was seen to be out of reach for a lot of us as we didn't have companies like Netflix or a fortune 500 company practising it but I think that now it's beginning to sway into the normal as businesses start adopting a DevOps practice.</p> <p>You will see from the below references that there are a lot of different industries and verticals using DevOps and hence having a huge positive effect on their business objectives.</p> <p>The overarching benefit here is that DevOps if done correctly should help improve your business's speed and quality of software development.</p> <p>I wanted to take this day to look at successful companies that have adopted a DevOps practice and share some resources around this. This will be a great oppurtunity for the community to dive in and help here. Have you adopted a DevOps culture in your business? Has it been successful?</p> <p>I mentioned Netflix above and will touch on them again as it is a very good model and quite advanced compared to what we generally see today but I'll also mention some other big brands that are succeeding at this.</p>"},{"location":"90DaysOfDevOps/day06/#amazon","title":"Amazon","text":"<p>In 2010 Amazon moved their physical server footprint to the AWS(Amazon Web Services) cloud. This allowed them to save resources by scaling capacity up and down in very small increments. We also know that AWS went on to generate high revenue itself whilst running Amazon's retail branch.</p> <p>Amazon adopted in 2011 (according to the link below) a continued deployment process where their developers could deploy code whenever they wanted and to whichever servers they needed to. This enabled Amazon to achieve deploying new software to production servers at an average of 11.6 seconds!</p>"},{"location":"90DaysOfDevOps/day06/#netflix","title":"Netflix","text":"<p>Who doesn't use Netflix? It's a high quality streaming service and personally speaking, has a great user experience.</p> <p>Why is that user experience so great? Well, the ability to deliver a service with no personal recollection of glitches requires speed, flexibility, and attention to quality.</p> <p>Netflix developers can automatically build pieces of code into deployable web images without relying on IT operations. As the images are updated, they are integrated into Netflix\u2019s infrastructure using a custom-built, web-based platform.</p> <p>Continuous Monitoring is in place so that if the deployment of the images fails, the new images are rolled back and traffic is rerouted back to the previous version.</p> <p>There is a great talk listed below that goes into more about the DOs and DONTs that Netflix lives and dies by within their teams.</p>"},{"location":"90DaysOfDevOps/day06/#etsy","title":"Etsy","text":"<p>As with many of us and with many companies, there was a real struggle around slow and painful deployments. In the same vein, we might have also experienced working in companies that have lots of silos and teams that are not working well together.</p> <p>From what I can make out by reading about Amazon and Netflix is that Etsy might have adopted letting developers deploy their code around the end of 2009 which might have been even before the other two. (Interesting!)</p> <p>An interesting takeaway I read here was that they realised that when developers feel responsible for deployment they also would take responsibility for application performance, uptime and other goals.</p> <p>A learning culture is a key part of DevOps. Even failure can be a success if lessons are learned. (not sure where this quote came from but it kind of makes sense!)</p> <p>I have added some other stories where DevOps has changed the game within some of these massively successful companies.</p>"},{"location":"90DaysOfDevOps/day06/#resources","title":"Resources","text":"<ul> <li>How Netflix Thinks of DevOps</li> <li>16 Popular DevOps Use Cases &amp; Real Life Applications [2021]</li> <li>DevOps: The Amazon Story</li> <li>How Etsy makes DevOps work</li> <li>Adopting DevOps @ Scale Lessons learned at Hertz, Kaiser Permanente and lBM</li> <li>Interplanetary DevOps at NASA JPL</li> <li>Target CIO explains how DevOps took root inside the retail giant</li> </ul>"},{"location":"90DaysOfDevOps/day06/#recap-of-our-first-few-days-looking-at-devops","title":"Recap of our first few days looking at DevOps","text":"<ul> <li> <p>DevOps is a combination of Development and Operations that allows a single team to manage the whole application development lifecycle which consists of Development, Testing, Deployment, Operations.</p> </li> <li> <p>The main focus and aim of DevOps are to shorten the development lifecycle while delivering features, fixes and functionality frequently in close alignment with business objectives.</p> </li> <li> <p>DevOps is a software development approach through which software can be delivered and developed reliably and quickly. You may also see this referenced as Continuous Development, Testing, Deployment, Monitoring</p> </li> </ul> <p>If you made it this far then you will know if this is where you want to be or not. See you on Day 7.</p> <p>Day 7 will be us diving into a programming language. I am not aiming to be a developer but I want to be able to understand what the developers are doing.</p> <p>Can we achieve that in a week? Probably not but if we spend 7 days or 7 hours learning something we are going to know more than when we started.</p>"},{"location":"90DaysOfDevOps/day07/","title":"#90DaysOfDevOps - The Big Picture: Learning a Programming Language - Day 7","text":""},{"location":"90DaysOfDevOps/day07/#the-big-picture-devops-learning-a-programming-language","title":"The Big Picture: DevOps &amp; Learning a Programming Language","text":"<p>I think it is fair to say to be successful in the long term as a DevOps engineer you've got to know at least one programming language at a foundational level. I want to take this first session of this section to explore why this is such a critical skill to have, and hopefully, by the end of this week or section, you are going to have a better understanding of the why, how and what to do to progress with your learning journey.</p> <p>I think if I was to ask out on social do you need to have programming skills for DevOps related roles, the answer will be most likely a hard yes? Let me know if you think otherwise? Ok but then a bigger question and this is where you won't get such a clear answer which programming language? The most common answer I have seen here has been Python or increasingly more often, we're seeing Golang or Go should be the language that you learn.</p> <p>To be successful in DevOps you have to have a good knowledge of programming skills is my takeaway from that at least. But we have to understand why we need it to choose the right path.</p>"},{"location":"90DaysOfDevOps/day07/#understand-why-you-need-to-learn-a-programming-language","title":"Understand why you need to learn a programming language.","text":"<p>The reason that Python and Go are recommended so often for DevOps engineers is that a lot of the DevOps tooling is written in either Python or Go, which makes sense if you are going to be building DevOps tools. Now this is important as this will determine really what you should learn and that would likely be the most beneficial. If you are going to be building DevOps tools or you are joining a team that does then it would make sense to learn that same language, if you are going to be heavily involved in Kubernetes or Containers then it's more than likely that you would want to choose Go as your programming language. For me, the company I work for (Kasten by Veeam) is in the Cloud-Native ecosystem focused on data management for Kubernetes and everything is written in Go.</p> <p>But then you might not have clear cut reasoning like that to choose you might be a student or transitioning careers with no real decision made for you. I think in this situation then you should choose the one that seems to resonate and fit with the applications you are looking to work with.</p> <p>Remember I am not looking to become a software developer here I just want to understand a little more about the programming language so that I can read and understand what those tools are doing and then that leads to possibly how we can help improve things.</p> <p>It is also important to know how you interact with those DevOps tools which could be Kasten K10 or it could be Terraform and HCL. These are what we will call config files and this is how you interact with those DevOps tools to make things happen, commonly these are going to be YAML. (We may use the last day of this section to dive a little into YAML)</p>"},{"location":"90DaysOfDevOps/day07/#did-i-just-talk-myself-out-of-learning-a-programming-language","title":"Did I just talk myself out of learning a programming language?","text":"<p>Most of the time or depending on the role, you will be helping engineering teams implement DevOps into their workflow, a lot of testing around the application and making sure that the workflow that is built aligns to those DevOps principles we mentioned over the first few days. But in reality, this is going to be a lot of the time troubleshooting an application performance issue or something along those lines. This comes back to my original point and reasoning, the programming language I need to know is the one that the code is written in? If their application is written in NodeJS it won\u2019t help much if you have a Go or Python badge.</p>"},{"location":"90DaysOfDevOps/day07/#why-go","title":"Why Go","text":"<p>Why Golang is the next programming language for DevOps, Go has become a very popular programming language in recent years. According to the StackOverflow Survey for 2021 Go came in fourth for the most wanted Programming, scripting and markup languages with Python being top but hear me out. StackOverflow 2021 Developer Survey \u2013 Most Wanted Link</p> <p>As I have also mentioned some of the most known DevOps tools and platforms are written in Go such as Kubernetes, Docker, Grafana and Prometheus.</p> <p>What are some of the characteristics of Go that make it great for DevOps?</p>"},{"location":"90DaysOfDevOps/day07/#build-and-deployment-of-go-programs","title":"Build and Deployment of Go Programs","text":"<p>An advantage of using a language like Python that is interpreted in a DevOps role is that you don\u2019t need to compile a python program before running it. Especially for smaller automation tasks, you don\u2019t want to be slowed down by a build process that requires compilation even though, Go is a compiled programming language, Go compiles directly into machine code. Go is known also for fast compilation times.</p>"},{"location":"90DaysOfDevOps/day07/#go-vs-python-for-devops","title":"Go vs Python for DevOps","text":"<p>Go Programs are statically linked, this means that when you compile a go program everything is included in a single binary executable, and no external dependencies will be required that would need to be installed on the remote machine, this makes the deployment of go programs easy, compared to python program that uses external libraries you have to make sure that all those libraries are installed on the remote machine that you wish to run on.</p> <p>Go is a platform-independent language, which means you can produce binary executables for *all the operating systems, Linux, Windows, macOS etc and very easy to do so. With Python, it is not as easy to create these binary executables for particular operating systems.</p> <p>Go is a very performant language, it has fast compilation and fast run time with lower resource usage like CPU and memory especially compared to python, numerous optimisations have been implemented in the Go language that makes it so performant. (Resources below)</p> <p>Unlike Python which often requires the use of third party libraries to implement a particular python program, go includes a standard library that has the majority of functionality that you would need for DevOps built directly into it. This includes functionality file processing, HTTP web services, JSON processing, native support for concurrency and parallelism as well as built-in testing.</p> <p>This is by no way throwing Python under the bus I am just giving my reasons for choosing Go but they are not the above Go vs Python it's generally because it makes sense as the company I work for develops software in Go so that is why.</p> <p>I will say that once you have or at least I am told as I am not many pages into this chapter right now, is that once you learn your first programming language it becomes easier to take on other languages. You're probably never going to have a single job in any company anywhere where you don't have to deal with managing, architect, orchestrating, debug JavaScript and Node JS applications.</p>"},{"location":"90DaysOfDevOps/day07/#resources","title":"Resources","text":"<ul> <li>StackOverflow 2021 Developer Survey</li> <li>Why we are choosing Golang to learn</li> <li>Jake Wright - Learn Go in 12 minutes</li> <li>Techworld with Nana - Golang full course - 3 hours 24 mins</li> <li>NOT FREE Nigel Poulton Pluralsight - Go Fundamentals - 3 hours 26 mins</li> <li>FreeCodeCamp - Learn Go Programming - Golang Tutorial for Beginners</li> <li>Hitesh Choudhary - Complete playlist</li> </ul> <p>Now for the next 6 days of this topic, I intend to work through some of the resources listed above and document my notes for each day. You will notice that they are generally around 3 hours as a full course, I wanted to share my complete list so that if you have time you should move ahead and work through each one if time permits, I will be sticking to my learning hour each day.</p> <p>See you on Day 8.</p>"},{"location":"90DaysOfDevOps/day08/","title":"#90DaysOfDevOps - Setting up your DevOps environment for Go & Hello World - Day 8","text":""},{"location":"90DaysOfDevOps/day08/#setting-up-your-devops-environment-for-go-hello-world","title":"Setting up your DevOps environment for Go &amp; Hello World","text":"<p>Before we get into some of the fundamentals of Go we should get Go installed on our workstation and do what every \"learning programming 101\" module teaches us which is to create the Hello World app. As this one is going to be walking through the steps to get Go installed on your workstation we are going to attempt to document the process in pictures so people can easily follow along.</p> <p>First of all, let's head on over to go.dev/dl and you will be greeted with some available options for downloads.</p> <p></p> <p>If we made it this far you probably know which workstation operating system you are running so select the appropriate download and then we can get installing. I am using Windows for this walkthrough, basically, from this next screen, we can leave all the defaults in place for now. (I will note that at the time of writing this was the latest version so screenshots might be out of date)</p> <p></p> <p>Also note if you do have an older version of Go installed you will have to remove this before installing, Windows has this built into the installer and will remove and install as one.</p> <p>Once finished you should now open a command prompt/terminal and we want to check that we have to Go installed. If you do not get the output that we see below then Go is not installed and you will need to retrace your steps.</p> <p><code>go version</code></p> <p></p> <p>Next up we want to check our environment for Go. This is always good to check to make sure your working directories are configured correctly, as you can see below we need to make sure you have the following directory on your system.</p> <p></p> <p>Did you check? Are you following along? You will probably get something like the below if you try and navigate there.</p> <p></p> <p>Ok, let's create that directory for ease I am going to use the mkdir command in my PowerShell terminal. We also need to create 3 folders within the Go folder as you will see also below.</p> <p></p> <p>Now we have to Go installed and we have our Go working directory ready for action. We now need an integrated development environment (IDE) Now there are many out there available that you can use but the most common and the one I use is Visual Studio Code or Code. You can learn more about IDEs here.</p> <p>If you have not downloaded and installed VSCode already on your workstation then you can do so by heading here. As you can see below you have your different OS options.</p> <p></p> <p>Much the same as with the Go installation we are going to download and install and keep the defaults. Once complete you can open VSCode you can select Open File and navigate to our Go directory that we created above.</p> <p></p> <p>You may get a popup about trust, read it if you want and then hit Yes, trust the authors. (I am not responsible later on though if you start opening things you don't trust!)</p> <p>Now you should see the three folders we also created earlier as well and what we want to do now is right click the src folder and create a new folder called <code>Hello</code></p> <p></p> <p>Pretty easy stuff I would say up till this point? Now we are going to create our first Go Program with no understanding of anything we put in this next phase.</p> <p>Next, create a file called <code>main.go</code> in your <code>Hello</code> folder. As soon as you hit enter on the main.go you will be asked if you want to install the Go extension and also packages you can also check that empty pkg file that we made a few steps back and notice that we should have some new packages in there now?</p> <p></p> <p>Now let's get this Hello World app going, copy the following code into your new main.go file and save that.</p> <pre><code>package main\n\nimport \"fmt\"\n\nfunc main() {\n    fmt.Println(\"Hello #90DaysOfDevOps\")\n}\n</code></pre> <p>Now I appreciate that the above might make no sense at all, but we will cover more about functions, packages and more in later days. For now, let's run our app. Back in the terminal and in our Hello folder we can now check that all is working. Using the command below we can check to see if our generic learning program is working.</p> <pre><code>go run main.go\n</code></pre> <p></p> <p>It doesn't end there though, what if we now want to take our program and run it on other Windows machines? We can do that by building our binary using the following command</p> <pre><code>go build main.go\n</code></pre> <p></p> <p>If we run that, we would see the same output:</p> <pre><code>$ ./main.exe\nHello #90DaysOfDevOps\n</code></pre>"},{"location":"90DaysOfDevOps/day08/#resources","title":"Resources","text":"<ul> <li>StackOverflow 2021 Developer Survey</li> <li>Why we are choosing Golang to learn</li> <li>Jake Wright - Learn Go in 12 minutes</li> <li>Techworld with Nana - Golang full course - 3 hours 24 mins</li> <li>NOT FREE Nigel Poulton Pluralsight - Go Fundamentals - 3 hours 26 mins</li> <li>FreeCodeCamp - Learn Go Programming - Golang Tutorial for Beginners</li> <li>Hitesh Choudhary - Complete playlist</li> </ul> <p>See you on Day 9.</p> <p></p>"},{"location":"90DaysOfDevOps/day09/","title":"#90DaysOfDevOps - Let's explain the Hello World code - Day 9","text":""},{"location":"90DaysOfDevOps/day09/#lets-explain-the-hello-world-code","title":"Let's explain the Hello World code","text":""},{"location":"90DaysOfDevOps/day09/#how-go-works","title":"How Go works","text":"<p>On Day 8 we walked through getting Go installed on your workstation and we then created our first Go application.</p> <p>In this section, we are going to take a deeper look into the code and understand a few more things about the Go language.</p>"},{"location":"90DaysOfDevOps/day09/#what-is-compiling","title":"What is Compiling?","text":"<p>Before we get into the 6 lines of the Hello World code we need to have a bit of an understanding of compiling.</p> <p>Programming languages that we commonly use such as Python, Java, Go and C++ are high-level languages. Meaning they are human-readable but when a machine is trying to execute a program it needs to be in a form that a machine can understand. We have to translate our human-readable code to machine code which is called compiling.</p> <p></p> <p>From the above you can see what we did on Day 8 here, we created a simple Hello World main.go and we then used the command <code>go build main.go</code> to compile our executable.</p>"},{"location":"90DaysOfDevOps/day09/#what-are-packages","title":"What are packages?","text":"<p>A package is a collection of source files in the same directory that are compiled together. We can simplify this further, a package is a bunch of .go files in the same directory. Remember our Hello folder from Day 8? If and when you get into more complex Go programs you might find that you have folder1 folder2 and folder3 containing different.go files that make up your program with multiple packages.</p> <p>We use packages so we can reuse other people's code, we don't have to write everything from scratch. Maybe we are wanting a calculator as part of our program, you could probably find an existing Go Package that contains the mathematical functions that you could import into your code saving you a lot of time and effort in the long run.</p> <p>Go encourages you to organise your code in packages so that it is easy to reuse and maintain source code.</p>"},{"location":"90DaysOfDevOps/day09/#hello-90daysofdevops-line-by-line","title":"Hello #90DaysOfDevOps Line by Line","text":"<p>Now let's take a look at our Hello #90DaysOfDevOps main.go file and walk through the lines.</p> <p></p> <p>In the first line, you have <code>package main</code> which means that this file belongs to a package called main. All .go files need to belong to a package, they should also have <code>package something</code> in the opening line.</p> <p>A package can be named whatever you wish. We have to call this <code>main</code> as this is the starting point of the program that is going to be in this package, this is a rule. (I need to understand more about this rule?)</p> <p></p> <p>Whenever we want to compile and execute our code we have to tell the machine where the execution needs to start. We do this by writing a function called main. The machine will look for a function called main to find the entry point of the program.</p> <p>A function is a block of code that can do some specific task and can be used across the program.</p> <p>You can declare a function with any name using <code>func</code> but in this case, we need to name it <code>main</code> as this is where the code starts.</p> <p></p> <p>Next, we are going to look at line 3 of our code, the import, this means you want to bring in another package to your main program. fmt is a standard package being used here provided by Go, this package contains the <code>Println()</code> function and because we have imported this we can use this in line 6. There are several standard packages you can include in your program and leverage or reuse them in your code saving you the hassle of having to write from scratch. Go Standard Library</p> <p></p> <p>the <code>Println()</code> that we have here is a way in which to write standard output to the terminal where ever the executable has been executed successfully. Feel free to change the message in between the ().</p> <p></p>"},{"location":"90DaysOfDevOps/day09/#tldr","title":"TLDR","text":"<ul> <li>Line 1 = This file will be in the package called <code>main</code> and this needs to be called <code>main</code> because includes the entry point of the program.</li> <li>Line 3 = For us to use the <code>Println()</code> we have to import the fmt package to use this on line 6.</li> <li>Line 5 = The actual starting point, its the <code>main</code> function.</li> <li>Line 6 = This will let us print \"Hello #90DaysOfDevOps\" on our system.</li> </ul>"},{"location":"90DaysOfDevOps/day09/#resources","title":"Resources","text":"<ul> <li>StackOverflow 2021 Developer Survey</li> <li>Why we are choosing Golang to learn</li> <li>Jake Wright - Learn Go in 12 minutes</li> <li>Techworld with Nana - Golang full course - 3 hours 24 mins</li> <li>NOT FREE Nigel Poulton Pluralsight - Go Fundamentals - 3 hours 26 mins</li> <li>FreeCodeCamp - Learn Go Programming - Golang Tutorial for Beginners</li> <li>Hitesh Choudhary - Complete playlist</li> </ul> <p>See you on Day 10.</p>"},{"location":"90DaysOfDevOps/day10/","title":"#90DaysOfDevOps - The Go Workspace - Day 10","text":""},{"location":"90DaysOfDevOps/day10/#the-go-workspace","title":"The Go Workspace","text":"<p>On Day 8 we briefly covered the Go workspace to get Go up and running to get to the demo of <code>Hello #90DaysOfDevOps</code> But we should explain a little more about the Go workspace.</p> <p>Remember we chose the defaults and we then went through and created our Go folder in the GOPATH that was already defined but in reality, this GOPATH can be changed to be wherever you want it to be.</p> <p>If you run</p> <pre><code>echo $GOPATH\n</code></pre> <p>The output should be similar to mine (with a different username may be) which is:</p> <pre><code>/home/michael/projects/go\n</code></pre> <p>Then here, we created 3 directories. src, pkg and bin</p> <p></p> <p>src is where all of your Go programs and projects are stored. This handles namespacing package management for all your Go repositories. This is where you will see on our workstation we have our Hello folder for the Hello #90DaysOfDevOps project.</p> <p></p> <p>pkg is where your archived files of packages that are or were installed in programs. This helps to speed up the compiling process based on if the packages being used have been modified.</p> <p></p> <p>bin is where all of your compiled binaries are stored.</p> <p></p> <p>Our Hello #90DaysOfDevOps is not a complex program so here is an example of a more complex Go Program taken from another great resource worth looking at GoChronicles</p> <p></p> <p>This page also goes into some great detail about why and how the layout is like this it also goes a little deeper on other folders we have not mentioned GoChronicles</p>"},{"location":"90DaysOfDevOps/day10/#compiling-running-code","title":"Compiling &amp; running code","text":"<p>On Day 9 we also covered a brief introduction to compiling code, but we can go a little deeper here.</p> <p>To run our code we first must compile it. There are three ways to do this within Go.</p> <ul> <li>go build</li> <li>go install</li> <li>go run</li> </ul> <p>Before we get to the above compile stage we need to take a look at what we get with the Go Installation.</p> <p>When we installed Go on Day 8 we installed something known as Go tools which consist of several programs that let us build and process our Go source files. One of the tools is <code>Go</code></p> <p>It is worth noting that you can install additional tools that are not in the standard Go installation.</p> <p>If you open your command prompt and type <code>go</code> you should see something like the image below and then you will see \"Additional Help Topics\" below that for now we don't need to worry about those.</p> <p></p> <p>You might also remember that we have already used at least two of these tools so far on Day 8.</p> <p></p> <p>The ones we want to learn more about are the build, install and run.</p> <p></p> <ul> <li><code>go run</code> - This command compiles and runs the main package comprised of the .go files specified on the command line. The command is compiled to a temporary folder.</li> <li><code>go build</code> - To compile packages and dependencies, compile the package in the current directory. If Go project contains a <code>main</code> package, it will create and place the executable in the current directory if not then it will put the executable in the <code>pkg</code> folder, and that can be imported and used by other Go programs. <code>go build</code> also enables you to build an executable file for any Go Supported OS platform.</li> <li><code>go install</code> - The same as go build but will place the executable in the <code>bin</code> folder.</li> </ul> <p>We have run through go build and go run but feel free to run through them again here if you wish, <code>go install</code> as stated above puts the executable in our bin folder.</p> <p></p> <p>Hopefully, if you are following along, you are watching one of the playlists or videos below. I am taking bits of all of these and translating these into my notes so that I can understand the foundational knowledge of the Golang language. The resources below are likely going to give you a much better understanding of a lot of the areas you need overall, but I am trying to document the 7 days or 7 hours worth of the journey with interesting things that I have found.</p>"},{"location":"90DaysOfDevOps/day10/#resources","title":"Resources","text":"<ul> <li>StackOverflow 2021 Developer Survey</li> <li>Why we are choosing Golang to learn</li> <li>Jake Wright - Learn Go in 12 minutes</li> <li>Techworld with Nana - Golang full course - 3 hours 24 mins</li> <li>NOT FREE Nigel Poulton Pluralsight - Go Fundamentals - 3 hours 26 mins</li> <li>FreeCodeCamp - Learn Go Programming - Golang Tutorial for Beginners</li> <li>Hitesh Choudhary - Complete playlist</li> </ul> <p>See you on Day 11.</p>"},{"location":"90DaysOfDevOps/day11/","title":"#90DaysOfDevOps - Variables & Constants in Go - Day 11","text":"<p>Before we get into the topics for today I want to give a massive shout out to Techworld with Nana and this fantastic concise journey through the fundamentals of Go.</p> <p>On Day8 we set our environment up, on Day9 we walked through the Hello #90DaysOfDevOps code and on Day10 we looked at our Go workspace and went a little deeper into compiling and running the code.</p> <p>Today we are going to take a look into Variables, Constants and Data Types whilst writing a new program.</p>"},{"location":"90DaysOfDevOps/day11/#variables-constants-in-go","title":"Variables &amp; Constants in Go","text":"<p>Let's start by planning our application, I think it would be a good idea to work on a program that tells us how many days we have remained in our #90DaysOfDevOps challenge.</p> <p>The first thing to consider here is that as we are building our app and we are welcoming our attendees and we are giving the user feedback on the number of days they have completed we might use the term #90DaysOfDevOps many times throughout the program. This is a great use case to make #90DaysOfDevOps a variable within our program.</p> <ul> <li>Variables are used to store values.</li> <li>Like a little box with our saved information or values.</li> <li>We can then use this variable across the program which also benefits that if this challenge or variable changes then we only have to change this in one place. This means we could translate this to other challenges we have in the community by just changing that one variable value.</li> </ul> <p>To declare this in our Go Program we define a value by using a keyword for variables. This will live within our <code>func main</code> block of code that you will see later. You can find more about Keywords here.</p> <p>Remember to make sure that your variable names are descriptive. If you declare a variable you must use it or you will get an error, this is to avoid possible dead code, code that is never used. This is the same for packages not used.</p> <pre><code>var challenge = \"#90DaysOfDevOps\"\n</code></pre> <p>With the above set and used as we will see in the next code snippet you can see from the output below that we have used a variable.</p> <pre><code>package main\n\nimport \"fmt\"\n\nfunc main() {\n    var challenge = \"#90DaysOfDevOps\"\n    fmt.Println(\"Welcome to\", challenge, \"\")\n}\n</code></pre> <p>You can find the above code snippet in day11_example1.go</p> <p>You will then see from the below that we built our code with the above example and we got the output shown below.</p> <p></p> <p>We also know that our challenge is 90 days at least for this challenge, but next, maybe it's 100 so we want to define a variable to help us here as well. However, for our program, we want to define this as a constant. Constants are like variables, except that their value cannot be changed within code (we can still create a new app later on down the line with this code and change this constant but this 90 will not change while we are running our application)</p> <p>Adding the <code>const</code> to our code and adding another line of code to print this.</p> <pre><code>package main\n\nimport \"fmt\"\n\nfunc main() {\n    var challenge = \"#90DaysOfDevOps\"\n    const daystotal = 90\n\n    fmt.Println(\"Welcome to\", challenge, \"\")\n    fmt.Println(\"This is a\", daystotal, \"challenge\")\n}\n</code></pre> <p>You can find the above code snippet in day11_example2.go</p> <p>If we then go through that <code>go build</code> process again and run you will see below the outcome.</p> <p></p> <p>Finally, and this won't be the end of our program we will come back to this in Day12 to add more functionality. We now want to add another variable for the number of days we have completed the challenge.</p> <p>Below I added the <code>dayscomplete</code> variable with the number of days completed.</p> <pre><code>package main\n\nimport \"fmt\"\n\nfunc main() {\n    var challenge = \"#90DaysOfDevOps\"\n    const daystotal = 90\n    var dayscomplete = 11\n\n    fmt.Println(\"Welcome to\", challenge, \"\")\n    fmt.Println(\"This is a\", daystotal, \"challenge and you have completed\", dayscomplete, \"days\")\n    fmt.Println(\"Great work\")\n}\n</code></pre> <p>You can find the above code snippet in day11_example3.go</p> <p>Let's run through that <code>go build</code> process again or you could just use <code>go run</code></p> <p></p> <p>Here are some other examples that I have used to make the code easier to read and edit. We have up till now been using <code>Println</code> but we can simplify this by using <code>Printf</code> by using <code>%v</code> which means we define our variables in order at the end of the line of code. we also use <code>\\n</code> for a line break.</p> <p>I am using <code>%v</code> as this uses a default value but there are other options that can be found here in the fmt package documentation you can find the code example day11_example4.go</p> <p>Variables may also be defined in a simpler format in your code. Instead of defining that it is a <code>var</code> and the <code>type</code> you can code this as follows to get the same functionality but a nice cleaner and simpler look for your code. This will only work for variables though and not constants.</p> <pre><code>func main() {\n    challenge := \"#90DaysOfDevOps\"\n    const daystotal = 90\n</code></pre>"},{"location":"90DaysOfDevOps/day11/#data-types","title":"Data Types","text":"<p>In the above examples, we have not defined the type of variables, this is because we can give it a value here and Go is smart enough to know what that type is or at least can infer what it is based on the value you have stored. However, if we want a user to input this will require a specific type.</p> <p>We have used Strings and Integers in our code so far. Integers for the number of days and strings are for the name of the challenge.</p> <p>It is also important to note that each data type can do different things and behaves differently. For example, integers can multiply where strings do not.</p> <p>There are four categories</p> <ul> <li>Basic type: Numbers, strings, and booleans come under this category.</li> <li>Aggregate type: Array and structs come under this category.</li> <li>Reference type: Pointers, slices, maps, functions, and channels come under this category.</li> <li>Interface type</li> </ul> <p>The data type is an important concept in programming. Data type specifies the size and type of variable values.</p> <p>Go is statically typed, meaning that once a variable type is defined, it can only store data of that type.</p> <p>Go has three basic data types:</p> <ul> <li>bool: represents a boolean value and is either true or false</li> <li>Numeric: represents integer types, floating-point values, and complex types</li> <li>string: represents a string value</li> </ul> <p>I found this resource super detailed on data types Golang by example</p> <p>I would also suggest Techworld with Nana at this point covers in detail a lot about the data types in Go.</p> <p>If we need to define a type in our variable we can do this like so:</p> <pre><code>var TwitterHandle string\nvar DaysCompleted uint\n</code></pre> <p>Because Go implies variables where a value is given we can print out those values with the following:</p> <pre><code>fmt.Printf(\"challenge is %T, daystotal is %T, dayscomplete is %T\\n\", conference, daystotal, dayscomplete)\n</code></pre> <p>There are many different types of integer and float types the links above will cover these in detail.</p> <ul> <li>int = whole numbers</li> <li>unint = positive whole numbers</li> <li>floating point types = numbers that contain a decimal component</li> </ul>"},{"location":"90DaysOfDevOps/day11/#resources","title":"Resources","text":"<ul> <li>StackOverflow 2021 Developer Survey</li> <li>Why we are choosing Golang to learn</li> <li>Jake Wright - Learn Go in 12 minutes</li> <li>Techworld with Nana - Golang full course - 3 hours 24 mins</li> <li>NOT FREE Nigel Poulton Pluralsight - Go Fundamentals - 3 hours 26 mins</li> <li>FreeCodeCamp - Learn Go Programming - Golang Tutorial for Beginners</li> <li>Hitesh Choudhary - Complete playlist</li> </ul> <p>Next up we are going to start adding some user input functionality to our program so that we are asked how many days have been completed.</p> <p>See you on Day 12.</p>"},{"location":"90DaysOfDevOps/day12/","title":"#90DaysOfDevOps - Getting user input with Pointers and a finished program - Day 12","text":""},{"location":"90DaysOfDevOps/day12/#getting-user-input-with-pointers-and-a-finished-program","title":"Getting user input with Pointers and a finished program","text":"<p>Yesterday (Day 11), we created our first Go program that was self-contained and the parts we wanted to get user input for were created as variables within our code and given values, we now want to ask the user for their input to give the variable the value for the end message.</p>"},{"location":"90DaysOfDevOps/day12/#getting-user-input","title":"Getting user input","text":"<p>Before we do that let's take a look at our application again and walk through the variables we want as a test before getting that user input.</p> <p>Yesterday we finished up with our code looking like this day11_example4.go we have manually in code defined our <code>challenge, daystotal, dayscomplete</code> variables and constants.</p> <p>Let's now add a new variable called <code>TwitterName</code> you can find this new code at day12_example1.go and if we run this code this is our output.</p> <p></p> <p>We are on day 12 and we would need to change that <code>dayscomplete</code> every day and compile our code each day if this was hardcoded which doesn't sound so great.</p> <p>Getting user input, we want to get the value of maybe a name and the number of days completed. For us to do this we can use another function from within the <code>fmt</code> package.</p> <p>Recap on the <code>fmt</code> package, different functions for formatted input and output (I/O)</p> <ul> <li>Print Messages</li> <li>Collect User Input</li> <li>Write into a file</li> </ul> <p>This is instead of assigning the value of a variable we want to ask the user for their input.</p> <pre><code>fmt.Scan(&amp;TwitterName)\n</code></pre> <p>Notice that we also use <code>&amp;</code> before the variable. This is known as a pointer which we will cover in the next section.</p> <p>In our code day12_example2.go you can see that we are asking the user to input two variables, <code>TwitterName</code> and <code>DaysCompleted</code></p> <p>Let's now run our program and you see we have input for both of the above.</p> <p></p> <p>Ok, that's great we got some user input and we printed a message but what about getting our program to tell us how many days we have left in our challenge.</p> <p>For us to do that we have created a variable called <code>remainingDays</code> and we have hard valued this in our code as <code>90</code> we then need to change the value of this value to print out the remaining days when we get our user input of <code>DaysCompleted</code> we can do this with this simple variable change.</p> <pre><code>remainingDays = remainingDays - DaysCompleted\n</code></pre> <p>You can see how our finished program looks here day12_example2.go.</p> <p>If we now run this program you can see that simple calculation is made based on the user input and the value of the <code>remainingDays</code></p> <p></p>"},{"location":"90DaysOfDevOps/day12/#what-is-a-pointer-special-variables","title":"What is a pointer? (Special Variables)","text":"<p>A pointer is a (special) variable that points to the memory address of another variable.</p> <p>A great explanation of this can be found here at geeksforgeeks</p> <p>Let's simplify our code now and show with and without the <code>&amp;</code> in front of one of our print commands, this gives us the memory address of the pointer. I have added this code example here. day12_example4.go</p> <p>Below is running this code.</p> <p></p>"},{"location":"90DaysOfDevOps/day12/#resources","title":"Resources","text":"<ul> <li>StackOverflow 2021 Developer Survey</li> <li>Why we are choosing Golang to learn</li> <li>Jake Wright - Learn Go in 12 minutes</li> <li>Techworld with Nana - Golang full course - 3 hours 24 mins</li> <li>NOT FREE Nigel Poulton Pluralsight - Go Fundamentals - 3 hours 26 mins</li> <li>FreeCodeCamp - Learn Go Programming - Golang Tutorial for Beginners</li> <li>Hitesh Choudhary - Complete playlist</li> </ul> <p>See you on Day 13.</p>"},{"location":"90DaysOfDevOps/day13/","title":"#90DaysOfDevOps - Tweet your progress with our new App - Day 13","text":""},{"location":"90DaysOfDevOps/day13/#tweet-your-progress-with-our-new-app","title":"Tweet your progress with our new App","text":"<p>On the final day of looking into this programming language, we have only just touched the surface here of the language but it is at that start that I think we need to get interested and excited and want to dive more into it.</p> <p>Over the last few days, we have taken a small idea for an application and we have added functionality to it, in this session I want to take advantage of those packages we mentioned and create the functionality for our app to not only give you the update of your progress on screen but also send a tweet with the details of the challenge and your status.</p>"},{"location":"90DaysOfDevOps/day13/#adding-the-ability-to-tweet-your-progress","title":"Adding the ability to tweet your progress","text":"<p>The first thing we need to do is set up our developer API access with Twitter for this to work.</p> <p>Head to the Twitter Developer Platform and sign in with your Twitter handle and details. Once in you should see something like the below without the app that I already have created.</p> <p></p> <p>From here you may also want to request elevated access, this might take some time but it was very fast for me.</p> <p>Next, we should select Projects &amp; Apps and create our App. Limits are depending on the account access you have, with essential you only have one app and one project and with elevated you can have 3 apps.</p> <p></p> <p>Give your application a name</p> <p></p> <p>You will be then given these API tokens, you must save these somewhere secure. (I have since deleted this app) We will need these later with our Go Application.</p> <p></p> <p>Now we have our app created,(I did have to change my app name as the one in the screenshot above was already taken, these names need to be unique)</p> <p></p> <p>The keys that we gathered before are known as our consumer keys and we will also need our access token and secrets. We can gather this information using the \"Keys &amp; Tokens\" tab.</p> <p></p> <p>Ok, we are done in the Twitter developer portal for now. Make sure you keep your keys safe because we will need them later.</p>"},{"location":"90DaysOfDevOps/day13/#go-twitter-bot","title":"Go Twitter Bot","text":"<p>Remember the code we are starting within our application as well day13_example1 but first, we need to check we have the correct code to make something tweet</p> <p>We now need to think about the code to get our output or message to Twitter in the form of a tweet. We are going to be using go-twitter This is a Go client library for the Twitter API.</p> <p>To test this before putting this into our main application, I created a new directory in our <code>src</code> folder called go-twitter-bot, issued the <code>go mod init github.com/michaelcade/go-Twitter-bot</code> on the folder which then created a <code>go.mod</code> file and then we can start writing our new main.go and test this out.</p> <p>We now need those keys, tokens and secrets we gathered from the Twitter developer portal. We are going to set these in our environment variables. This will depend on the OS you are running:</p> <p>I have had a few questions regarding environment variables, here is a blog post that goes into more detail so you can understand what is happening. How To Set Environment Variables</p> <p>Windows</p> <pre><code>set CONSUMER_KEY\nset CONSUMER_SECRET\nset ACCESS_TOKEN\nset ACCESS_TOKEN_SECRET\n</code></pre> <p>Linux / macOS</p> <pre><code>export CONSUMER_KEY\nexport CONSUMER_SECRET\nexport ACCESS_TOKEN\nexport ACCESS_TOKEN_SECRET\n</code></pre> <p>At this stage, you can take a look at day13_example2 at the code but you will see here that we are using a struct to define our keys, secrets and tokens.</p> <p>We then have a <code>func</code> to parse those credentials and make that connection to the Twitter API</p> <p>Then based on the success we will then send a tweet.</p> <pre><code>package main\n\nimport (\n    // other imports\n    \"fmt\"\n    \"log\"\n    \"os\"\n\n    \"github.com/dghubble/go-twitter/twitter\"\n    \"github.com/dghubble/oauth1\"\n)\n\n// Credentials stores all of our access/consumer tokens\n// and secret keys needed for authentication against\n// the twitter REST API.\ntype Credentials struct {\n    ConsumerKey       string\n    ConsumerSecret    string\n    AccessToken       string\n    AccessTokenSecret string\n}\n\n// getClient is a helper function that will return a twitter client\n// that we can subsequently use to send tweets, or to stream new tweets\n// this will take in a pointer to a Credential struct which will contain\n// everything needed to authenticate and return a pointer to a twitter Client\n// or an error\nfunc getClient(creds *Credentials) (*twitter.Client, error) {\n    // Pass in your consumer key (API Key) and your Consumer Secret (API Secret)\n    config := oauth1.NewConfig(creds.ConsumerKey, creds.ConsumerSecret)\n    // Pass in your Access Token and your Access Token Secret\n    token := oauth1.NewToken(creds.AccessToken, creds.AccessTokenSecret)\n\n    httpClient := config.Client(oauth1.NoContext, token)\n    client := twitter.NewClient(httpClient)\n\n    // Verify Credentials\n    verifyParams := &amp;twitter.AccountVerifyParams{\n        SkipStatus:   twitter.Bool(true),\n        IncludeEmail: twitter.Bool(true),\n    }\n\n    // we can retrieve the user and verify if the credentials\n    // we have used successfully allow us to log in!\n    user, _, err := client.Accounts.VerifyCredentials(verifyParams)\n    if err != nil {\n        return nil, err\n    }\n\n    log.Printf(\"User's ACCOUNT:\\n%+v\\n\", user)\n    return client, nil\n}\nfunc main() {\n    fmt.Println(\"Go-Twitter Bot v0.01\")\n    creds := Credentials{\n        AccessToken:       os.Getenv(\"ACCESS_TOKEN\"),\n        AccessTokenSecret: os.Getenv(\"ACCESS_TOKEN_SECRET\"),\n        ConsumerKey:       os.Getenv(\"CONSUMER_KEY\"),\n        ConsumerSecret:    os.Getenv(\"CONSUMER_SECRET\"),\n    }\n\n    client, err := getClient(&amp;creds)\n    if err != nil {\n        log.Println(\"Error getting Twitter Client\")\n        log.Println(err)\n    }\n\n    tweet, resp, err := client.Statuses.Update(\"A Test Tweet from the future, testing a #90DaysOfDevOps Program that tweets, tweet tweet\", nil)\n    if err != nil {\n        log.Println(err)\n    }\n    log.Printf(\"%+v\\n\", resp)\n    log.Printf(\"%+v\\n\", tweet)\n}\n</code></pre> <p>The above will either give you an error based on what is happening or it will succeed and you will have a tweet sent with the message outlined in the code.</p>"},{"location":"90DaysOfDevOps/day13/#pairing-the-two-together-go-twitter-bot-our-app","title":"Pairing the two together - Go-Twitter-Bot + Our App","text":"<p>Now we need to merge these two in our <code>main.go</code> I am sure someone out there is screaming that there is a better way of doing this and please comment on this as you can have more than one <code>.go</code> file in a project it might make sense but this works.</p> <p>You can see the merged codebase day13_example3 but I will also show it below.</p> <pre><code>package main\n\nimport (\n    // other imports\n    \"fmt\"\n    \"log\"\n    \"os\"\n\n    \"github.com/dghubble/go-twitter/twitter\"\n    \"github.com/dghubble/oauth1\"\n)\n\n// Credentials stores all of our access/consumer tokens\n// and secret keys needed for authentication against\n// the twitter REST API.\ntype Credentials struct {\n    ConsumerKey       string\n    ConsumerSecret    string\n    AccessToken       string\n    AccessTokenSecret string\n}\n\n// getClient is a helper function that will return a twitter client\n// that we can subsequently use to send tweets, or to stream new tweets\n// this will take in a pointer to a Credential struct which will contain\n// everything needed to authenticate and return a pointer to a twitter Client\n// or an error\nfunc getClient(creds *Credentials) (*twitter.Client, error) {\n    // Pass in your consumer key (API Key) and your Consumer Secret (API Secret)\n    config := oauth1.NewConfig(creds.ConsumerKey, creds.ConsumerSecret)\n    // Pass in your Access Token and your Access Token Secret\n    token := oauth1.NewToken(creds.AccessToken, creds.AccessTokenSecret)\n\n    httpClient := config.Client(oauth1.NoContext, token)\n    client := twitter.NewClient(httpClient)\n\n    // Verify Credentials\n    verifyParams := &amp;twitter.AccountVerifyParams{\n        SkipStatus:   twitter.Bool(true),\n        IncludeEmail: twitter.Bool(true),\n    }\n\n    // we can retrieve the user and verify if the credentials\n    // we have used successfully allow us to log in!\n    user, _, err := client.Accounts.VerifyCredentials(verifyParams)\n    if err != nil {\n        return nil, err\n    }\n\n    log.Printf(\"User's ACCOUNT:\\n%+v\\n\", user)\n    return client, nil\n}\nfunc main() {\n    creds := Credentials{\n        AccessToken:       os.Getenv(\"ACCESS_TOKEN\"),\n        AccessTokenSecret: os.Getenv(\"ACCESS_TOKEN_SECRET\"),\n        ConsumerKey:       os.Getenv(\"CONSUMER_KEY\"),\n        ConsumerSecret:    os.Getenv(\"CONSUMER_SECRET\"),\n    }\n    {\n        const DaysTotal int = 90\n        var remainingDays uint = 90\n        challenge := \"#90DaysOfDevOps\"\n\n        fmt.Printf(\"Welcome to the %v challenge.\\nThis challenge consists of %v days\\n\", challenge, DaysTotal)\n\n        var TwitterName string\n        var DaysCompleted uint\n\n        // asking for user input\n        fmt.Println(\"Enter Your Twitter Handle: \")\n        fmt.Scanln(&amp;TwitterName)\n\n        fmt.Println(\"How many days have you completed?: \")\n        fmt.Scanln(&amp;DaysCompleted)\n\n        // calculate remaining days\n        remainingDays = remainingDays - DaysCompleted\n\n        //fmt.Printf(\"Thank you %v for taking part and completing %v days.\\n\", TwitterName, DaysCompleted)\n        //fmt.Printf(\"You have %v days remaining for the %v challenge\\n\", remainingDays, challenge)\n        // fmt.Println(\"Good luck\")\n\n        client, err := getClient(&amp;creds)\n        if err != nil {\n            log.Println(\"Error getting Twitter Client, this is expected if you did not supply your Twitter API tokens\")\n            log.Println(err)\n        }\n\n        message := fmt.Sprintf(\"Hey I am %v I have been doing the %v for %v days and I have %v Days left\", TwitterName, challenge, DaysCompleted, remainingDays)\n        tweet, resp, err := client.Statuses.Update(message, nil)\n        if err != nil {\n            log.Println(err)\n        }\n        log.Printf(\"%+v\\n\", resp)\n        log.Printf(\"%+v\\n\", tweet)\n    }\n\n}\n</code></pre> <p>The outcome of this should be a tweet but if you did not supply your environment variables then you should get an error like the one below.</p> <p></p> <p>Once you have fixed that or if you choose not to authenticate with Twitter then you can use the code we finished with yesterday. The terminal output on success will look similar to this:</p> <p></p> <p>The resulting tweet should look something like this:</p> <p></p>"},{"location":"90DaysOfDevOps/day13/#how-to-compile-for-multiple-oss","title":"How to compile for multiple OSs","text":"<p>I next want to cover the question, \"How do you compile for multiple Operating Systems?\" The great thing about Go is that it can easily compile for many different Operating Systems. You can get a full list by running the following command:</p> <pre><code>go tool dist list\n</code></pre> <p>Using our <code>go build</code> commands so far is great and it will use the <code>GOOS</code> and <code>GOARCH</code> environment variables to determine the host machine and what the build should be built for. But we can also create other binaries by using the code below as an example.</p> <pre><code>GOARCH=amd64 GOOS=darwin go build -o ${BINARY_NAME}_0.1_darwin main.go\nGOARCH=amd64 GOOS=linux go build -o ${BINARY_NAME}_0.1_linux main.go\nGOARCH=amd64 GOOS=windows go build -o ${BINARY_NAME}_0.1_windows main.go\nGOARCH=arm64 GOOS=linux go build -o ${BINARY_NAME}_0.1_linux_arm64 main.go\nGOARCH=arm64 GOOS=darwin go build -o ${BINARY_NAME}_0.1_darwin_arm64 main.go\n</code></pre> <p>This will then give you binaries in your directory for all of the above platforms. You can then take this and create a makefile to build these binaries whenever you add new features and functionality to your code. I have included the makefile</p> <p>This is what I have used to create the releases you can now see on the repository</p>"},{"location":"90DaysOfDevOps/day13/#resources","title":"Resources","text":"<ul> <li>StackOverflow 2021 Developer Survey</li> <li>Why we are choosing Golang to learn</li> <li>Jake Wright - Learn Go in 12 minutes</li> <li>Techworld with Nana - Golang full course - 3 hours 24 mins</li> <li>NOT FREE Nigel Poulton Pluralsight - Go Fundamentals - 3 hours 26 mins</li> <li>FreeCodeCamp - Learn Go Programming - Golang Tutorial for Beginners</li> <li>Hitesh Choudhary - Complete playlist</li> <li>A great repo full of all things DevOps &amp; exercises</li> <li>GoByExample - Example based learning</li> <li>go.dev/tour/list</li> <li>go.dev/learn</li> </ul> <p>This wraps up the Programming language for 7 days! So much more that can be covered and I hope you have been able to continue through the content above and be able to understand some of the other aspects of the Go programming language.</p> <p>Next, we take our focus into Linux and some of the fundamentals that we should all know there.</p> <p>See you on Day 14.</p>"},{"location":"90DaysOfDevOps/day14/","title":"#90DaysOfDevOps - The Big Picture: DevOps and Linux - Day 14","text":""},{"location":"90DaysOfDevOps/day14/#the-big-picture-devops-and-linux","title":"The Big Picture: DevOps and Linux","text":"<p>Linux and DevOps share very similar cultures and perspectives; both are focused on customization and scalability. Both of these aspects of Linux are of particular importance for DevOps.</p> <p>A lot of technologies start on Linux, especially if they are related to software development or managing infrastructure.</p> <p>As well lots of open source projects, especially DevOps tools, were designed to run on Linux from the start.</p> <p>From a DevOps perspective or any operations role perspective, you are going to come across Linux I would say mostly. There is a place for WinOps but the majority of the time you are going to be administering and deploying Linux servers.</p> <p>I have been using Linux daily for several years but my go to desktop machine has always been either macOS or Windows. However, when I moved into the Cloud Native role I am in now I took the plunge to make sure that my laptop was fully Linux based and my daily driver, whilst I still needed Windows for work-based applications and a lot of my audio and video gear does not run on Linux I was forcing myself to run a Linux desktop full time to get a better grasp of a lot of the things we are going to touch on over the next 7 days.</p>"},{"location":"90DaysOfDevOps/day14/#getting-started","title":"Getting Started","text":"<p>I am not suggesting you do the same as me by any stretch as there are easier options which are less destructive but I will say that taking that full-time step forces you to learn faster how to make things work on Linux.</p> <p>For the majority of these 7 days, I am going to deploy a Virtual Machine in Virtual Box on my Windows machine. I am also going to deploy a desktop version of a Linux distribution, whereas a lot of the Linux servers you will be administering will likely be servers that come with no GUI and everything is shell-based. However, as I said at the start a lot of the tools that we covered throughout this whole 90 days started on Linux I would also strongly encourage you to dive into running that Linux Desktop for that learning experience as well.</p> <p>For the rest of this post, we are going to concentrate on getting a Ubuntu Desktop virtual machine up and running in our Virtual Box environment. Now we could just download Virtual Box and grab the latest Ubuntu ISO from the sites linked and go ahead and build out our desktop environment but that wouldn't be very DevOps of us, would it?</p> <p>Another good reason to use most Linux distributions is that they are free and open-source. We are also choosing Ubuntu as it is probably the most widely used distribution deployed not thinking about mobile devices and enterprise RedHat Enterprise servers. I might be wrong there but with CentOS and the history there I bet Ubuntu is high on the list and it's super simple.</p>"},{"location":"90DaysOfDevOps/day14/#introducing-hashicorp-vagrant","title":"Introducing HashiCorp Vagrant","text":"<p>Vagrant is a CLI utility that manages the lifecycle of your virtual machines. We can use vagrant to spin up and down virtual machines across many different platforms including vSphere, Hyper-v, Virtual Box and also Docker. It does have other providers but we will stick with Virtual Box here so we are good to go.</p> <p>The first thing we need to do is get Vagrant installed on our machine, when you go to the downloads page you will see all the operating systems listed for your choice. HashiCorp Vagrant I am using Windows so I grabbed the binary for my system and went ahead and installed this on my system.</p> <p>Next up we also need to get Virtual Box installed. Again, this can also be installed on many different operating systems and a good reason to choose this and vagrant is that if you are running Windows, macOS, or Linux then we have you covered here.</p> <p>Both installations are pretty straightforward and both have great communitites around them so feel free to reach out if you have issues and I can try and assist too.</p> <p>If you are using m1 macOS, I recommend to use multiplass instead of Vagrant and VirtualBox. (reference : https://github.com/MichaelCade/90DaysOfDevOps/issues/365)</p>"},{"location":"90DaysOfDevOps/day14/#our-first-vagrantfile","title":"Our first VAGRANTFILE","text":"<p>The VAGRANTFILE describes the type of machine we want to deploy. It also defines the configuration and provisioning for this machine.</p> <p>When it comes to saving these and organizing your VAGRANTFILEs I tend to put them in their folders in my workspace. You can see below how this looks on my system. Hopefully following this you will play around with Vagrant and see the ease of spinning up different systems, it is also great for that rabbit hole known as distro hopping for Linux Desktops.</p> <p></p> <p>Let's take a look at that VAGRANTFILE and see what we are building.</p> <pre><code>Vagrant.configure(\"2\") do |config|\n\n  config.vm.box = \"chenhan/ubuntu-desktop-20.04\"\n\n  config.vm.provider :virtualbox do |v|\n\n   v.memory  = 8096\n\n   v.cpus    = 4\n\n   v.customize [\"modifyvm\", :id, \"--vram\", \"128\"]\n\nend\n\nend\n</code></pre> <p>This is a very simple VAGRANTFILE overall. We are saying that we want a specific \"box\", a box being possibly either a public image or private build of the system you are looking for. You can find a long list of \"boxes\" publicly available here in the public catalogue of Vagrant boxes</p> <p>Next line we're saying that we want to use a specific provider and in this case it's <code>VirtualBox</code>. We also define our machine's memory to <code>8GB</code> and the number of CPUs to <code>4</code>. My experience tells me that you may want to also add the following line if you experience display issues. This will set the video memory to what you want, I would ramp this right up to <code>128MB</code> but it depends on your system.</p> <pre><code>v.customize [\"modifyvm\", :id, \"--vram\", \"\"]\n</code></pre> <p>I have also placed a copy of this specific vagrant file in the Linux Folder</p>"},{"location":"90DaysOfDevOps/day14/#provisioning-our-linux-desktop","title":"Provisioning our Linux Desktop","text":"<p>We are now ready to get our first machine up and running, in our workstation's terminal. In my case I am using PowerShell on my Windows machine. Navigate to your projects folder and where you will find your VAGRANTFILE. Once there you can type the command <code>vagrant up</code> and if everything's alright you will see something like this.</p> <p></p> <p>Another thing to add here is that the network will be set to <code>NAT</code> on your virtual machine. At this stage we don't need to know about NAT and I plan to have a whole session talking about it in the Networking session. Know that it is the easy button when it comes to getting a machine on your home network, it is also the default networking mode on Virtual Box. You can find out more in the Virtual Box documentation</p> <p>Once <code>vagrant up</code> is complete we can now use <code>vagrant ssh</code> to jump straight into the terminal of our new VM.</p> <p></p> <p>This is where we will do most of our exploring over the next few days but I also want to dive into some customizations for your developer workstation that I have done and it makes your life much simpler when running this as your daily driver, and of course, are you really in DevOps unless you have a cool nonstandard terminal?</p> <p>But just to confirm in Virtual Box you should see the login prompt when you select your VM.</p> <p></p> <p>Oh and if you made it this far and you have been asking \"WHAT IS THE USERNAME &amp; PASSWORD?\"</p> <ul> <li> <p>Username = vagrant</p> </li> <li> <p>Password = vagrant</p> </li> </ul> <p>Tomorrow we are going to get into some of the commands and what they do, The terminal is going to be the place to make everything happen.</p>"},{"location":"90DaysOfDevOps/day14/#resources","title":"Resources","text":"<ul> <li>Learn the Linux Fundamentals - Part 1</li> <li>Linux for hackers (don't worry you don't need to be a hacker!)</li> </ul> <p>There are going to be lots of resources I find as we go through and much like the Go resources I am generally going to be keeping them to FREE content so we can all partake and learn here.</p> <p>As I mentioned next up we will take a look at the commands we might be using on a daily whilst in our Linux environments.</p> <p>See you on Day15</p>"},{"location":"90DaysOfDevOps/day15/","title":"#90DaysOfDevOps - Linux Commands for DevOps (Actually everyone) - Day 15","text":""},{"location":"90DaysOfDevOps/day15/#linux-commands-for-devops-actually-everyone","title":"Linux Commands for DevOps (Actually everyone)","text":"<p>I mentioned yesterday that we are going to be spending a lot of time in the terminal with some commands to get stuff done.</p> <p>I also mentioned that with our vagrant provisioned VM we can use <code>vagrant ssh</code> and gain access to our box. You will need to be in the same directory as we provisioned it from.</p> <p>For SSH you won't need the username and password, you will only need that if you decide to log in to the Virtual Box console.</p> <p>This is where we want to be as per below:</p> <p></p>"},{"location":"90DaysOfDevOps/day15/#commands","title":"Commands","text":"<p>I cannot cover all the commands here, there are pages and pages of documentation that cover these but also if you are ever in your terminal and you just need to understand options to a specific command we have the <code>man</code> pages short for manual. We can use this to go through each of the commands we touch on during this post to find out more options for each one. We can run <code>man man</code> which will give you the help for manual pages. To escape the man pages you should press <code>q</code> for quit.</p> <p> </p> <p><code>sudo</code> If you are familiar with Windows and the right click <code>run as administrator</code> we can think of <code>sudo</code> as very much this. When you run a command with this command you will be running it as <code>root</code> it will prompt you for the password before running the command.</p> <p></p> <p>For one off jobs like installing applications or services, you might need that <code>sudo command</code> but what if you have several tasks to deal with and you want to live as <code>sudo</code> for a while? This is where you can use <code>sudo su</code> again the same as <code>sudo</code> once entered you will be prompted for your <code>root</code> password. In a test VM like ours, this is fine but I would find it very hard for us to be rolling around as <code>root</code> for prolonged periods, bad things can happen. To get out of this elevated position you simply type in <code>exit</code></p> <p></p> <p>I find myself using <code>clear</code> all the time, the <code>clear</code> command does exactly what it says it is going to clear the screen of all previous commands, putting your prompt to the top and giving you a nice clean workspace. Windows I think is <code>cls</code> in the .mdprompt.</p> <p></p> <p>Let's now look at some commands where we can actually create things within our system and then visualise them in our terminal, first of all, we have <code>mkdir</code> which will allow us to create a folder in our system. With the following command, we can create a folder in our home directory called Day15 <code>mkdir Day15</code></p> <p></p> <p>With <code>cd</code> this allows us to change the directory, so for us to move into our newly created directory we can do this with <code>cd Day15</code> tab can also be used to autocomplete the directory available. If we want to get back to where we started we can use <code>cd ..</code></p> <p></p> <p><code>rmdir</code> allows for us to remove the directory, if we run <code>rmdir Day15</code> then the folder will be removed (note that this will only work if you have nothing in the folder)</p> <p></p> <p>I am sure we have all done it where we have navigated to the depths of our file system to a directory and not known where we are. <code>pwd</code> gives us the printout of the working directory, pwd as much as it looks like password it stands for print working directory.</p> <p></p> <p>We know how to create folders and directories but how do we create files? We can create files using the <code>touch</code> command if we were to run <code>touch Day15</code> this would create a file. Ignore <code>mkdir</code> we are going to see this again later.</p> <p></p> <p><code>ls</code> I can put my house on this, you will use this command so many times, this is going to list all the files and folders in the current directory. Let's see if we can see that file we just created.</p> <p></p> <p>How can we find files on our Linux system? <code>locate</code> is going to allow us to search our file system. If we use <code>locate Day15</code> it will report back the location of the file. The bonus round is that if you know that the file does exist but you get a blank result then run <code>sudo updatedb</code> which will index all the files in the file system then run your <code>locate</code> again. If you do not have <code>locate</code> available to you, you can install it using this command <code>sudo apt install mlocate</code></p> <p></p> <p>What about moving files from one location to another? <code>mv</code> is going to allow you to move your files. Example <code>mv Day15 90DaysOfDevOps</code> will move your file to the 90DaysOfDevOps folder.</p> <p></p> <p>We have moved our file but what if we want to rename it now to something else? We can do that using the <code>mv</code> command again... WOT!!!? yep we can simply use <code>mv Day15 day15</code> to change to upper case or we could use <code>mv day15 AnotherDay</code> to change it altogether, now use <code>ls</code> to check the file.</p> <p></p> <p>Enough is enough, let's now get rid (delete)of our file and maybe even our directory if we have one created. <code>rm</code> simply <code>rm AnotherDay</code> will remove our file. We will also use quite a bit <code>rm -R</code> which will recursively work through a folder or location. We might also use <code>rm -R -f</code> to force the removal of all of those files. Spoiler if you run <code>rm -R -f /</code> add sudo to it and you can say goodbye to your system....!</p> <p></p> <p>We have looked at moving files around but what if I just want to copy files from one folder to another, simply put its very similar to the <code>mv</code> command but we use <code>cp</code> so we can now say <code>cp Day15 Desktop</code></p> <p></p> <p>We have created folders and files but we haven't put any contents into our folder, we can add contents a few ways but an easy way is <code>echo</code> we can also use <code>echo</code> to print out a lot of things in our terminal, I use echo a lot to print out system variables to know if they are set or not at least. we can use <code>echo \"Hello #90DaysOfDevOps\" &gt; Day15</code> and this will add this to our file. We can also append to our file using <code>echo \"Commands are fun!\" &gt;&gt; Day15</code></p> <p></p> <p>Another one of those commands you will use a lot! <code>cat</code> short for concatenate. We can use <code>cat Day15</code> to see the contents inside the file. Great for quickly reading those configuration files.</p> <p></p> <p>If you have a long complex configuration file and you want or need to find something fast in that file vs reading every line then <code>grep</code> is your friend, this will allow us to search your file for a specific word using <code>cat Day15 | grep \"#90DaysOfDevOps\"</code></p> <p></p> <p>If you are like me and you use that <code>clear</code> command a lot then you might miss some of the commands previously ran, we can use <code>history</code> to find out all those commands we have run prior. <code>history -c</code> will remove the history.</p> <p>When you run <code>history</code> and you would like to pick a specific command you can use <code>!3</code> to choose the 3rd command in the list.</p> <p>You are also able to use <code>history | grep \"Command\"</code> to search for something specific.</p> <p>On servers to trace back when was a command executed, it can be useful to append the date and time to each command in the history file.</p> <p>The following system variable controls this behaviour:</p> <pre><code>HISTTIMEFORMAT=\"%d-%m-%Y %T \"\n</code></pre> <p>You can easily add to your bash_profile:</p> <pre><code>echo 'export HISTTIMEFORMAT=\"%d-%m-%Y %T \"' &gt;&gt; ~/.bash_profile\n</code></pre> <p>So as useful to allow the history file to grow bigger:</p> <pre><code>echo 'export HISTSIZE=100000' &gt;&gt; ~/.bash_profile\necho 'export HISTFILESIZE=10000000' &gt;&gt; ~/.bash_profile\n</code></pre> <p></p> <p>Need to change your password? <code>passwd</code> is going to allow us to change our password. Note that when you add your password like this when it is hidden it will not be shown in <code>history</code> however if your command has <code>-p PASSWORD</code> then this will be visible in your <code>history</code>.</p> <p></p> <p>We might also want to add new users to our system, we can do this with <code>useradd</code> we have to add the user using our <code>sudo</code> command, we can add a new user with <code>sudo useradd NewUser</code></p> <p></p> <p>Creating a group again requires <code>sudo</code> and we can use <code>sudo groupadd DevOps</code> then if we want to add our new user to that group we can do this by running <code>sudo usermod -a -G DevOps</code> <code>-a</code> is add and <code>-G</code> is group name.</p> <p></p> <p>How do we add users to the <code>sudo</code> group, this would be a very rare occasion for this to happen but to do this it would be <code>usermod -a -G sudo NewUser</code></p>"},{"location":"90DaysOfDevOps/day15/#permissions","title":"Permissions","text":"<p>read, write and execute are the permissions we have on all of our files and folders on our Linux system.</p> <p>A full list:</p> <ul> <li>0 = None <code>---</code></li> <li>1 = Execute only <code>--X</code></li> <li>2 = Write only <code>-W-</code></li> <li>3 = Write &amp; Execute <code>-WX</code></li> <li>4 = Read Only <code>R--</code></li> <li>5 = Read &amp; Execute <code>R-X</code></li> <li>6 = Read &amp; Write <code>RW-</code></li> <li>7 = Read, Write &amp; Execute <code>RWX</code></li> </ul> <p>You will also see <code>777</code> or <code>775</code> and these represent the same numbers as the list above but each one represents User - Group - Everyone</p> <p>Let's take a look at our file. <code>ls -al Day15</code> you can see the 3 groups mentioned above, user and group have read &amp; write but everyone only has read.</p> <p></p> <p>We can change this using <code>chmod</code> you might find yourself doing this if you are creating binaries a lot on your systems as well and you need to give the ability to execute those binaries. <code>chmod 750 Day15</code> now run <code>ls -al Day15</code> if you want to run this for a whole folder then you can use <code>-R</code> to recursively do that.</p> <p></p> <p>What about changing the owner of the file? We can use <code>chown</code> for this operation, if we wanted to change the ownership of our <code>Day15</code> from user <code>vagrant</code> to <code>NewUser</code> we can run <code>sudo chown NewUser Day15</code> again <code>-R</code> can be used.</p> <p></p> <p>A command that you will come across is <code>awk</code> which comes in real use when you have an output that you only need specific data from. like running <code>who</code> we get lines with information, but maybe we only need the names. We can run <code>who | awk '{print $1}'</code> to get just a list of that first column.</p> <p></p> <p>If you are looking to read streams of data from standard input, then generate and execute command lines; meaning it can take the output of a command and passes it as an argument of another command. <code>xargs</code> is a useful tool for this use case. If for example, I want a list of all the Linux user accounts on the system I can run. <code>cut -d: -f1 &lt; /etc/passwd</code> and get the long list we see below.</p> <p></p> <p>If I want to compact that list I can do so by using <code>xargs</code> in a command like this <code>cut -d: -f1 &lt; /etc/passwd | sort | xargs</code></p> <p></p> <p>I didn't mention the <code>cut</code> command either, this allows us to remove sections from each line of a file. It can be used to cut parts of a line by byte position, character and field. The <code>cut -d \" \" -f 2 list.txt</code> command allows us to remove that first letter we have and just display our numbers. There are so many combinations that can be used here with this command, I am sure I have spent too much time trying to use this command when I could have extracted data quicker manually.</p> <p></p> <p>Also to note if you type a command and you are no longer happy with it and you want to start again just hit control + c and this will cancel that line and start you fresh.</p>"},{"location":"90DaysOfDevOps/day15/#resources","title":"Resources","text":"<ul> <li>Learn the Linux Fundamentals - Part 1</li> <li>Linux for hackers (don't worry you don't need to be a hacker!)</li> </ul> <p>See you on Day16</p> <p>This is a pretty heavy list already but I can safely say that I have used all of these commands in my day to day, be it from an administering Linux servers or on my Linux Desktop, it is very easy when you are in Windows or macOS to navigate the UI but in Linux Servers, they are not there, everything is done through the terminal.</p>"},{"location":"90DaysOfDevOps/day16/","title":"#90DaysOfDevOps - Managing your Linux System, Filesystem & Storage - Day 16","text":""},{"location":"90DaysOfDevOps/day16/#managing-your-linux-system-filesystem-storage","title":"Managing your Linux System, Filesystem &amp; Storage","text":"<p>So far we have had a brief overview of Linux and DevOps and then we got our lab environment set up using Vagrant (Day 14), we then touched on a small portion of commands that will be in your daily toolkit when in the terminal and getting things done (Day 15).</p> <p>Here we are going to look into three key areas of looking after your Linux systems with updates, installing software, understanding what system folders are used for and we will also take a look at storage.</p>"},{"location":"90DaysOfDevOps/day16/#managing-ubuntu-software","title":"Managing Ubuntu &amp; Software","text":"<p>The first thing we are going to look at is how we update our operating system. Most of you will be familiar with this process in a Windows OS and macOS, this looks slightly different on a Linux desktop and server.</p> <p>We are going to be looking at the apt package manager, this is what we are going to use on our Ubuntu VM for updates and software installation.</p> <p>Generally, at least on dev workstations, I run this command to make sure that I have the latest available updates from the central repositories, before any software installation.</p> <p><code>sudo apt-get update</code></p> <p></p> <p>Now we have an updated Ubuntu VM with the latest OS updates installed. We now want to get some software installed here.</p> <p>Let's choose <code>figlet</code> which is a program that generates text banners.</p> <p>If we type <code>figlet</code> in our terminal you are going to see that we do not have it installed on our system.</p> <p></p> <p>You will see from the above though that it does give us some <code>apt</code> install options that we could try. This is because in the default repositories there is a program called figlet. Let's try <code>sudo apt install figlet</code></p> <p></p> <p>We can now use our <code>figlet</code> app as you can see below.</p> <p></p> <p>If we want to remove that or any of our software installations we can also do that via the <code>apt</code> package manager.</p> <p><code>sudo apt remove figlet</code></p> <p></p> <p>There are third party repositories that we can also add to our system, the ones we have access to out of the box are the Ubuntu default repositories.</p> <p>If for example, we wanted to install vagrant on our Ubuntu VM we would not be able to right now and you can see this below on the first command issued. We then add the key to trust the HashiCorp repository, then add the repository to our system.</p> <p></p> <p>Once we have the HashiCorp repository added we can go ahead and run <code>sudo apt install vagrant</code> and get vagrant installed on our system.</p> <p></p> <p>There are so many options when it comes to software installation, different options for package managers, built into Ubuntu we could also use snaps for our software installations.</p> <p>Hopefully, this gives you a feel about how to manage your OS and software installations on Linux.</p>"},{"location":"90DaysOfDevOps/day16/#file-system-explained","title":"File System Explained","text":"<p>Linux is made up of configuration files, if you want to change anything then you change these configuration files.</p> <p>On Windows, you have C: drive and that is what we consider the root. On Linux we have <code>/</code> this is where we are going to find the important folders on our Linux system.</p> <p></p> <ul> <li><code>/bin</code> - Short for binary, the bin folder is where our binaries that your system needs, executables and tools will mostly be found here.</li> </ul> <p></p> <ul> <li><code>/boot</code> - All the files your system needs to boot up. How to boot up, and what drive to boot from.</li> </ul> <p></p> <ul> <li><code>/dev</code> - You can find device information here, this is where you will find pointers to your disk drives <code>sda</code> will be your main OS disk.</li> </ul> <p></p> <ul> <li><code>/etc</code> Likely the most important folder on your Linux system, this is where the majority of your configuration files are.</li> </ul> <p></p> <ul> <li><code>/home</code> - this is where you will find your user folders and files. We have our vagrant user folder. This is where you will find your <code>Documents</code> and <code>Desktop</code> folders that we worked in for the commands section.</li> </ul> <p></p> <ul> <li><code>/lib</code> - We mentioned that <code>/bin</code> is where our binaries and executables live, and <code>/lib</code> is where you will find the shared libraries for those.</li> </ul> <p></p> <ul> <li><code>/media</code> - This is where we will find removable devices.</li> </ul> <p></p> <ul> <li><code>/mnt</code> - This is a temporary mount point. We will cover more here in the next storage section.</li> </ul> <p></p> <ul> <li><code>/opt</code> - Optional software packages. You will notice here that we have some vagrant and virtual box software stored here.</li> </ul> <p></p> <ul> <li><code>/proc</code> - Kernel &amp; process information, similar to <code>/dev</code></li> </ul> <p></p> <ul> <li><code>/root</code> - To gain access you will need to sudo into this folder. The home folder for root.</li> </ul> <p></p> <ul> <li><code>/run</code> -Placeholder for application states.</li> </ul> <p></p> <ul> <li><code>/sbin</code> - Sudo bin, similar to the bin folder but these tools are intended for elevated superuser privileges on the system.</li> </ul> <p></p> <ul> <li><code>/tmp</code> - temporary files.</li> </ul> <p></p> <ul> <li><code>/usr</code> - If we as a standard user have installed software packages it would generally be installed in the <code>/usr/bin</code> location.</li> </ul> <p></p> <ul> <li><code>/var</code> - Our applications get installed in a <code>bin</code> folder. We need somewhere to store all of the log files this is <code>/var</code></li> </ul> <p></p>"},{"location":"90DaysOfDevOps/day16/#storage","title":"Storage","text":"<p>When we come to a Linux system or any system we might want to know the available disks and how much free space we have on those disks. The next few commands will help us identify and use and manage storage.</p> <ul> <li><code>lsblk</code> List Block devices. <code>sda</code> is our physical disk and then <code>sda1, sda2, sda3</code> are our partitions on that disk.</li> </ul> <p></p> <ul> <li><code>df</code> gives us a little more detail about those partitions, total, used and available. You can parse other flags here I generally use <code>df -h</code> to give us a human output of the data.</li> </ul> <p></p> <p>If you were adding a new disk to your system and this is the same in Windows you would need to format the disk in disk management, in the Linux terminal you can do this by using the <code>sudo mkfs -t ext4 /dev/sdb</code> with sdb relating to our newly added disk.</p> <p>We would then need to mount our newly formatted disk so that it was useable. We would do this in our <code>/mnt</code> folder previously mentioned and we would create a directory there with <code>sudo mkdir NewDisk</code> we would then use <code>sudo mount /dev/sdb newdisk</code> to mount the disk to that location.</p> <p>It is also possible that you will need to unmount storage from your system safely vs just pulling it from the configuration. We can do this with <code>sudo umount /dev/sdb</code></p> <p>If you did not want to unmount that disk and you were going to be using this disk for a database or some other persistent use case then you want it to be there when you reboot your system. For this to happen we need to add this disk to our <code>/etc/fstab</code> configuration file for it to persist, if you don't it won't be useable when the machine reboots and you would manually have to go through the above process. The data will still be there on the disk but it won't automount unless you add the configuration to this file.</p> <p>Once you have edited the <code>fstab</code> configuration file you can check your workings with <code>sudo mount -a</code> if no errors then your changes will now be persistent across restarts.</p> <p>We will cover how you would edit a file using a text editor in a future session.</p>"},{"location":"90DaysOfDevOps/day16/#resources","title":"Resources","text":"<ul> <li>Learn the Linux Fundamentals - Part 1</li> <li>Linux for hackers (don't worry you don't need to be a hacker!)</li> </ul> <p>See you on Day17</p>"},{"location":"90DaysOfDevOps/day17/","title":"#90DaysOfDevOps - Text Editors - nano vs vim - Day 17","text":""},{"location":"90DaysOfDevOps/day17/#text-editors-nano-vs-vim","title":"Text Editors - nano vs vim","text":"<p>The majority of your Linux systems are going to be servers and these are not going to have a GUI. I also mentioned in the last session that Linux is mostly made up of configuration files, to make changes you are going to need to be able to edit those configuration files to change anything on the system.</p> <p>There are lots of options out there but I think we should cover probably the two most common terminal text editors. I have used both of these editors and for me, I find <code>nano</code> the easy button when it comes to quick changes but <code>vim</code> has such a broad set of capabilities.</p>"},{"location":"90DaysOfDevOps/day17/#nano","title":"nano","text":"<ul> <li>Not available on every system.</li> <li>Great for getting started.</li> </ul> <p>If you run <code>nano 90DaysOfDevOps.txt</code> we will create a new file with nothing in, from here we can add our text and we have our instructions below for what we want to do with that file.</p> <p></p> <p>We can now use <code>control x + enter</code> and then run <code>ls</code> you can now see our new text file.</p> <p></p> <p>We can now run <code>cat</code> against that file to read our file. We can then use that same <code>nano 90DaysOfDevOps.txt</code> to add additional text or modify your file.</p> <p>For me, nano is super easy when it comes to getting small changes done on configuration files.</p>"},{"location":"90DaysOfDevOps/day17/#vim","title":"vim","text":"<p>Possibly the most common text editor around? A sibling of the UNIX text editor vi from 1976 we get a lot of functionality with vim.</p> <ul> <li>Pretty much supported on every single Linux distribution.</li> <li>Incredibly powerful! You can likely find a full 7-hour course just covering vim.</li> </ul> <p>We can jump into vim with the <code>vim</code> command or if we want to edit our new txt file we could run <code>vim 90DaysOfDevOps.txt</code> but you are going to first see the lack of help menus at the bottom.</p> <p>The first question might be \"How do I exit vim?\" that is going to be <code>escape</code> and if we have not made any changes then it will be <code>:q</code></p> <p></p> <p>You start in <code>normal</code> mode, there are other modes <code>command, normal, visual, insert</code>, if we want to add the text we will need to switch from <code>normal</code> to <code>insert</code> we need to press <code>i</code> if you have added some text and would like to save these changes then you would hit escape and then <code>:wq</code></p> <p></p> <p></p> <p>You can confirm this with the <code>cat</code> command to check you have saved those changes.</p> <p>There is some cool fast functionality with vim that allows you to do menial tasks very quickly if you know the shortcuts which is a lecture in itself. Let's say we have added a list of repeated words and we now need to change that, maybe it's a configuration file and we repeat a network name and now this has changed and we quickly want to change this. I am using the word day for this example.</p> <p></p> <p>Now we want to replace that word with 90DaysOfDevOps, we can do this by hitting <code>ESC</code> and typing <code>:%s/Day/90DaysOfDevOps</code></p> <p></p> <p>The outcome when you hit enter is that the word day is then replaced with 90DaysOfDevOps.</p> <p></p> <p>Copy and Paste was a big eye-opener for me. Copy is not copied it is yanked. we can copy using <code>yy</code> on our keyboard in normal mode. <code>p</code> paste on the same line, <code>P</code> paste on a new line.</p> <p>You can also delete these lines by choosing the number of lines you wish to delete followed by <code>dd</code></p> <p>There is also likely a time you will need to search a file, now we can use <code>grep</code> as mentioned in a previous session but we can also use vim. we can use <code>/word</code> and this will find the first match, to navigate through to the next you will use the <code>n</code> key and so on.</p> <p>For vim this is not even touching the surface, the biggest advice I can give is to get hands-on and use vim wherever possible.</p> <p>A common interview question is what is your favourite text editor in Linux and I would make sure you have at least this knowledge of both so you can answer, it is fine to say nano because it's simple. At least you show competence in understanding what a text editor is. But get hands-on with them to be more proficient.</p> <p>Another pointer to navigate around in vim we can use <code>H,J,K,L</code> as well as our arrow keys.</p>"},{"location":"90DaysOfDevOps/day17/#resources","title":"Resources","text":"<ul> <li>Vim in 100 Seconds</li> <li>Vim tutorial</li> <li>Learn the Linux Fundamentals - Part 1</li> <li>Linux for hackers (don't worry you don't need to be a hacker!)</li> </ul> <p>See you on Day18</p>"},{"location":"90DaysOfDevOps/day18/","title":"#90DaysOfDevOps - SSH & Web Server - Day 18","text":""},{"location":"90DaysOfDevOps/day18/#ssh-web-server","title":"SSH &amp; Web Server","text":"<p>As we have mentioned throughout you are going to most likely be managing lots of remote Linux servers, because of this, you will need to make sure that your connectivity to these remote servers is secure. In this section, we want to cover some of the basics of SSH that everyone should know that will help you with that secure tunnel to your remote systems.</p> <ul> <li>Setting up a connection with SSH</li> <li>Transferring files</li> <li>Create your private key</li> </ul>"},{"location":"90DaysOfDevOps/day18/#ssh-introduction","title":"SSH introduction","text":"<ul> <li>Secure shell</li> <li>Networking Protocol</li> <li>Allows secure communications</li> <li>Can secure any network service</li> <li>Typically used for remote command-line access</li> </ul> <p>In our environment, if you have been following along we have been using SSH already but this was all configured and automated through our vagrant configuration so we only had to run <code>vagrant ssh</code> and we gained access to our remote virtual machine.</p> <p>If our remote machine was not on the same system as our workstation and was in a remote location, maybe a cloud-based system or running in a data centre that we could only access over the internet we would need a secure way of being able to access the system to manage it.</p> <p>SSH provides a secure tunnel between client and server so that nothing can be intercepted by bad actors.</p> <p></p> <p>The server has a server-side SSH service always running and listening on a specific TCP port (22).</p> <p>If we use our client to connect with the correct credentials or SSH key then we gain access to that server.</p>"},{"location":"90DaysOfDevOps/day18/#adding-a-bridged-network-adapter-to-our-system","title":"Adding a bridged network adapter to our system","text":"<p>For us to use this with our current virtual box VM, we need to add a bridged network adapter to our machine.</p> <p>Power down your virtual machine, right-click on your machine within Virtual Box and select settings. In the new window then select networking.</p> <p></p> <p>Now power your machine back on and you will now have an IP address on your local machine. You can confirm this with the <code>IP addr</code> command.</p>"},{"location":"90DaysOfDevOps/day18/#confirming-ssh-server-is-running","title":"Confirming SSH server is running","text":"<p>We know SSH is already configured on our machine as we have been using it with vagrant but we can confirm by running</p> <p><code>sudo systemctl status ssh</code></p> <p></p> <p>If your system does not have the SSH server then you can install it by issuing this command <code>sudo apt install OpenSSH-server</code></p> <p>You then want to make sure that our SSH is allowed if the firewall is running. We can do this with <code>sudo ufw allow ssh</code> this is not required on our configuration as we automated this with our vagrant provisioning.</p>"},{"location":"90DaysOfDevOps/day18/#remote-access-ssh-password","title":"Remote Access - SSH Password","text":"<p>Now that we have our SSH Server listening out on port 22 for any incoming connection requests and we have added the bridged networking we could use putty or an SSH client on our local machine to connect to our system using SSH.</p> <p># PuTTy installation Guide.</p> <p></p> <p>Then hit open, if this is the first time you have connected to this system via this IP address you will get this warning. We know that this is our system so you can choose yes.</p> <p></p> <p>We are then prompted for our username (vagrant) and password (default password - vagrant) Below you will see we are now using our SSH client (Putty) to connect to our machine using username and password.</p> <p></p> <p>At this stage, we are connected to our VM from our remote client and we can issue our commands on our system.</p>"},{"location":"90DaysOfDevOps/day18/#remote-access-ssh-key","title":"Remote Access - SSH Key","text":"<p>The above is an easy way to gain access to your systems however it still relies on username and password, if some malicious actor was to gain access to this information plus the public address or IP of your system then it could be easily compromised. This is where SSH keys are preferred.</p> <p>SSH Keys means that we provide a key pair so that both the client and server know that this is a trusted device.</p> <p>Creating a key is easy. On our local machine (Windows) We can issue the following command in fact if you have an ssh-client installed on any system I believe this same command will work?</p> <p><code>ssh-keygen -t ed25519</code></p> <p>I am not going to get into what <code>ed25519</code> is and means here but you can have a search if you want to learn more about cryptography</p> <p></p> <p>At this point, we have our created SSH key stored in <code>C:\\Users\\micha/.ssh/</code></p> <p>But to link this with our Linux VM we need to copy the key. We can do this by using the <code>ssh-copy-id vagrant@192.168.169.135</code></p> <p>I used Powershell to create my keys on my Windows client but there is no <code>ssh-copy-id</code> available here. There are ways in which you can do this on Windows and a small search online will find you an alternative, but I will just use git bash on my Windows machine to make the copy.</p> <p></p> <p>We can now go back to Powershell to test that our connection now works with our SSH Keys and no password is required.</p> <p><code>ssh vagrant@192.168.169.135</code></p> <p></p> <p>We could secure this further if needed by using a passphrase. We could also go one step further saying that no passwords at all meaning only key pairs over SSH would be allowed. You can make this happen in the following configuration file.</p> <p><code>sudo nano /etc/ssh/sshd_config</code></p> <p>there is a line in here with <code>PasswordAuthentication yes</code> this will be <code>#</code> commented out, you should uncomment and change the yes to no. You will then need to reload the SSH service with <code>sudo systemctl reload sshd</code></p>"},{"location":"90DaysOfDevOps/day18/#setting-up-a-web-server","title":"Setting up a Web Server","text":"<p>Not specifically related to what we have just done with SSH above but I wanted to include this as this is again another task that you might find a little daunting but it really should not be.</p> <p>We have our Linux playground VM and at this stage, we want to add an apache webserver to our VM so that we can host a simple website from it that serves my home network. Note that this web page will not be accessible from the internet, this can be done but it will not be covered here.</p> <p>You might also see this referred to as a LAMP stack.</p> <ul> <li>Linux Operating System</li> <li>Apache Web Server</li> <li>mySQL database</li> <li>PHP</li> </ul>"},{"location":"90DaysOfDevOps/day18/#apache2","title":"Apache2","text":"<p>Apache2 is an open-source HTTP server. We can install apache2 with the following command.</p> <p><code>sudo apt-get install apache2</code></p> <p>To confirm that apache2 is installed correctly we can run <code>sudo service apache2 restart</code></p> <p>Then using the bridged network address from the SSH walkthrough open a browser and go to that address. Mine was <code>http://192.168.169.135/</code></p> <p></p>"},{"location":"90DaysOfDevOps/day18/#mysql","title":"mySQL","text":"<p>MySQL is a database in which we will be storing our data for our simple website. To get MySQL installed we should use the following command <code>sudo apt-get install mysql-server</code></p>"},{"location":"90DaysOfDevOps/day18/#php","title":"PHP","text":"<p>PHP is a server-side scripting language, we will use this to interact with a MySQL database. The final installation is to get PHP and dependencies installed using <code>sudo apt-get install php libapache2-mod-php php-mysql</code></p> <p>The first configuration change we want to make out of the box apache is using index.html and we want it to use index.php instead.</p> <p>We are going to use <code>sudo nano /etc/apache2/mods-enabled/dir.conf</code> and we are going to move index.php to the first item in the list.</p> <p></p> <p>Restart the apache2 service <code>sudo systemctl restart apache2</code></p> <p>Now let's confirm that our system is configured correctly for PHP. Create the following file using this command, this will open a blank file in nano.</p> <p><code>sudo nano /var/www/html/90Days.php</code></p> <p>then copy the following and use control + x to exit and save your file.</p> <pre><code>&lt;?php\nphpinfo();\n?&gt;\n</code></pre> <p>Now navigate to your Linux VM IP again with the additional 90Days.php on the end of the URL. <code>http://192.168.169.135/90Days.php</code> you should see something similar to the below if PHP is configured correctly.</p> <p></p>"},{"location":"90DaysOfDevOps/day18/#wordpress-installation","title":"WordPress Installation","text":"<p>I then walked through this tutorial to get WordPress up on our LAMP stack, some commands are shown below if not shown correctly in the walkthrough How to install WordPress on Ubuntu with LAMP</p> <p><code>sudo mysql -u root -p</code></p> <p><code>CREATE DATABASE wordpressdb;</code></p> <p><code>CREATE USER 'admin-user'@'localhost' IDENTIFIED BY 'password';</code></p> <p><code>GRANT ALL PRIVILEGES ON wordpressdb.* TO 'admin-user'@'localhost';</code></p> <p><code>FLUSH PRIVILEGES;</code></p> <p><code>EXIT;</code></p> <p><code>sudo apt install php-curl php-gd php-mbstring php-xml php-xmlrpc php-soap php-intl php-zip</code></p> <p><code>sudo systemctl restart apache2</code></p> <p><code>cd /var/www</code></p> <p><code>sudo curl -O https://wordpress.org/latest.tar.gz</code></p> <p><code>sudo tar -xvf latest.tar.gz</code></p> <p><code>sudo rm latest.tar.gz</code></p> <p>At this point you are in Step 4 in the linked article, you will need to follow the steps to make sure all correct permissions are in place for the WordPress directory.</p> <p>Because this is internal only you do not need to \"generate security keys\" in this step. Move to Step 5 which is changing the Apache configuration to WordPress.</p> <p>Then providing everything is configured correctly you will be able to access via your internal network address and run through the WordPress installation.</p>"},{"location":"90DaysOfDevOps/day18/#resources","title":"Resources","text":"<ul> <li>Client SSH GUI - Remmina</li> <li>The Beginner's guide to SSH</li> <li>Vim in 100 Seconds</li> <li>Vim tutorial</li> <li>Learn the Linux Fundamentals - Part 1</li> <li>Linux for hackers (don't worry you don't need to be a hacker!)</li> </ul> <p>See you on Day19</p>"},{"location":"90DaysOfDevOps/day19/","title":"#90DaysOfDevOps - Automate tasks with bash scripts - Day 19","text":""},{"location":"90DaysOfDevOps/day19/#automate-tasks-with-bash-scripts","title":"Automate tasks with bash scripts","text":"<p>The shell that we are going to use today is the bash but we will cover another shell tomorrow when we dive into ZSH.</p> <p>BASH - Bourne Again Shell</p> <p>We could almost dedicate a whole section of 7 days to shell scripting much like the programming languages, bash gives us the capability of working alongside other automation tools to get things done.</p> <p>I still speak to a lot of people who have set up some complex shell scripts to make something happen and they rely on this script for some of the most important things in the business, I am not saying we need to understand shell/bash scripting for this purpose, this is not the way. But we should learn shell/bash scripting to work alongside our automation tools and for ad-hoc tasks.</p> <p>An example of this that we have used in this section could be the VAGRANTFILE we used to create our VM, we could wrap this into a simple bash script that deleted and renewed this every Monday morning so that we have a fresh copy of our Linux VM every week, we could also add all the software stack that we need on said Linux machine and so on all through this one bash script.</p> <p>I think another thing I am at least hearing is that hands-on scripting questions are becoming more and more apparent in all lines of interviews.</p>"},{"location":"90DaysOfDevOps/day19/#getting-started","title":"Getting started","text":"<p>As with a lot of things we are covering in this whole 90 days, the only real way to learn is through doing. Hands-on experience is going to help soak all of this into your muscle memory.</p> <p>First of all, we are going to need a text editor. On Day 17 we covered probably the two most common text editors and a little on how to use them.</p> <p>Let's get straight into it and create our first shell script.</p> <p><code>touch 90DaysOfDevOps.sh</code></p> <p>Followed by <code>nano 90DaysOfDevOps.sh</code> this will open our new blank shell script in nano. Again you can choose your text editor of choice here.</p> <p>The first line of all bash scripts will need to look something like this <code>#!/usr/bin/bash</code> this is the path to your bash binary.</p> <p>You should however check this in the terminal by running <code>which bash</code> if you are not using Ubuntu then you might also try <code>whereis bash</code> from the terminal.</p> <p>However, you may see other paths listed in already created shell scripts which could include:</p> <ul> <li><code>#!/bin/bash</code></li> <li><code>#!/usr/bin/env bash</code></li> </ul> <p>In the next line in our script, I like to add a comment and add the purpose of the script or at least some information about me. You can do this by using the <code>#</code> This allows us to comment on particular lines in our code and provide descriptions of what the upcoming commands will be doing. I find the more notes the better for the user experience especially if you are sharing this.</p> <p>I sometimes use figlet, a program we installed earlier in the Linux section to create some asci art to kick things off in our scripts.</p> <p></p> <p>All of the commands we have been through earlier in this Linux section (Day15) could be used here as a simple command to test our script.</p> <p>Let's add a simple block of code to our script.</p> <pre><code>mkdir 90DaysOfDevOps\ncd 90DaysOfDevOps\ntouch Day19\nls\n</code></pre> <p>You can then save this and exit your text editor, if we run our script with <code>./90DaysOfDevOps.sh</code> you should get a permission denied message. You can check the permissions of this file using the <code>ls -al</code> command and you can see highlighted we do not have executable rights on this file.</p> <p></p> <p>We can change this using <code>chmod +x 90DaysOfDevOps.sh</code> and then you will see the <code>x</code> meaning we can now execute our script.</p> <p></p> <p>Now we can run our script again using <code>./90DaysOfDevOps.sh</code> after running the script has now created a new directory, changed into that directory and then created a new file.</p> <p></p> <p>Pretty basic stuff but you can start to see hopefully how this could be used to call on other tools as part of ways to make your life easier and automate things.</p>"},{"location":"90DaysOfDevOps/day19/#variables-conditionals","title":"Variables, Conditionals","text":"<p>A lot of this section is a repeat of what we covered when we were learning Golang but I think it's worth us diving in here again.</p> <p>Variables enable us to define once a particular repeated term that is used throughout a potentially complex script.</p> <p>To add a variable you simply add it like this to a clean line in your script.</p> <p><code>challenge=\"90DaysOfDevOps\"</code></p> <p>This way when and where we use <code>$challenge</code> in our code, if we change the variable it will be reflected throughout.</p> <p></p> <p>If we now run our <code>sh</code> script you will see the printout that was added to our script.</p> <p></p> <p>We can also ask for user input that can set our variables using the following:</p> <pre><code>echo \"Enter your name\"\nread name\n</code></pre> <p>This would then define the input as the variable <code>$name</code> We could then use this later on.</p> <p>Maybe we want to find out who we have on our challenge and how many days they have completed, we can define this using <code>if</code> <code>if-else</code> <code>else-if</code> conditionals, this is what we have defined below in our script.</p> <pre><code>#!/bin/bash\n#  ___   ___  ____                   ___   __ ____              ___\n# / _ \\ / _ \\|  _ \\  __ _ _   _ ___ / _ \\ / _|  _ \\  _____   __/ _ \\ _ __  ___\n#| (_) | | | | | | |/ _` | | | / __| | | | |_| | | |/ _ \\ \\ / / | | | '_ \\/ __|\n# \\__, | |_| | |_| | (_| | |_| \\__ \\ |_| |  _| |_| |  __/\\ V /| |_| | |_) \\__ \\\n#   /_/ \\___/|____/ \\__,_|\\__, |___/\\___/|_| |____/ \\___| \\_/  \\___/| .__/|___/\n#                         |___/                                     |_|\n#\n# This script is to demonstrate bash scripting!\n\n# Variables to be defined\n\nChallengeName=#90DaysOfDevOps\nTotalDays=90\n\n# User Input\n\necho \"Enter Your Name\"\nread name\necho \"Welcome $name to $ChallengeName\"\necho \"How Many Days of the $ChallengeName challenge have you completed?\"\nread DaysCompleted\n\nif [ $DaysCompleted -eq 90 ]\nthen\n  echo \"You have finished, well done\"\nelif [ $DaysCompleted -lt 90 ]\nthen\n  echo \"Keep going you are doing great\"\nelse\n  echo \"You have entered the wrong amount of days\"\nfi\n</code></pre> <p>You can also see from the above that we are running some comparisons or checking values against each other to move on to the next stage. We have different options here worth noting.</p> <ul> <li><code>eq</code> - if the two values are equal will return TRUE</li> <li><code>ne</code> - if the two values are not equal will return TRUE</li> <li><code>gt</code> - if the first value is greater than the second value will return TRUE</li> <li><code>ge</code> - if the first value is greater than or equal to the second value will return TRUE</li> <li><code>lt</code> - if the first value is less than the second value will return TRUE</li> <li><code>le</code> - if the first value is less than or equal to the second value will return TRUE</li> </ul> <p>We might also use bash scripting to determine information about files and folders, this is known as file conditions.</p> <ul> <li><code>-d file</code> True if the file is a directory</li> <li><code>-e file</code> True if the file exists</li> <li><code>-f file</code> True if the provided string is a file</li> <li><code>-g file</code> True if the group id is set on a file</li> <li><code>-r file</code> True if the file is readable</li> <li><code>-s file</code> True if the file has a non-zero size</li> </ul> <pre><code>FILE=\"90DaysOfDevOps.txt\"\nif [ -f \"$FILE\" ]\nthen\n  echo \"$FILE is a file\"\nelse\n  echo \"$FILE is not a file\"\nfi\n</code></pre> <p></p> <p>Providing we have that file still in our directory we should get the first echo command back. But if we remove that file then we should get the second echo command.</p> <p></p> <p>You can hopefully see how this can be used to save you time when searching through a system for specific items.</p> <p>I found this amazing repository on GitHub that has what seems to be an endless amount of scripts DevOps Bash Tools</p>"},{"location":"90DaysOfDevOps/day19/#variables","title":"Variables","text":""},{"location":"90DaysOfDevOps/day19/#conditionals","title":"Conditionals","text":""},{"location":"90DaysOfDevOps/day19/#example","title":"Example","text":"<p>Scenario: We have our company called \"90DaysOfDevOps\" and we have been running a while and now it is time to expand the team from 1 person to lots more over the coming weeks, I am the only one so far that knows the onboarding process so we want to reduce that bottleneck by automating some of these tasks.</p> <p>Requirements:</p> <ul> <li>A user can be passed in as a command line argument.</li> <li>A user is created with the name of the command line argument.</li> <li>A password can be parsed as a command line argument.</li> <li>The password is set for the user</li> <li>A message of successful account creation is displayed.</li> </ul> <p>Let's start with creating our shell script with <code>touch create_user.sh</code></p> <p>Before we move on let's also make this executable using <code>chmod +x create_user.sh</code></p> <p>then we can use <code>nano create_user.sh</code> to start editing our script for the scenario we have been set.</p> <p>We can take a look at the first requirement \"A user can be passed in as a command line argument\" we can use the following</p> <pre><code>#! /usr/bin/bash\n\n#A user can be passed in as a command line argument\necho \"$1\"\n</code></pre> <p></p> <p>Go ahead and run this using <code>./create_user.sh Michael</code> replace Michael with your name when you run the script.</p> <p></p> <p>Next up we can take that second requirement \"A user is created with the name of command line argument\" this can be done with the <code>useradd</code> command. The <code>-m</code> option is to create the user home directory as /home/username</p> <pre><code>#! /usr/bin/bash\n\n#A user can be passed in as a command line argument\necho \"$1 user account being created.\"\n\n#A user is created with the name of the command line argument\nsudo useradd -m \"$1\"\n</code></pre> <p>Warning: If you do not provide a user account name then it will error as we have not filled the variable <code>$1</code></p> <p>We can then check this account has been created with the <code>awk -F: '{ print $1}' /etc/passwd</code> command.</p> <p></p> <p>Our next requirement is \"A password can be parsed as a command line argument.\" First of all, we are not going to ever do this in production it is more for us to work through a list of requirements in the lab to understand.</p> <pre><code>#! /usr/bin/bash\n\n#A user can be passed in as a command line argument\necho \"$1 user account being created.\"\n\n#A user is created with the name of the command line argument\nsudo useradd -m \"$1\"\n\n#A password can be parsed as a command line argument.\nsudo chpasswd &lt;&lt;&lt; \"$1\":\"$2\"\n</code></pre> <p>If we then run this script with the two parameters <code>./create_user.sh 90DaysOfDevOps password</code></p> <p>You can see from the below image that we executed our script it created our user and password and then we manually jumped into that user and confirmed with the <code>whoami</code> command.</p> <p></p> <p>The final requirement is \"A message of successful account creation is displayed.\" We already have this in the top line of our code and we can see on the above screenshot that we have a <code>90DaysOfDevOps user account being created</code> shown. This was left from our testing with the <code>$1</code> parameter.</p> <p>Now, this script can be used to quickly onboard and set up new users on to our Linux systems. But maybe instead of a few of the historic people having to work through this and then having to get other people their new usernames or passwords we could add some user input that we have previously covered earlier on to capture our variables.</p> <pre><code>#! /usr/bin/bash\n\necho \"What is your intended username?\"\nread  username\necho \"What is your password\"\nread  password\n\n#A user can be passed in as a command line argument\necho \"$username user account being created.\"\n\n#A user is created with the name of the command line argument\nsudo useradd -m $username\n\n#A password can be parsed as a command line argument.\nsudo chpasswd &lt;&lt;&lt; $username:$password\n</code></pre> <p>With the steps being more interactive,</p> <p></p> <p>Just to finish this off maybe we do want to output a successful output to say that our new user account has finished being created.</p> <p></p> <p>One thing I did notice was that we are displaying the password on our input we can hide this by using the <code>-s</code> flag in the line of code <code>read -s password</code></p> <p></p> <p>If you do want to delete the user you have created for lab purposes then you can do that with <code>sudo userdel test_user</code></p> <p>Example Script</p> <p>Once again I am not saying this is going to be something that you do create in your day to day but it was something I thought of that would highlight the flexibility of what you could use shell scripting for.</p> <p>Think about any repeatable tasks that you do every day or week or month and how could you better automate that, first option is likely going to be using a bash script before moving into more complex territory.</p> <p>I have created a very simple bash file that helps me spin up a Kubernetes cluster using minikube on my local machine along with data services and Kasten K10 to help demonstrate the requirements and needs around data management. Project Pace But I did not feel this appropriate to raise here as we have not covered Kubernetes yet.</p>"},{"location":"90DaysOfDevOps/day19/#resources","title":"Resources","text":"<ul> <li>Bash in 100 seconds</li> <li>Bash script with practical examples - Full Course</li> <li>Client SSH GUI - Remmina</li> <li>The Beginner's guide to SSH</li> <li>Vim in 100 Seconds</li> <li>Vim tutorial</li> <li>Learn the Linux Fundamentals - Part 1</li> <li>Linux for hackers (don't worry you don't need to be a hacker!)</li> </ul> <p>See you on Day20</p>"},{"location":"90DaysOfDevOps/day20/","title":"#90DaysOfDevOps - Dev workstation setup - All the pretty things - Day 20","text":""},{"location":"90DaysOfDevOps/day20/#dev-workstation-setup-all-the-pretty-things","title":"Dev workstation setup - All the pretty things","text":"<p>Not to be confused with us setting Linux servers up this way but I wanted to also show off the choice and flexibility that we have within the Linux desktop.</p> <p>I have been using a Linux Desktop for almost a year now and I have it configured just the way I want from a look and feel perspective. Using our Ubuntu VM on Virtual Box we can run through some of the customisations I have made to my daily driver.</p> <p>I have put together a YouTube video walking through the rest as some people might be able to better follow along:</p> <p></p> <p>Out of the box, our system will look something like the below:</p> <p></p> <p>We can also see our default bash shell below,</p> <p></p> <p>A lot of this comes down to dotfiles something we will cover in this final Linux session of the series.</p>"},{"location":"90DaysOfDevOps/day20/#dotfiles","title":"dotfiles","text":"<p>First up I want to dig into dotfiles, I have said on a previous day that Linux is made up of configuration files. These dotfiles are configuration files for your Linux system and applications.</p> <p>I will also add that dotfiles are not just used to customise and make your desktop look pretty, there are also dotfile changes and configurations that will help you with productivity.</p> <p>As I mentioned many software programs store their configurations in these dotfiles. These dotfiles assist in managing functionality.</p> <p>Each dotfile starts with a <code>.</code> You can probably guess where the naming came from?</p> <p>So far we have been using bash as our shell which means you will have a .bashrc and .bash_profile in our home folder. You can see below a few dotfiles we have on our system.</p> <p></p> <p>We are going to be changing our shell, so we will later be seeing a new <code>.zshrc</code> configuration dotfile.</p> <p>But now you know if we refer to dotfiles you know they are configuration files. We can use them to add aliases to our command prompt as well as paths to different locations. Some people publish their dotfiles so they are publicly available. You will find mine here on my GitHub MichaelCade/dotfiles here you will find my custom <code>.zshrc</code> file, my terminal of choice is terminator which also has some configuration files in the folder and then also some background options.</p>"},{"location":"90DaysOfDevOps/day20/#zsh","title":"ZSH","text":"<p>As I mentioned throughout our interactions so far we have been using a bash shell the default shell with Ubuntu. ZSH is very similar but it does have some benefits over bash.</p> <p>Zsh has features like interactive Tab completion, automated file searching, regex integration, advanced shorthand for defining command scope, and a rich theme engine.</p> <p>We can use our <code>apt</code> package manager to get zsh installed on our system. Let's go ahead and run <code>sudo apt install zsh</code> from our bash terminal. I am going to do this from within the VM console vs being connected over SSH.</p> <p>When the installation command is complete you can run <code>zsh</code> inside your terminal, this will then start a shell configuration script.</p> <p></p> <p>I selected <code>1</code> to the above question and now we have some more options.</p> <p></p> <p>You can see from this menu that we can make some out of the box edits to make ZSH configured to our needs.</p> <p>If you exit the wizard with a <code>0</code> and then use the <code>ls -al | grep .zshrc</code> you should see we have a new configuration file.</p> <p>Now we want to make zsh our default shell every time we open our terminal, we can do this by running the following command to change our shell <code>chsh -s $(which zsh)</code> we then need to log out and back in again for the changes to take place.</p> <p>When you log back and open a terminal it should look something like this. We can also confirm our shell has now been changed over by running <code>which $SHELL</code></p> <p></p> <p>I generally perform this step on each Ubuntu desktop I spin up and find in general without going any further that the zsh shell is a little faster than bash.</p>"},{"location":"90DaysOfDevOps/day20/#ohmyzsh","title":"OhMyZSH","text":"<p>Next up we want to make things look a little better and also add some functionality to help us move around within the terminal.</p> <p>OhMyZSH is a free and open source framework for managing your zsh configuration. There are lots of plugins, themes and other things that just make interacting with the zsh shell a lot nicer.</p> <p>You can find out more about ohmyzsh</p> <p>Let's get Oh My ZSH installed, we have a few options with <code>curl</code> <code>wget</code> or <code>fetch</code> we have the first two available on our system but I will lead with <code>curl</code></p> <p><code>sh -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"</code></p> <p>When you have run the above command you should see some output like the below.</p> <p></p> <p>Now we can move on to start putting a theme in for our experience, there are well over 100 bundled with Oh My ZSH but my go-to for all of my applications and everything is the Dracula theme.</p> <p>I also want to add that these two plugins are a must when using Oh My ZSH.</p> <p><code>git clone https://github.com/zsh-users/zsh-autosuggestions.git $ZSH_CUSTOM/plugins/zsh-autosuggestions</code></p> <p><code>git clone https://github.com/zsh-users/zsh-syntax-highlighting.git $ZSH_CUSTOM/plugins/zsh-syntax-highlighting</code></p> <p><code>nano ~/.zshrc</code></p> <p>edit the plugins to now include <code>plugins=(git zsh-autosuggestions zsh-syntax-highlighting)</code></p>"},{"location":"90DaysOfDevOps/day20/#gnome-extensions","title":"Gnome Extensions","text":"<p>I also use Gnome extensions, and in particular the list below</p> <p>Gnome extensions</p> <pre><code>- Caffeine\n- CPU Power Manager\n- Dash to Dock\n- Desktop Icons\n- User Themes\n</code></pre>"},{"location":"90DaysOfDevOps/day20/#software-installation","title":"Software Installation","text":"<p>A short list of the programs I install on the machine using <code>apt</code></p> <pre><code>- VSCode\n- azure-cli\n- containerd.io\n- docker\n- docker-ce\n- google-cloud-sdk\n- insomnia\n- packer\n- terminator\n- terraform\n- vagrant\n</code></pre>"},{"location":"90DaysOfDevOps/day20/#dracula-theme","title":"Dracula theme","text":"<p>This site is the only theme I am using at the moment. Looks clear, and clean and everything looks great. Dracula Theme It also has you covered when you have lots of other programs you use on your machine.</p> <p>From the link above we can search for zsh on the site and you will find at least two options.</p> <p>Follow the instructions listed to install either manually or using git. Then you will need to finally edit your <code>.zshrc</code> configuration file as per below.</p> <p></p> <p>You are next going to want the Gnome Terminal Dracula theme with all instructions available here as well.</p> <p>It would take a long time for me to document every step so I created a video walkthrough of the process. (Click on the image below)</p> <p></p> <p>If you made it this far, then we have now finished our Linux section of the #90DaysOfDevOps. Once again I am open to feedback and additions to resources here.</p> <p>I also thought on this it was easier to show you a lot of the steps through video vs writing them down here, what do you think about this? I do have a goal to work back through these days and where possible create video walkthroughs to add in and better maybe explain and show some of the things we have covered. What do you think?</p>"},{"location":"90DaysOfDevOps/day20/#resources","title":"Resources","text":"<ul> <li>Bash in 100 seconds</li> <li>Bash script with practical examples - Full Course</li> <li>Client SSH GUI - Remmina</li> <li>The Beginner's guide to SSH</li> <li>Vim in 100 Seconds</li> <li>Vim tutorial</li> <li>Learn the Linux Fundamentals - Part 1</li> <li>Linux for hackers (don't worry you don't need to be a hacker!)</li> </ul> <p>Tomorrow we start our 7 days of diving into Networking, we will be looking to give ourselves the foundational knowledge and understanding of Networking around DevOps.</p> <p>See you on Day21</p>"},{"location":"90DaysOfDevOps/day21/","title":"#90DaysOfDevOps - The Big Picture: DevOps and Networking - Day 21","text":""},{"location":"90DaysOfDevOps/day21/#the-big-picture-devops-and-networking","title":"The Big Picture: DevOps and Networking","text":"<p>As with all sections, I am using open and free training materials and a lot of the content can be attributed to others. In the case of the networking section a large majority of the content shown is from Practical Networking's free Networking Fundamentals series.  It is mentioned in the resources as well as a link but it's appropriate to highlight this as from a community point of view, I have leveraged this course to help myself understand more about particular areas of technologies. This repository is a repository for my note taking and enabling the community to hopefully benefit from this and the listed resources. </p> <p>Welcome to Day 21! We are going to be getting into Networking over the next 7 days, Networking and DevOps are the overarching themes but we will need to get into some of the networking fundamentals as well.</p> <p>Ultimately as we have said previously DevOps is about a culture and process change within your organisation this as we have discussed can be Virtual Machines, Containers, or Kubernetes but it can also be the network, If we are using those DevOps principles for our infrastructure that has to include the network more to the point from a DevOps point of view you also need to know about the network as in the different topologies and networking tools and stacks that we have available.</p> <p>I would argue that we should have our networking devices configured using infrastructure as code and have everything automated like we would our virtual machines, but to do that we have to have a good understanding of what we are automating.</p>"},{"location":"90DaysOfDevOps/day21/#what-is-netdevops-network-devops","title":"What is NetDevOps | Network DevOps?","text":"<p>You may also hear the terms Network DevOps or NetDevOps. Maybe you are already a Network engineer and have a great grasp on the network components within the infrastructure you understand the elements used around networking such as DHCP, DNS, NAT etc. You will also have a good understanding of the hardware or software-defined networking options, switches, routers etc.</p> <p>But if you are not a network engineer then we probably need to get foundational knowledge across the board in some of those areas so that we can understand the end goal of Network DevOps.</p> <p>But in regards to those terms, we can think of NetDevOps or Network DevOps as applying the DevOps Principles and Practices to the network, applying version control and automation tools to the network creation, testing, monitoring, and deployments.</p> <p>If we think of Network DevOps as having to require automation, we mentioned before about DevOps breaking down the silos between teams. If the networking teams do not change to a similar model and process then they become the bottleneck or even the failure overall.</p> <p>Using the automation principles around provisioning, configuration, testing, version control and deployment is a great start. Automation is overall going to enable speed of deployment, stability of the networking infrastructure and consistent improvement as well as the process being shared across multiple environments once they have been tested. Such as a fully tested Network Policy that has been fully tested on one environment can be used quickly in another location because of the nature of this being in code vs a manually authored process which it might have been before. A really good viewpoint and outline of this thinking can be found here. Network DevOps</p>"},{"location":"90DaysOfDevOps/day21/#networking-the-basics","title":"Networking The Basics","text":"<p>Let's forget the DevOps side of things to begin with here and we now need to look very briefly into some of the Networking fundamentals.</p>"},{"location":"90DaysOfDevOps/day21/#network-devices","title":"Network Devices","text":"<p>If you prefer this content in video form, check out these videos from Practical Networking:</p> <ul> <li>Network Devices - Hosts, IP Addresses, Networks - Networking Fundamentals - Lesson 1a</li> <li>Network Devices - Hub, Bridge, Switch, Router - Networking Fundamentals - Lesson 1b </li> </ul> <p>Host are any devices which send or receive traffic.</p> <p></p> <p>IP Address the identity of each host.</p> <p></p> <p>Network is what transports traffic between hosts. If we did not have networks there would be a lot of manual movement of data!</p> <p>A logical group of hosts which require similar connectivity.</p> <p></p> <p>Switches facilitate communication within a network. A switch forwards data packets between hosts. A switch sends packets directly to hosts.</p> <ul> <li>Network: A Grouping of hosts which require similar connectivity.</li> <li>Hosts on a Network share the same IP address space.</li> </ul> <p></p> <p>Router facilitates communication between networks. As we said before that a switch looks after communication within a network a router allows us to join these networks together or at least give them access to each other if permitted.</p> <p>A router can provide a traffic control point (security, filtering, redirecting) More and more switches also provide some of these functions now.</p> <p>Routers learn which networks they are attached to. These are known as routes, a routing table is all the networks a router knows about.</p> <p>A router has an IP address in the networks they are attached to. This IP is also going to be each host's way out of their local network also known as a gateway.</p> <p>Routers also create the hierarchy in networks I mentioned earlier.</p> <p></p>"},{"location":"90DaysOfDevOps/day21/#switches-vs-routers","title":"Switches vs Routers","text":"<p>Routing is the process of moving data between networks.</p> <ul> <li>A router is a device whose primary purpose is Routing.</li> </ul> <p>Switching is the process of moving data within networks.</p> <ul> <li>A Switch is a device whose primary purpose is switching.</li> </ul> <p>This is very much a foundational overview of devices as we know there are many different Network Devices such as:</p> <ul> <li>Access Points</li> <li>Firewalls</li> <li>Load Balancers</li> <li>Layer 3 Switches</li> <li>IDS / IPS</li> <li>Proxies</li> <li>Virtual Switches</li> <li>Virtual Routers</li> </ul> <p>Although all of these devices are going to perform Routing and/or Switching.</p> <p>Over the next few days, we are going to get to know a little more about this list.</p> <ul> <li>OSI Model</li> <li>Network Protocols</li> <li>DNS (Domain Name System)</li> <li>NAT</li> <li>DHCP</li> <li>Subnets</li> </ul>"},{"location":"90DaysOfDevOps/day21/#resources","title":"Resources","text":"<ul> <li>Networking Fundamentals</li> <li>Computer Networking full course</li> </ul> <p>See you on Day22</p>"},{"location":"90DaysOfDevOps/day22/","title":"#90DaysOfDevOps - The OSI Model - The 7 Layers - Day 22","text":"<p>The content below comes mostly from Practical Networking's Networking Fundamentals series. If you prefer this content in video form, check out these two videos:</p> <ul> <li>The OSI Model: A Practical Perspective - Layers 1 / 2 / 3</li> <li>The OSI Model: A Practical Perspective - Layers 4 / 5+</li> </ul>"},{"location":"90DaysOfDevOps/day22/#the-osi-model-the-7-layers","title":"The OSI Model - The 7 Layers","text":"<p>The overall purpose of networking as an industry is to allow two hosts to share data. Before networking if I want to get data from this host to this host I'd have to plug something into this host walk it over to the other host and plug it into the other host.</p> <p>Networking allows us to automate this by allowing the host to share data automatically across the wire for these hosts to do this they must follow a set of rules.</p> <p>This is no different than any language. English has a set of rules that two English speakers must follow. Spanish has its own set of rules. French has its own set of rules, while networking also has its own set of rules</p> <p>The rules for networking are divided into seven different layers and those layers are known as the OSI model.</p>"},{"location":"90DaysOfDevOps/day22/#introduction-to-the-osi-model","title":"Introduction to the OSI Model","text":"<p>The OSI Model (Open Systems Interconnection Model) is a framework used to describe the functions of a networking system. The OSI model characterises computing functions into a universal set of rules and requirements to support interoperability between different products and software. In the OSI reference model, the communications between a computing system are split into seven different abstraction layers: Physical, Data Link, Network, Transport, Session, Presentation, and Application.</p> <p></p>"},{"location":"90DaysOfDevOps/day22/#physical","title":"Physical","text":"<p>Layer 1 in the OSI model and this is known as physical, the premise of being able to get data from one host to another through a means be it physical cable or we could also consider Wi-Fi in this layer as well. We might also see some more legacy hardware seen here around hubs and repeaters to transport the data from one host to another.</p> <p></p>"},{"location":"90DaysOfDevOps/day22/#data-link","title":"Data Link","text":"<p>Layer 2, the data link enables a node to node transfer where data is packaged into frames. There is also a level of error correcting that might have occurred at the physical layer. This is also where we introduce or first see MAC addresses.</p> <p>This is where we see the first mention of switches that we covered on our first day of networking on Day 21</p> <p></p>"},{"location":"90DaysOfDevOps/day22/#network","title":"Network","text":"<p>You have likely heard the term layer 3 switches or layer 2 switches. In our OSI model Layer 3, the Network has a goal of an end to end delivery, this is where we see our IP addresses also mentioned in the first-day overview.</p> <p>Routers and hosts exist at layer 3, remember the router is the ability to route between multiple networks. Anything with an IP could be considered Layer 3.</p> <p></p> <p>So why do we need addressing schemes on both Layers 2 and 3? (MAC Addresses vs IP Addresses)</p> <p>If we think about getting data from one host to another, each host has an IP address but there are several switches and routers in between. Each of the devices has that layer 2 MAC address.</p> <p>The layer 2 MAC address will go from host to switch/router only, it is focused on hops whereas the layer 3 IP addresses will stay with that packet of data until it reaches its end host. (End to End)</p> <p>IP Addresses - Layer 3 = End to End Delivery</p> <p>MAC Addresses - Layer 2 = Hop to Hop Delivery</p> <p>Now there is a network protocol that we will get into but not today called ARP(Address Resolution Protocol) which links our Layer3 and Layer2 addresses.</p>"},{"location":"90DaysOfDevOps/day22/#transport","title":"Transport","text":"<p>Service to Service delivery, Layer 4 is there to distinguish data streams. In the same way that Layer 3 and Layer 2 both had their addressing schemes, in Layer 4 we have ports.</p> <p></p>"},{"location":"90DaysOfDevOps/day22/#session-presentation-application","title":"Session, Presentation, Application","text":"<p>The distinction between Layers 5,6,7 is or had become somewhat vague.</p> <p>It is worth looking at the TCP IP Model to get a more recent understanding.</p> <p>Let's now try and explain what's happening when hosts are communicating with each other using this networking stack. This host has an application that's going to generate data that is meant to be sent to another host.</p> <p>The source host is going to go through is what's known as the encapsulation process. That data will be first sent to layer 4.</p> <p>Layer 4 is going to add a header to that data which can facilitate the goal of layer 4 which is service to service delivery. This is going to be a port using either TCP or UDP. It is also going to include the source port and destination port.</p> <p>This may also be known as a segment (Data and Port)</p> <p>This segment is going to be passed down the OSI stack to layer 3, the network layer, and the network layer is going to add another header to this data. This header is going to facilitate the goal of layer 3 which is the end to end delivery meaning in this header you will have a source IP address and a destination IP, the header plus data may also be referred to as a packet.</p> <p>Layer 3 will then take that packet and hand it off to layer 2, layer 2 will once again add another header to that data to accomplish layer 2's goal of hop to hop delivery meaning this header will include a source and destination mac address. This is known as a frame when you have the layer 2 header and data.</p> <p>That frame then gets converted into ones and zeros and sent over the Layer 1 Physical cable or wifi.</p> <p></p> <p>I did mention above the naming for each layer of header plus data but decided to draw this out as well.</p> <p></p> <p>The Application sending the data is being sent somewhere so the receiving is somewhat in reverse to get that back up the stack and into the receiving host.</p> <p></p>"},{"location":"90DaysOfDevOps/day22/#resources","title":"Resources","text":"<ul> <li>Networking Fundamentals</li> <li>Computer Networking full course</li> </ul> <p>See you on Day23</p>"},{"location":"90DaysOfDevOps/day23/","title":"#90DaysOfDevOps - Network Protocols - Day 23","text":"<p>The content below comes mostly from Practical Networking's Networking Fundamentals series. If you prefer this content in video form, check out this video:</p> <ul> <li>Network Protocols - ARP, FTP, SMTP, HTTP, SSL, TLS, HTTPS, DNS, DHCP</li> </ul>"},{"location":"90DaysOfDevOps/day23/#network-protocols","title":"Network Protocols","text":"<p>A set of rules and messages that form a standard. An Internet Standard.</p> <ul> <li>ARP - Address Resolution Protocol</li> </ul> <p>If you want to get really into the weeds on ARP you can read the Internet Standard here. RFC 826</p> <p>Connects IP addresses to fixed physical machine addresses, also known as MAC addresses across a layer 2 network.</p> <p></p> <ul> <li>FTP - File Transfer Protocol</li> </ul> <p>Allows for the transfer of files from source to destination. Generally, this process is authenticated but there is the ability if configured to use anonymous access. You will more frequently now see FTPS which provides SSL/TLS connectivity to FTP servers from the client for better security. This protocol would be found in the Application layer of the OSI Model.</p> <p></p> <ul> <li>SMTP - Simple Mail Transfer Protocol</li> </ul> <p>Used for email transmission, mail servers use SMTP to send and receive mail messages. You will still find even with Microsoft 365 that the SMTP protocol is used for the same purpose.</p> <p></p> <ul> <li>HTTP - Hyper Text Transfer Protocol</li> </ul> <p>HTTP is the foundation of the internet and browsing content. Giving us the ability to easily access our favourite websites. HTTP is still heavily used but HTTPS is more so used or should be used on most of your favourite sites.</p> <p></p> <ul> <li>SSL - Secure Sockets Layer | TLS - Transport Layer Security</li> </ul> <p>TLS has taken over from SSL, TLS is a Cryptographic Protocol that provides secure communications over a network. It can and will be found in the mail, Instant Messaging and other applications but most commonly it is used to secure HTTPS.</p> <p></p> <ul> <li>HTTPS - HTTP secured with SSL/TLS</li> </ul> <p>An extension of HTTP, used for secure communications over a network, HTTPS is encrypted with TLS as mentioned above. The focus here was to bring authentication, privacy and integrity whilst data is exchanged between hosts.</p> <p></p> <ul> <li>DNS - Domain Name System</li> </ul> <p>The DNS is used to map human-friendly domain names for example we all know google.com but if you were to open a browser and put in 8.8.8.8 you would get Google as we pretty much know it. However good luck trying to remember all of the IP addresses for all of your websites where some of them we even use google to find information.</p> <p>This is where DNS comes in, it ensures that hosts, services and other resources are reachable.</p> <p>On all hosts, if they require internet connectivity then they must have DNS to be able to resolve those domain names. DNS is an area you could spend Days and Years on learning. I would also say from experience that DNS is mostly the common cause of all errors when it comes to Networking. Not sure if a Network engineer would agree there though.</p> <p></p> <ul> <li>DHCP - Dynamic Host Configuration Protocol</li> </ul> <p>We have discussed a lot about protocols that are required to make our hosts work, be it accessing the internet or transferring files between each other.</p> <p>There are 4 things that we need on every host for it to be able to achieve both of those tasks.</p> <ul> <li>IP Address</li> <li>Subnet Mask</li> <li>Default Gateway</li> <li>DNS</li> </ul> <p>We have covered IP address being a unique address for your host on the network it resides, we can think of this as our house number.</p> <p>Subnet mask we will cover shortly, but you can think of this as postcode or zip code.</p> <p>A default gateway is the IP of our router generally on our network providing us with that Layer 3 connectivity. You could think of this as the single road that allows us out of our street.</p> <p>Then we have DNS as we just covered to help us convert complicated public IP addresses to more suitable and rememberable domain names. Maybe we can think of this as the giant sorting office to make sure we get the right post.</p> <p>As I said each host requires these 4 things, if you have 1000 or 10,000 hosts then that is going to take you a very long time to determine each one of these individually. This is where DHCP comes in and allows you to determine a scope for your network and then this protocol will distribute to all available hosts in your network.</p> <p>Another example is you head into a coffee shop, grab a coffee and sit down with your laptop or your phone let's call that your host. You connect your host to the coffee shop WiFi and you gain access to the internet, messages and mail start pinging through and you can navigate web pages and social media. When you connected to the coffee shop WiFi your machine would have picked up a DHCP address either from a dedicated DHCP server or most likely from the router also handling DHCP.</p> <p></p>"},{"location":"90DaysOfDevOps/day23/#subnetting","title":"Subnetting","text":"<p>A subnet is a logical subdivision of an IP network.</p> <p>Subnets break large networks into smaller, more manageable networks that run more efficiently.</p> <p>Each subnet is a logical subdivision of the bigger network. Connected devices with enough subnet share common IP address identifiers, enabling them to communicate with each other.</p> <p>Routers manage communication between subnets.</p> <p>The size of a subnet depends on the connectivity requirements and the network technology used.</p> <p>An organisation is responsible for determining the number and size of the subnets within the limits of address space available, and the details remain local to that organisation. Subnets can also be segmented into even smaller subnets for things like Point to Point links, or subnetworks supporting a few devices.</p> <p>Among other advantages, segmenting large networks into subnets enable IP address reallocation and relieves network congestion, streamlining, network communication and efficiency.</p> <p>Subnets can also improve network security. If a section of a network is compromised, it can be quarantined, making it difficult for bad actors to move around the larger network.</p> <p></p>"},{"location":"90DaysOfDevOps/day23/#resources","title":"Resources","text":"<ul> <li>Networking Fundamentals</li> <li>Subnetting Mastery</li> <li>Computer Networking full course</li> </ul> <p>See you on Day 24</p>"},{"location":"90DaysOfDevOps/day24/","title":"#90DaysOfDevOps - Network Automation - Day 24","text":""},{"location":"90DaysOfDevOps/day24/#network-automation","title":"Network Automation","text":""},{"location":"90DaysOfDevOps/day24/#basics-of-network-automation","title":"Basics of network automation","text":"<p>Primary drivers for Network Automation</p> <ul> <li>Achieve Agility</li> <li>Reduce Cost</li> <li>Eliminate Errors</li> <li>Ensure Compliance</li> <li>Centralised Management</li> </ul> <p>The automation adoption process is specific to each business. There's no one size fits all when it comes to deploying automation, the ability to identify and embrace the approach that works best for your organisation is critical in advancing towards maintaining or creating a more agile environment, the focus should always be on business value and end-user experience. (We said something similar right at the start in regards to the whole of DevOps and the culture change and the automated process that this brings)</p> <p>To break this down you would need to identify how the task or process that you're trying to automate is going to achieve and improve the end-user experience or business value whilst following a step-by-step systematic approach.</p> <p>\"If you don't know where you are going, any road will take you there.\"</p> <p>Have a framework or design structure that you're trying to achieve know what your end goal is and then work step by step towards achieving that goal measuring the automation success at various stages based on the business outcomes.</p> <p>Build concepts modelled around existing applications there's no need to design the concepts around automation in a bubble because they need to be applied to your application, your service, and your infrastructure, so begin to build the concepts and model them around your existing infrastructure, you're existing applications.</p>"},{"location":"90DaysOfDevOps/day24/#approach-to-networking-automation","title":"Approach to Networking Automation","text":"<p>We should identify the tasks and perform a discovery on network change requests so that you have the most common issues and problems to automate a solution to.</p> <ul> <li>Make a list of all the change requests and workflows that are currently being addressed manually.</li> <li>Determine the most common, time-consuming and error-prone activities.</li> <li>Prioritise the requests by taking a business-driven approach.</li> <li>This is the framework for building an automation process, what must be automated and what must not.</li> </ul> <p>We should then divide tasks and analyse how different network functions work and interact with each other.</p> <ul> <li>The infrastructure/Network team receives change tickets at multiple layers to deploy applications.</li> <li>Based on Network services, divide them into different areas and understand how they interact with each other.</li> <li>Application Optimisation</li> <li>ADC (Application Delivery Controller)</li> <li>Firewall</li> <li>DDI (DNS, DHCP, IPAM etc)</li> <li>Routing</li> <li>Others</li> <li>Identify various dependencies to address business and cultural differences and bring in cross-team collaboration.</li> </ul> <p>Reusable policies, define and simplify reusable service tasks, processes and input/outputs.</p> <ul> <li>Define offerings for various services, processes and input/outputs.</li> <li>Simplifying the deployment process will reduce the time to market for both new and existing workloads.</li> <li>Once you have a standard process, it can be sequenced and aligned to individual requests for a multi-threaded approach and delivery.</li> </ul> <p>Combine the policies with business-specific activities. How does implementing this policy help the business? Saves time? Saves Money? Provides a better business outcome?</p> <ul> <li>Ensure that service tasks are interoperable.</li> <li>Associate the incremental service tasks so that they align to create business services.</li> <li>Allow for the flexibility to associate and re-associate service tasks on demand.</li> <li>Deploy Self-Service capabilities and pave the way for improved operational efficiency.</li> <li>Allow for the multiple technology skillsets to continue to contribute with oversight and compliance.</li> </ul> <p>Iterate on the policies and process, adding and improving while maintaining availability and service.</p> <ul> <li>Start small by automating existing tasks.</li> <li>Get familiar with the automation process, so that you can identify other areas that can benefit from automation.</li> <li>iterate your automation initiatives, adding agility incrementally while maintaining the required availability.</li> <li>Taking an incremental approach paves the way for success!</li> </ul> <p>Orchestrate the network service!</p> <ul> <li>Automation of the deployment process is required to deliver applications rapidly.</li> <li>Creating an agile service environment requires different elements to be managed across technology skillsets.</li> <li>Prepare for an end to end orchestration that provides for control over automation and the order of deployments.</li> </ul>"},{"location":"90DaysOfDevOps/day24/#network-automation-tools","title":"Network Automation Tools","text":"<p>The good news here is that for the most part, the tools we use here for Network automation are generally the same that we will use for other areas of automation or what we have already covered so far or what we will cover in future sessions.</p> <p>Operating System - As I have throughout this challenge, I am focusing on doing most of my learning with a Linux OS, those reasons were given in the Linux section but almost all of the tooling that we will touch albeit cross-OS platforms maybe today they all started as Linux based applications or tools, to begin with.</p> <p>Integrated Development Environment (IDE) - Again not much to say here other than throughout I would suggest Visual Studio Code as your IDE, based on the extensive plugins that are available for so many different languages.</p> <p>Configuration Management - We have not got to the Configuration management section yet, but it is very clear that Ansible is a favourite in this area for managing and automating configurations. Ansible is written in Python but you do not need to know Python.</p> <ul> <li>Agentless</li> <li>Only requires SSH</li> <li>Large Support Community</li> <li>Lots of Network Modules</li> <li>Push only model</li> <li>Configured with YAML</li> <li>Open Source!</li> </ul> <p>Link to Ansible Network Modules</p> <p>We will also touch on Ansible Tower in the configuration management section, see this as the GUI front end for Ansible.</p> <p>CI/CD - Again we will cover more about the concepts and tooling around this but it's important to at least mention here as this spans not only networking but all provisioning of service and platform.</p> <p>In particular, Jenkins provides or seems to be a popular tool for Network Automation.</p> <ul> <li>Monitors git repository for changes and then initiates them.</li> </ul> <p>Version Control - Again something we will dive deeper into later on.</p> <ul> <li>Git provides version control of your code on your local device - Cross-Platform</li> <li>GitHub, GitLab, BitBucket etc are online websites where you define your repositories and upload your code.</li> </ul> <p>Language | Scripting - Something we have not covered here is Python as a language, I decided to dive into Go instead as the programming language based on my circumstances, I would say that it was a close call between Golang and Python and Python it seems to be the winner for Network Automation.</p> <ul> <li>Nornir is something to mention here, an automation framework written in Python. This seems to take the role of Ansible but specifically around Network Automation. Nornir documentation</li> </ul> <p>Analyse APIs - Postman is a great tool for analysing RESTful APIs. Helps to build, test and modify APIs.</p> <ul> <li>POST &gt;&gt;&gt; To create resources objects.</li> <li>GET &gt;&gt;&gt; To retrieve a resources.</li> <li>PUT &gt;&gt;&gt; To create or replace the resources.</li> <li>PATCH &gt;&gt;&gt; To create or update the resources object.</li> <li>Delete &gt;&gt;&gt; To delete a resources</li> </ul> <p>Postman tool Download</p>"},{"location":"90DaysOfDevOps/day24/#other-tools-to-mention","title":"Other tools to mention","text":"<p>Cisco NSO (Network Services Orchestrator)</p> <p>NetYCE - Simplify Network Automation</p> <p>Network Test Automation</p> <p>Over the next 3 days, I am planning to get more hands-on with some of the things we have covered and put some work in around Python and Network automation.</p> <p>We have nowhere near covered all of the networking topics so far but wanted to make this broad enough to follow along and still keep learning from the resources I am adding below.</p>"},{"location":"90DaysOfDevOps/day24/#resources","title":"Resources","text":"<ul> <li>3 Necessary Skills for Network Automation</li> <li>Computer Networking full course</li> <li>Practical Networking</li> <li>Python Network Automation</li> </ul> <p>See you on Day 25</p>"},{"location":"90DaysOfDevOps/day25/","title":"#90DaysOfDevOps - Python for Network Automation - Day 25","text":""},{"location":"90DaysOfDevOps/day25/#python-for-network-automation","title":"Python for Network Automation","text":"<p>Python is the standard language used for automated network operations.</p> <p>Whilst it is not only for network automation it seems to be everywhere when you are looking for resources and as previously mentioned if it's not Python then it's generally Ansible which is written also in Python.</p> <p>I think I have mentioned this already but during the \"Learn a programming language\" section I chose Golang over Python for reasons around my company is developing in Go so that was a good reason for me to learn but if that was not the case then Python would have taken that time.</p> <ul> <li>Readability and ease of use - It seems that Python seems just makes sense. There don't seem to be the requirements around <code>{}</code> in the code to start and end blocks. Couple this with a strong IDE like VS Code you have a pretty easy start when wanting to run some python code.</li> </ul> <p>Pycharm might be another IDE worth mentioning here.</p> <ul> <li>Libraries - The extensibility of Python is the real gold mine here, I mentioned before that this is not just for Network Automation but in fact, there are libraries plenty for all sorts of devices and configurations. You can see the vast amount here PyPi</li> </ul> <p>When you want to download the library to your workstation, then you use a tool called <code>pip</code> to connect to PyPI and download it locally. Network vendors such as Cisco, Juniper, and Arista developed libraries to facilitate access to their devices.</p> <ul> <li>Powerful &amp; Efficient - Remember during the Go days I went through the \"Hello World\" scenario and we went through I think 6 lines of code? In Python it is</li> </ul> <pre><code>print('hello world')\n</code></pre> <p>Put all of the above points together and it should be easy to see why Python is generally mentioned as the de-facto tool when working on automating.</p> <p>I think it's important to note that it's possible that several years back there were scripts that might have interacted with your network devices to maybe automate the backup of configuration or to gather logs and other insights into your devices. The automation we are talking about here is a little different and that's because the overall networking landscape has also changed to suit this way of thinking better and enabled more automation.</p> <ul> <li> <p>Software-Defined Network - SDN Controllers take the responsibility of delivering the control plane configuration to all devices on the network, meaning just a single point of contact for any network changes, no longer having to telnet or SSH into every device and also relying on humans to do this which has a repeatable chance of failure or misconfiguration.</p> </li> <li> <p>High-Level Orchestration - Go up a level from those SDN controllers and this allows for orchestration of service levels then there is the integration of this orchestration layer into your platforms of choice, VMware, Kubernetes, Public Clouds etc.</p> </li> <li> <p>Policy-based management - What do you want to have? What is the desired state? You describe this and the system has all the details on how to figure it out to become the desired state.</p> </li> </ul>"},{"location":"90DaysOfDevOps/day25/#setting-up-the-lab-environment","title":"Setting up the lab environment","text":"<p>Not everyone has access to physical routers, switches and other networking devices.</p> <p>I wanted to make it possible for us to look at some of the tooling pre-mentioned but also get hands-on and learn how to automate the configuration of our networks.</p> <p>When it comes to options there are a few that we can choose from.</p> <ul> <li>GNS3 VM</li> <li>Eve-ng</li> <li>Unimus Not a lab environment but an interesting concept.</li> </ul> <p>We will build our lab out using Eve-ng as mentioned before you can use a physical device but to be honest a virtual environment means that we can have a sandbox environment to test many different scenarios. Plus being able to play with different devices and topologies might be of interest.</p> <p>We are going to do everything on EVE-NG with the community edition.</p>"},{"location":"90DaysOfDevOps/day25/#getting-started","title":"Getting started","text":"<p>The community edition comes in ISO and OVF formats for download</p> <p>We will be using the OVF download but with the ISO there is the option to build out on a bare metal server without the need for a hypervisor.</p> <p></p> <p>For our walkthrough, we will be using VMware Workstation as I have a license via my vExpert but you can equally use VMware Player or any of the other options mentioned in the documentation. Unfortunately we cannot use our previously used Virtual box!</p> <p>This is also where I had an issue with GNS3 with Virtual Box even though supported.</p> <p>Download VMware Workstation Player - FREE</p> <p>VMware Workstation PRO Also noted that there is an evaluation period for free!</p>"},{"location":"90DaysOfDevOps/day25/#installation-on-vmware-workstation-pro","title":"Installation on VMware Workstation PRO","text":"<p>Now we have our hypervisor software downloaded and installed, and we have the EVE-NG OVF downloaded. If you are using VMware Player please let me know if this process is the same.</p> <p>We are now ready to get things configured.</p> <p>Open VMware Workstation and then select <code>file</code> and <code>open</code></p> <p></p> <p>When you download the EVE-NG OVF Image it is going to be within a compressed file. Extract the contents out into its folder so it looks like this.</p> <p></p> <p>Navigate to the location where you downloaded the EVE-NG OVF image and begin the import.</p> <p>Give it a recognisable name and store the virtual machine somewhere on your system.</p> <p></p> <p>When the import is complete increase the number of processors to 4 and the memory allocated to 8 GB. (This should be the case after import with the latest version if not then edit VM settings)</p> <p>Also, make sure the Virtualise Intel VT-x/EPT or AMD-V/RVI checkbox is enabled. This option instructs the VMware workstation to pass the virtualisation flags to the guest OS (nested virtualisation) This was the issue I was having with GNS3 with Virtual Box even though my CPU allows this.</p> <p></p>"},{"location":"90DaysOfDevOps/day25/#power-on-access","title":"Power on &amp; Access","text":"<p>Sidenote &amp; Rabbit hole: Remember I mentioned that this would not work with VirtualBox! Well yeah had the same issue with VMware Workstation and EVE-NG but it was not the fault of the virtualisation platform!</p> <p>I have WSL2 running on my Windows Machine and this seems to remove the capability of being able to run anything nested inside of your environment. I am confused as to why the Ubuntu VM does run as it seems to take out the Intel VT-d virtualisation aspect of the CPU when using WSL2.</p> <p>To resolve this we can run the following command on our Windows machine and reboot the system, note that whilst this is off then you will not be able to use WSL2.</p> <p><code>bcdedit /set hypervisorlaunchtype off</code></p> <p>When you want to go back and use WSL2 then you will need to run this command and reboot.</p> <p><code>bcdedit /set hypervisorlaunchtype auto</code></p> <p>Both of these commands should be run as administrator!</p> <p>Ok back to the show, You should now have a powered-on machine in VMware Workstation and you should have a prompt looking similar to this.</p> <p></p> <p>On the prompt above you can use:</p> <p>username = root password = eve</p> <p>You will then be asked to provide the root password again, this will be used to SSH into the host later on.</p> <p>We then can change the hostname.</p> <p></p> <p>Next, we define a DNS Domain Name, I have used the one below but I am not sure if this will need to be changed later on.</p> <p></p> <p>We then configure networking, I am selecting static so that the IP address given will be persistent after reboots.</p> <p></p> <p>The final step, provide a static IP address from a network that is reachable from your workstation.</p> <p></p> <p>There are some additional steps here where you will have to provide a subnet mask for your network, default gateway and DNS.</p> <p>Once finished it will reboot, when it is back up you can take your static IP address and put this into your browser.</p> <p></p> <p>The default username for the GUI is <code>admin</code> and the password is <code>eve</code> while the default username for SSH is <code>root</code> and the password is <code>eve</code> but this would have been changed if you changed during the setup.</p> <p></p> <p>I chose HTML5 for the console vs native as this will open a new tab in your browser when you are navigating through different consoles.</p> <p>Next up we are going to:</p> <ul> <li>Install the EVE-NG client pack</li> <li>Load some network images into EVE-NG</li> <li>Build a Network Topology</li> <li>Adding Nodes</li> <li>Connecting Nodes</li> <li>Start building Python Scripts</li> <li>Look at telnetlib, Netmiko, Paramiko and Pexpect</li> </ul>"},{"location":"90DaysOfDevOps/day25/#resources","title":"Resources","text":"<ul> <li>Free Course: Introduction to EVE-NG</li> <li>EVE-NG - Creating your first lab</li> <li>3 Necessary Skills for Network Automation</li> <li>Computer Networking full course</li> <li>Practical Networking</li> <li>Python Network Automation</li> </ul> <p>See you on Day 26</p>"},{"location":"90DaysOfDevOps/day26/","title":"#90DaysOfDevOps - Building our Lab - Day 26","text":""},{"location":"90DaysOfDevOps/day26/#building-our-lab","title":"Building our Lab","text":"<p>We are going to continue our setup of our emulated network using EVE-NG and then hopefully get some devices deployed and start thinking about how we can automate the configuration of these devices. On Day 25 we covered the installation of EVE-NG onto our machine using VMware Workstation.</p>"},{"location":"90DaysOfDevOps/day26/#installing-eve-ng-client","title":"Installing EVE-NG Client","text":"<p>There is also a client pack that allows us to choose which application is used when we SSH to the devices. It will also set up Wireshark for packet captures between links. You can grab the client pack for your OS (Windows, macOS, Linux).</p> <p>EVE-NG Client Download</p> <p></p> <p>Quick Tip: If you are using Linux as your client then there is this client pack.</p> <p>The install is straightforward next, next and I would suggest leaving the defaults.</p>"},{"location":"90DaysOfDevOps/day26/#obtaining-network-images","title":"Obtaining network images","text":"<p>This step has been a challenge, I have followed some videos that I will link at the end that links to some resources and downloads for our router and switch images whilst telling us how and where to upload them.</p> <p>It is important to note that I using everything for education purposes. I would suggest downloading official images from network vendors.</p> <p>Blog &amp; Links to YouTube videos</p> <p>How To Add Cisco VIRL vIOS image to Eve-ng</p> <p>Overall the steps here are a little complicated and could be much easier but the above blogs and videos walk through the process of adding the images to your EVE-NG box.</p> <p>I used FileZilla to transfer the qcow2 to the VM over SFTP.</p> <p>For our lab, we need Cisco vIOS L2 (switches) and Cisco vIOS (router)</p>"},{"location":"90DaysOfDevOps/day26/#create-a-lab","title":"Create a Lab","text":"<p>Inside the EVE-NG web interface, we are going to create our new network topology. We will have four switches and one router that will act as our gateway to outside networks.</p> Node IP Address Router 10.10.88.110 Switch1 10.10.88.111 Switch2 10.10.88.112 Switch3 10.10.88.113 Switch4 10.10.88.114"},{"location":"90DaysOfDevOps/day26/#adding-our-nodes-to-eve-ng","title":"Adding our Nodes to EVE-NG","text":"<p>When you first log in to EVE-NG you will see a screen like the below, we want to start by creating our first lab.</p> <p></p> <p>Give your lab a name and the other fields are optional.</p> <p></p> <p>You will be then greeted with a blank canvas to start creating your network. Right-click on your canvas and choose add node.</p> <p>From here you will have a long list of node options, If you have followed along above you will have the two in blue shown below and the others are going to be grey and unselectable.</p> <p></p> <p>We want to add the following to our lab:</p> <ul> <li>1 x Cisco vIOS Router</li> <li>4 x Cisco vIOS Switch</li> </ul> <p>Run through the simple wizard to add them to your lab and it should look something like this.</p> <p></p>"},{"location":"90DaysOfDevOps/day26/#connecting-our-nodes","title":"Connecting our nodes","text":"<p>We now need to add our connectivity between our routers and switches. We can do this quite easily by hovering over the device and seeing the connection icon as per below and then connecting that to the device we wish to connect to.</p> <p></p> <p>When you have finished connecting your environment you may also want to add some way to define physical boundaries or locations using boxes or circles which can also be found in the right-click menu. You can also add text which is useful when we want to define our naming or IP addresses in our labs.</p> <p>I went ahead and made my lab look like the below.</p> <p></p> <p>You will also notice that the lab above is all powered off, we can start our lab by selecting everything and right-clicking and selecting start selected.</p> <p></p> <p>Once we have our lab up and running you will be able to console into each device and you will notice at this stage they are pretty dumb with no configuration. We can add some configuration to each node by copying or creating your own in each terminal.</p> <p>I will leave my configuration in the Networking folder of the repository for reference.</p> Node Configuration Router R1 Switch1 SW1 Switch2 SW2 Switch3 SW3 Switch4 SW4"},{"location":"90DaysOfDevOps/day26/#resources","title":"Resources","text":"<ul> <li>Free Course: Introduction to EVE-NG</li> <li>EVE-NG - Creating your first lab</li> <li>3 Necessary Skills for Network Automation</li> <li>Computer Networking full course</li> <li>Practical Networking</li> <li>Python Network Automation</li> </ul> <p>Most of the examples I am using here as I am not a Network Engineer have come from this extensive book which is not free but I am using some of the scenarios to help understand Network Automation.</p> <ul> <li>Hands-On Enterprise Automation with Python (Book)</li> </ul> <p>See you on Day 27</p>"},{"location":"90DaysOfDevOps/day27/","title":"#90DaysOfDevOps - Getting Hands-On with Python & Network - Day 27","text":""},{"location":"90DaysOfDevOps/day27/#getting-hands-on-with-python-network","title":"Getting Hands-On with Python &amp; Network","text":"<p>In this final section of Networking fundamentals, we are going to cover some automation tasks and tools with our lab environment created on Day 26</p> <p>We will be using an SSH tunnel to connect to our devices from our client vs telnet. The SSH tunnel created between client and device is encrypted. We also covered SSH in the Linux section on Day 18</p>"},{"location":"90DaysOfDevOps/day27/#access-our-virtual-emulated-environment","title":"Access our virtual emulated environment","text":"<p>For us to interact with our switches we either need a workstation inside the EVE-NG network or you can deploy a Linux box there with Python installed to perform your automation (Resource for setting up Linux inside EVE-NG) or you can do something like me and define a cloud for access from your workstation.</p> <p></p> <p>To do this, we have right-clicked on our canvas and we have selected network and then selected \"Management(Cloud0)\" this will bridge out to our home network.</p> <p></p> <p>However, we do not have anything inside this network so we need to add connections from the new network to each of our devices. (My networking knowledge needs more attention and I feel that you could just do this next step to the top router and then have connectivity to the rest of the network through this one cable?)</p> <p>I have then logged on to each of our devices and I have run through the following commands for the interfaces applicable to where the cloud comes in.</p> <pre><code>enable\nconfig t\nint gi0/0\nIP add DHCP\nno sh\nexit\nexit\nsh ip int br\n</code></pre> <p>The final step gives us the DHCP address from our home network. My device network list is as follows:</p> Node IP Address Home Network IP Router 10.10.88.110 192.168.169.115 Switch1 10.10.88.111 192.168.169.178 Switch2 10.10.88.112 192.168.169.193 Switch3 10.10.88.113 192.168.169.125 Switch4 10.10.88.114 192.168.169.197"},{"location":"90DaysOfDevOps/day27/#ssh-to-a-network-device","title":"SSH to a network device","text":"<p>With the above in place, we can now connect to our devices on our home network using our workstation. I am using Putty but also have access to other terminals such as git bash that give me the ability to SSH to our devices.</p> <p>Below you can see we have an SSH connection to our router device. (R1)</p> <p></p>"},{"location":"90DaysOfDevOps/day27/#using-python-to-gather-information-from-our-devices","title":"Using Python to gather information from our devices","text":"<p>The first example of how we can leverage Python is to gather information from all of our devices and in particular, I want to be able to connect to each one and run a simple command to provide me with interface configuration and settings. I have stored this script here netmiko_con_multi.py</p> <p>Now when I run this I can see each port configuration over all of my devices.</p> <p></p> <p>This could be handy if you have a lot of different devices, create this one script so that you can centrally control and understand quickly all of the configurations in one place.</p>"},{"location":"90DaysOfDevOps/day27/#using-python-to-configure-our-devices","title":"Using Python to configure our devices","text":"<p>The above is useful but what about using Python to configure our devices, in our scenario we have a trunked port between <code>SW1</code> and <code>SW2</code> again imagine if this was to be done across many of the same switches we want to automate that and not have to manually connect to each switch to make the configuration change.</p> <p>We can use netmiko_sendchange.py to achieve this. This will connect over SSH and perform that change on our <code>SW1</code> which will also change to <code>SW2</code>.</p> <p></p> <p>Now for those that look at the code, you will see the message appears and tells us <code>sending configuration to device</code> but there is no confirmation that this has happened we could add additional code to our script to perform that check and validation on our switch or we could modify our script before to show us this. netmiko_con_multi_vlan.py</p> <p></p>"},{"location":"90DaysOfDevOps/day27/#backing-up-your-device-configurations","title":"backing up your device configurations","text":"<p>Another use case would be to capture our network configurations and make sure we have those backed up, but again we don't want to be connecting to every device we have on our network so we can also automate this using backup.py. You will also need to populate the backup.txt with the IP addresses you want to backup.</p> <p>Run your script and you should see something like the below.</p> <p></p> <p>That could be me just writing a simple print script in python so I should show you the backup files as well.</p> <p></p>"},{"location":"90DaysOfDevOps/day27/#paramiko","title":"Paramiko","text":"<p>A widely used Python module for SSH. You can find out more at the official GitHub link here</p> <p>We can install this module using the <code>pip install paramiko</code> command.</p> <p></p> <p>We can verify the installation by entering the Python shell and importing the paramiko module.</p> <p></p>"},{"location":"90DaysOfDevOps/day27/#netmiko","title":"Netmiko","text":"<p>The netmiko module targets network devices specifically whereas paramiko is a broader tool for handling SSH connections overall.</p> <p>Netmiko which we have used above alongside paramiko can be installed using <code>pip install netmiko</code></p> <p>Netmiko supports many network vendors and devices, you can find a list of supported devices on the GitHub Page</p>"},{"location":"90DaysOfDevOps/day27/#other-modules","title":"Other modules","text":"<p>It is also worth mentioning a few other modules that we have not had the chance to look at but they give a lot more functionality when it comes to network automation.</p> <p><code>netaddr</code> is used for working with and manipulating IP addresses, again the installation is simple with <code>pip install netaddr</code></p> <p>you might find yourself wanting to store a lot of your switch configuration in an excel spreadsheet, the <code>xlrd</code> will allow your scripts to read the excel workbook and convert rows and columns into a matrix. <code>pip install xlrd</code> to get the module installed.</p> <p>Some more use cases where network automation can be used that I have not had the chance to look into can be found here</p> <p>I think this wraps up our Networking section of the #90DaysOfDevOps, Networking is one area that I have not touched for a while really and there is so much more to cover but I am hoping between my notes and the resources shared throughout it is helpful for some.</p>"},{"location":"90DaysOfDevOps/day27/#resources","title":"Resources","text":"<ul> <li>Free Course: Introduction to EVE-NG</li> <li>EVE-NG - Creating your first lab</li> <li>3 Necessary Skills for Network Automation</li> <li>Computer Networking full course</li> <li>Practical Networking</li> <li>Python Network Automation</li> </ul> <p>Most of the examples I am using here as I am not a Network Engineer have come from this extensive book which is not free but I am using some of the scenarios to help understand Network Automation.</p> <ul> <li>Hands-On Enterprise Automation with Python (Book)</li> </ul> <p>See you on Day 28 where will start looking into cloud computing and get a good grasp and foundational knowledge of the topic and what is available.</p>"},{"location":"90DaysOfDevOps/day28/","title":"#90DaysOfDevOps - The Big Picture: DevOps & The Cloud - Day 28","text":""},{"location":"90DaysOfDevOps/day28/#the-big-picture-devops-the-cloud","title":"The Big Picture: DevOps &amp; The Cloud","text":"<p>When it comes to cloud computing and what is offered, it goes very nicely with the DevOps ethos and processes. We can think of Cloud Computing as bringing the technology and services whilst DevOps as we have mentioned many times before is about the process and process improvement.</p> <p>But to start with that cloud learning journey is a steep one and making sure you know and understand all elements or the best service to choose for the right price point is confusing.</p> <p></p> <p>Does the public cloud require a DevOps mindset? My answer here is not, but to really take advantage of cloud computing and possibly avoid those large cloud bills that so many people have been hit with then it is important to think of Cloud Computing and DevOps together.</p> <p>If we look at what we mean by the Public Cloud at a 40,000ft view, it is about removing some responsibility to a managed service to enable you and your team to focus on more important aspects which name should be the application and the end-users. After all the Public Cloud is just someone else's computer.</p> <p></p> <p>In this first section, I want to get into and describe a little more of what a Public Cloud is and some of the building blocks that get referred to as the Public Cloud overall.</p>"},{"location":"90DaysOfDevOps/day28/#saas","title":"SaaS","text":"<p>The first area to cover is Software as a service, this service is removing almost all of the management overhead of a service that you may have once run on-premises. Let's think about Microsoft Exchange for our email, this used to be a physical box that lived in your data centre or maybe in the cupboard under the stairs. You would need to feed and water that server. By that I mean you would need to keep it updated and you would be responsible for buying the server hardware, most likely installing the operating system, installing the applications required and then keeping that patched, if anything went wrong you would have to troubleshoot and get things back up and running.</p> <p>Oh, and you would also have to make sure you were backing up your data, although this doesn't change with SaaS for the most part either.</p> <p>What SaaS does and in particular Microsoft 365, because I mentioned Exchange is removing that administration overhead and they provide a service that delivers your exchange functionality by way of mail but also much other productivity (Office 365) and storage options (OneDrive) that overall gives a great experience to the end-user.</p> <p>Other SaaS applications are widely adopted, such as Salesforce, SAP, Oracle, Google, and Apple. All removing that burden of having to manage more of the stack.</p> <p>I am sure there is a story with DevOps and SaaS-based applications but I am struggling to find out what they may be. I know Azure DevOps has some great integrations with Microsoft 365 that I might have a look into and report back to.</p> <p></p>"},{"location":"90DaysOfDevOps/day28/#public-cloud","title":"Public Cloud","text":"<p>Next up we have the public cloud, most people would think of this in a few different ways, some would see this as the hyper scalers only such as Microsoft Azure, Google Cloud Platform and AWS.</p> <p></p> <p>Some will also see the public cloud as a much wider offering that includes those hyper scalers but also the thousands of MSPs all over the world as well. For this post, we are going to consider Public Cloud including hyper scalers and MSPs, although later on, we will specifically dive into one or more of the hyper scalers to get that foundational knowledge.</p> <p> thousands more companies could land on this, I am merely picking from local, regional, telco and global brands I have worked with and am aware of.</p> <p>We mentioned in the SaaS section that Cloud removed the responsibility or the burden of having to administer parts of a system. If SaaS we see a lot of the abstraction layers removed i.e the physical systems, network, storage, operating system, and even application to some degree. When it comes to the cloud there are various levels of abstraction we can remove or keep depending on your requirements.</p> <p>We have already mentioned SaaS but there are at least two more to mention regarding the public cloud.</p> <p>Infrastructure as a service - You can think of this layer as a virtual machine but whereas on-premises you will be having to look after the physical layer in the cloud this is not the case, the physical is the cloud provider's responsibility and you will manage and administer the Operating System, the data and the applications you wish to run.</p> <p>Platform as a service - This continues to remove the responsibility of layers and this is really about you taking control of the data and the application but not having to worry about the underpinning hardware or operating system.</p> <p>There are many other aaS offerings out there but these are the two fundamentals. You might see offerings around StaaS (Storage as a service) which provide you with your storage layer but without having to worry about the hardware underneath. Or you might have heard CaaS for Containers as a service which we will get onto, later on, another aaS we will look to cover over the next 7 days is FaaS (Functions as a Service) where maybe you do not need a running system up all the time and you just want a function to be executed as and when.</p> <p>There are many ways in which the public cloud can provide abstraction layers of control that you wish to pass up and pay for.</p> <p></p>"},{"location":"90DaysOfDevOps/day28/#private-cloud","title":"Private Cloud","text":"<p>Having your own data centre is not a thing of the past I would think that this has become a resurgence among a lot of companies that have found the OPEX model difficult to manage as well as skill sets in just using the public cloud.</p> <p>The important thing to note here is the public cloud is likely now going to be your responsibility and it is going to be on your premises.</p> <p>We have some interesting things happening in this space not only with VMware that dominated the virtualisation era and on-premises infrastructure environments. We also have the hyper scalers offering an on-premises version of their public clouds.</p> <p></p>"},{"location":"90DaysOfDevOps/day28/#hybrid-cloud","title":"Hybrid Cloud","text":"<p>To follow on from the Public and Private cloud mentions we also can span across both of these environments to provide flexibility between the two, maybe take advantage of services available in the public cloud but then also take advantage of features and functionality of being on-premises or it might be a regulation that dictates you having to store data locally.</p> <p></p> <p>Putting this all together we have a lot of choices for where we store and run our workloads.</p> <p></p> <p>Before we get into a specific hyper-scale, I have asked the power of Twitter where we should go?</p> <p></p> <p>Link to Twitter Poll</p> <p>Whichever one gets the highest percentage we will take a deeper dive into the offerings, I think the important to mention though is that services from all of these are quite similar which is why I say to start with one because I have found that in knowing the foundation of one and how to create virtual machines, set up networking etc. I have been able to go to the others and quickly ramp up in those areas.</p> <p>Either way, I am going to share some great FREE resources that cover all three of the hyper scalers.</p> <p>I am also going to build out a scenario as I have done in the other sections where we can build something as we move through the days.</p>"},{"location":"90DaysOfDevOps/day28/#resources","title":"Resources","text":"<ul> <li>Hybrid Cloud and MultiCloud</li> <li>Microsoft Azure Fundamentals</li> <li>Google Cloud Digital Leader Certification Course</li> <li>AWS Basics for Beginners - Full Course</li> </ul> <p>See you on Day 29</p>"},{"location":"90DaysOfDevOps/day29/","title":"#90DaysOfDevOps - Microsoft Azure Fundamentals - Day 29","text":""},{"location":"90DaysOfDevOps/day29/#microsoft-azure-fundamentals","title":"Microsoft Azure Fundamentals","text":"<p>Before we get going, the winner of the Twitter poll was Microsoft Azure, hence the title of the page. It was close and also quite interesting to see the results come in over the 24 hours.</p> <p></p> <p>I would say in terms of covering this topic is going to give me a better understanding and update around the services available on Microsoft Azure, I lean towards Amazon AWS when it comes to my day today. I have however left resources I had lined up for all three of the major cloud providers.</p> <p>I do appreciate that there are more and the poll only included these 3 and in particular, there were some comments about Oracle Cloud. I would love to hear more about other cloud providers being used out in the wild.</p>"},{"location":"90DaysOfDevOps/day29/#the-basics","title":"The Basics","text":"<ul> <li>Provides public cloud services</li> <li>Geographically distributed (60+ Regions worldwide)</li> <li>Accessed via the internet and/or private connections</li> <li>Multi-tenant model</li> <li>Consumption-based billing - (Pay as you go | Pay as you grow)</li> <li> <p>A large number of service types and offerings for different requirements.</p> </li> <li> <p>Microsoft Azure Global Infrastructure</p> </li> </ul> <p>As much as we spoke about SaaS and Hybrid Cloud we are not planning on covering those topics here.</p> <p>The best way to get started and follow along is by clicking the link, which will enable you to spin up a Microsoft Azure Free Account</p>"},{"location":"90DaysOfDevOps/day29/#regions","title":"Regions","text":"<p>I linked the interactive map above, but we can see the image below the breadth of regions being offered in the Microsoft Azure platform worldwide.</p> <p> image taken from Microsoft Docs - 01/05/2021</p> <p>You will also see several \"sovereign\" clouds meaning they are not linked or able to speak to the other regions, for example, these would be associated with governments such as the <code>AzureUSGovernment</code> also <code>AzureChinaCloud</code> and others.</p> <p>When we are deploying our services within Microsoft Azure we will choose a region for almost everything. However, it is important to note that not every service is available in every region. You can see Products available by region at the time of my writing this that in West Central US we cannot use Azure Databricks.</p> <p>I also mentioned \"almost everything\" above, there are certain services that are linked to the region such as Azure Bot Services, Bing Speech, Azure Virtual Desktop, Static Web Apps, and some more.</p> <p>Behind the scenes, a region may be made up of more than one data centre. These will be referred to as Availability Zones.</p> <p>In the below image you will see and again this is taken from the Microsoft official documentation it describes what a region is and how it is made up of Availability Zones. However not all regions have multiple Availability Zones.</p> <p></p> <p>The Microsoft Documentation is very good, and you can read up more on Regions and Availability Zones here.</p>"},{"location":"90DaysOfDevOps/day29/#subscriptions","title":"Subscriptions","text":"<p>Remember we mentioned that Microsoft Azure is a consumption model cloud you will find that all major cloud providers follow this model.</p> <p>If you are an Enterprise then you might want or have an Enterprise Agreement set up with Microsoft to enable your company to consume these Azure Services.</p> <p>If you are like me and you are using Microsoft Azure for education then we have a few other options.</p> <p>We have the Microsoft Azure Free Account which generally gives you several free cloud credits to spend in Azure over some time.</p> <p>There is also the ability to use a Visual Studio subscription which gives you maybe some free credits each month alongside your annual subscription to Visual Studio, this was commonly known as the MSDN years ago. Visual Studio</p> <p>Then finally there is the hand over a credit card and have a pay as you go, model. Pay-as-you-go</p> <p>A subscription can be seen as a boundary between different subscriptions potentially cost centres but completely different environments. A subscription is where the resources are created.</p>"},{"location":"90DaysOfDevOps/day29/#management-groups","title":"Management Groups","text":"<p>Management groups give us the ability to segregate control across our Azure Active Directory (AD) or our tenant environment. Management groups allow us to control policies, Role Based Access Control (RBAC), and budgets.</p> <p>Subscriptions belong to these management groups so you could have many subscriptions in your Azure AD Tenant, these subscriptions then can also control policies, RBAC, and budgets.</p>"},{"location":"90DaysOfDevOps/day29/#resource-manager-and-resource-groups","title":"Resource Manager and Resource Groups","text":""},{"location":"90DaysOfDevOps/day29/#azure-resource-manager","title":"Azure Resource Manager","text":"<ul> <li>JSON based API that is built on resource providers.</li> <li>Resources belong to a resource group and share a common life cycle.</li> <li>Parallelism</li> <li>JSON-Based deployments are declarative, idempotent and understand dependencies between resources to govern creation and order.</li> </ul>"},{"location":"90DaysOfDevOps/day29/#resource-groups","title":"Resource Groups","text":"<ul> <li>Every Azure Resource Manager resource exists in one and only one resource group!</li> <li>Resource groups are created in a region that can contain resources from outside the region.</li> <li>Resources can be moved between resource groups</li> <li>Resource groups are not walled off from other resource groups, there can be communication between resource groups.</li> <li>Resource Groups can also control policies, RBAC, and budgets.</li> </ul>"},{"location":"90DaysOfDevOps/day29/#hands-on","title":"Hands-On","text":"<p>Let's go and get connected and make sure we have a Subscription available to us. We can check our simple out of the box Management Group, We can then go and create a new dedicated Resource Group in our preferred Region.</p> <p>When we first login to our Azure portal you will see at the top the ability to search for resources, services and docs.</p> <p></p> <p>We are going to first look at our subscription, you will see here that I am using a Visual Studio Professional subscription which gives me some free credit each month.</p> <p></p> <p>If we go into that you will get a wider view and a look into what is happening or what can be done with the subscription, we can see billing information with control functions on the left where you can define IAM Access Control and further down there are more resources available.</p> <p></p> <p>There might be a scenario where you have multiple subscriptions and you want to manage them all under one, this is where management groups can be used to segregate responsibility groups. In mine below, you can see there is just my tenant root group with my subscription.</p> <p>You will also see in the previous image that the parent management group is the same id used on the tenant root group.</p> <p></p> <p>Next up we have Resource groups, this is where we combine our resources and we can easily manage them in one place. I have a few created for various other projects.</p> <p></p> <p>With what we are going to be doing over the next few days, we want to create our resource group. This is easily done in this console by hitting the create option on the previous image.</p> <p></p> <p>A validation step takes place and then you have the chance to review your creation and then create. You will also see down the bottom \"Download a template for automation\" this allows us to grab the JSON format so that we can perform this simple in an automated fashion later on if we wanted, we will cover this later on as well.</p> <p></p> <p>Hit create, then in our list of resource groups, we now have our \"90DaysOfDevOps\" group ready for what we do in the next session.</p> <p></p>"},{"location":"90DaysOfDevOps/day29/#resources","title":"Resources","text":"<ul> <li>Hybrid Cloud and MultiCloud</li> <li>Microsoft Azure Fundamentals</li> <li>Google Cloud Digital Leader Certification Course</li> <li>AWS Basics for Beginners - Full Course</li> </ul> <p>See you on Day 30</p>"},{"location":"90DaysOfDevOps/day30/","title":"#90DaysOfDevOps - Microsoft Azure Security Models - Day 30","text":""},{"location":"90DaysOfDevOps/day30/#microsoft-azure-security-models","title":"Microsoft Azure Security Models","text":"<p>Following on from the Microsoft Azure Overview, we are going to start with Azure Security and see where this can help in our day to day. For the most part, I have found the built-in roles have been sufficient but knowing that we can create and work with many different areas of authentication and configurations. I have found Microsoft Azure to be quite advanced with its Active Directory background compared to other public clouds.</p> <p>This is one area in which Microsoft Azure seemingly works differently from other public cloud providers, in Azure there is ALWAYS Azure AD.</p>"},{"location":"90DaysOfDevOps/day30/#directory-services","title":"Directory Services","text":"<ul> <li>Azure Active Directory hosts the security principles used by Microsoft Azure and other Microsoft cloud services.</li> <li>Authentication is accomplished through protocols such as SAML, WS-Federation, OpenID Connect and OAuth2.</li> <li>Queries are accomplished through REST API called Microsoft Graph API.</li> <li>Tenants have a tenant.onmicrosoft.com default name but can also have custom domain names.</li> <li>Subscriptions are associated with an Azure Active Directory tenant.</li> </ul> <p>If we think about AWS to compare the equivalent offering would be AWS IAM (Identity &amp; Access Management) Although still very different</p> <p>Azure AD Connect provides the ability to replicate accounts from AD to Azure AD. This can also include groups and sometimes objects. This can be granular and filtered. Supports multiple forests and domains.</p> <p>It is possible to create cloud accounts in Microsoft Azure Active Directory (AD) but most organisations already have accounted for their users in their own Active Directory being on-premises.</p> <p>Azure AD Connect also allows you to not only see Windows AD servers but also other Azure AD, Google and others. This also provides the ability to collaborate with external people and organisations this is called Azure B2B.</p> <p>Authentication options between Active Directory Domain Services and Microsoft Azure Active Directory are possible with both identity sync with a password hash.</p> <p></p> <p>The passing of the password hash is optional, if this is not used then pass-through authentication is required.</p> <p>There is a video linked below that goes into detail about Passthrough authentication.</p> <p>User sign-in with Azure Active Directory Pass-through Authentication</p> <p></p>"},{"location":"90DaysOfDevOps/day30/#federation","title":"Federation","text":"<p>It's fair to say that if you are using Microsoft 365, Microsoft Dynamics and on-premises Active Directory it is quite easy to understand and integrate into Azure AD for federation. However, you might be using other services outside of the Microsoft ecosystem.</p> <p>Azure AD can act as a federation broker to these other Non-Microsoft apps and other directory services.</p> <p>This will be seen in the Azure Portal as Enterprise Applications of which there are a large number of options.</p> <p></p> <p>If you scroll down on the enterprise application page you are going to see a long list of featured applications.</p> <p></p> <p>This option also allows for \"bring your own\" integration, an application you are developing or a non-gallery application.</p> <p>I have not looked into this before but I can see that this is quite the feature set when compared to the other cloud providers and capabilities.</p>"},{"location":"90DaysOfDevOps/day30/#role-based-access-control","title":"Role-Based Access Control","text":"<p>We have already covered on Day 29 the scopes we are going to cover here, we can set our role-based access control according to one of these areas.</p> <ul> <li>Subscriptions</li> <li>Management Group</li> <li>Resource Group</li> <li>Resources</li> </ul> <p>Roles can be split into three, there are many built-in roles in Microsoft Azure. Those three are:</p> <ul> <li>Owner</li> <li>Contributor</li> <li>Reader</li> </ul> <p>Owner and Contributor are very similar in their boundaries of scope however the owner can change permissions.</p> <p>Other roles are specific to certain types of Azure Resources as well as custom roles.</p> <p>We should focus on assigning permissions to groups vs users.</p> <p>Permissions are inherited.</p> <p>If we go back and look at the \"90DaysOfDevOps\" Resource group we created and check the Access Control (IAM) within you can see we have a list of contributors and a customer User Access Administrator, and we do have a list of owners (But I cannot show this)</p> <p></p> <p>We can also check the roles we have assigned here if they are BuiltInRoles and which category they fall under.</p> <p></p> <p>We can also use the check access tab if we want to check an account against this resource group and make sure that the account we wish to have that access to has the correct permissions or maybe we want to check if a user has too much access.</p> <p></p>"},{"location":"90DaysOfDevOps/day30/#microsoft-defender-for-cloud","title":"Microsoft Defender for Cloud","text":"<ul> <li> <p>Microsoft Defender for Cloud (formerly known as Azure Security Center) provides insight into the security of the entire Azure environment.</p> </li> <li> <p>A single dashboard for visibility into the overall security health of all Azure and non-Azure resources (via Azure Arc) and security hardening guidance.</p> </li> <li> <p>Free tier includes continuous assessment and security recommendations.</p> </li> <li> <p>Paid plans for protected resource types (e.g. Servers, AppService, SQL, Storage, Containers, KeyVault).</p> </li> </ul> <p>I have switched to another subscription to view the Azure Security Center and you can see here based on very few resources that I have some recommendations in one place.</p> <p></p>"},{"location":"90DaysOfDevOps/day30/#azure-policy","title":"Azure Policy","text":"<ul> <li> <p>Azure Policy is an Azure native service that helps to enforce organizational standards and assess compliance at scale.</p> </li> <li> <p>Integrated into Microsoft Defender for Cloud. Azure Policy audits non-compliant resources and applies remediation.</p> </li> <li> <p>Commonly used for governing resource consistency, regulatory compliance, security, cost, and management standards.</p> </li> <li> <p>Uses JSON format to store evaluation logic and determine whether a resource is compliant or not, and any actions to take for non-compliance (e.g. Audit, AuditIfNotExists, Deny, Modify, DeployIfNotExists).</p> </li> <li> <p>Free for use. The exception is Azure Arc connected resources charged per server/month for Azure Policy Guest Configuration usage.</p> </li> </ul>"},{"location":"90DaysOfDevOps/day30/#hands-on","title":"Hands-On","text":"<p>I have gone out and I have purchased www.90DaysOfDevOps.com and I would like to add this domain to my Azure Active Directory portal, Add your custom domain name using the Azure Active Directory Portal</p> <p></p> <p>With that now, we can create a new user on our new Active Directory Domain.</p> <p></p> <p>Now we want to create a group for all of our new 90DaysOfDevOps users in one group. We can create a group as per the below, notice that I am using \"Dynamic User\" which means Azure AD will query user accounts and add them dynamically vs assigned which is where you manually add the user to your group.</p> <p></p> <p>There are lots of options when it comes to creating your query, I plan to simply find the principal name and make sure that the name contains @90DaysOfDevOps.com.</p> <p></p> <p>Now because we have created our user account already for michael.cade@90DaysOfDevOps.com we can validate the rules are working. For comparison I have also added another account I have associated to another domain here and you can see that because of this rule our user will not land in this group.</p> <p></p> <p>I have since added a new user1@90DaysOfDevOps.com and if we go and check the group we can see our members.</p> <p></p> <p>If we have this requirement x100 then we are not going to want to do this all in the console we are going to want to take advantage of either bulk options to create, invite, and delete users or you are going to want to look into PowerShell to achieve this automated approach to scale.</p> <p>Now we can go to our Resource Group and specify that on the 90DaysOfDevOps resource group we want the owner to be the group we just created.</p> <p></p> <p>We can equally go in here and deny assignments access to our resource group as well.</p> <p>Now if we log in to the Azure Portal with our new user account, you can see that we only have access to our 90DaysOfDevOps resource group and not the others seen in previous pictures because we do not have the access.</p> <p></p> <p>The above is great if this is a user that has access to resources inside of your Azure portal, not every user needs to be aware of the portal, but to check access we can use the Apps Portal This is a single sign-on portal for us to test.</p> <p></p> <p>You can customise this portal with your branding and this might be something we come back to later on.</p>"},{"location":"90DaysOfDevOps/day30/#resources","title":"Resources","text":"<ul> <li>Hybrid Cloud and MultiCloud</li> <li>Microsoft Azure Fundamentals</li> <li>Google Cloud Digital Leader Certification Course</li> <li>AWS Basics for Beginners - Full Course</li> </ul> <p>See you on Day 31</p>"},{"location":"90DaysOfDevOps/day31/","title":"#90DaysOfDevOps - Microsoft Azure Compute Models - Day 31","text":""},{"location":"90DaysOfDevOps/day31/#microsoft-azure-compute-models","title":"Microsoft Azure Compute Models","text":"<p>Following on from covering the basics around security models within Microsoft Azure yesterday today we are going to look into the various compute services available to us in Azure.</p>"},{"location":"90DaysOfDevOps/day31/#service-availability-options","title":"Service Availability Options","text":"<p>This section is close to my heart given my role in Data Management. As with on-premises, it is critical to ensure the availability of your services.</p> <ul> <li>High Availability (Protection within a region)</li> <li>Disaster Recovery (Protection between regions)</li> <li>Backup (Recovery from a point in time)</li> </ul> <p>Microsoft deploys multiple regions within a geopolitical boundary.</p> <p>Two concepts with Azure for Service Availability. Both sets and zones.</p> <p>Availability Sets - Provide resiliency within a datacenter</p> <p>Availability Zones - Provide resiliency between data centres within a region.</p>"},{"location":"90DaysOfDevOps/day31/#virtual-machines","title":"Virtual Machines","text":"<p>Most likely the starting point for anyone in the public cloud.</p> <ul> <li>Provides a VM from a variety of series and sizes with different capabilities (Sometimes an overwhelming) Sizes for Virtual machines in Azure</li> <li>There are many different options and focuses for VMs from high performance, and low latency to high memory options VMs.</li> <li>We also have a burstable VM type which can be found under the B-Series. This is great for workloads where you can have a low CPU requirement for the most part but require that maybe once a month performance spike requirement.</li> <li>Virtual Machines are placed on a virtual network that can provide connectivity to any network.</li> <li>Windows and Linux guest OS support.</li> <li>There are also Azure-tuned kernels when it comes to specific Linux distributions. Azure Tuned Kernals</li> </ul>"},{"location":"90DaysOfDevOps/day31/#templating","title":"Templating","text":"<p>I have mentioned before that everything behind or underneath Microsoft Azure is JSON.</p> <p>There are several different management portals and consoles we can use to create our resources the preferred route is going to be via JSON templates.</p> <p>Idempotent deployments in incremental or complete mode - i.e repeatable desired state.</p> <p>There is a large selection of templates that can export deployed resource definitions. I like to think about this templating feature to something like AWS CloudFormation or could be Terraform for a multi-cloud option. We will cover Terraform more in the Infrastructure as code section.</p>"},{"location":"90DaysOfDevOps/day31/#scaling","title":"Scaling","text":"<p>Automatic scaling is a large feature of the Public Cloud, being able to spin down resources you are not using or spin up when you need them.</p> <p>In Azure, we have something called Virtual Machine Scale Sets (VMSS) for IaaS. This enables the automatic creation and scale from a gold standard image based on schedules and metrics.</p> <p>This is ideal for updating windows so that you can update your images and roll those out with the least impact.</p> <p>Other services such as Azure App Services have auto-scaling built in.</p>"},{"location":"90DaysOfDevOps/day31/#containers","title":"Containers","text":"<p>We have not covered containers as a use case and what and how they can and should be needed in our DevOps learning journey but we need to mention that Azure has some specific container-focused services to mention.</p> <p>Azure Kubernetes Service (AKS) - Provides a managed Kubernetes solution, no need to worry about the control plane or management of the underpinning cluster management. More on Kubernetes also later on.</p> <p>Azure Container Instances - Containers as a service with Per-Second Billing. Run an image and integrate it with your virtual network, no need for Container Orchestration.</p> <p>Service Fabric - Has many capabilities but includes orchestration for container instances.</p> <p>Azure also has the Container Registry which provides a private registry for Docker Images, Helm charts, OCI Artifacts and images. More on this again when we reach the containers section.</p> <p>We should also mention that a lot of the container services may indeed also leverage containers under the hood but this is abstracted away from your requirement to manage.</p> <p>These mentioned container-focused services we also find similar services in all other public clouds.</p>"},{"location":"90DaysOfDevOps/day31/#application-services","title":"Application Services","text":"<ul> <li>Azure Application Services provides an application hosting solution that provides an easy method to establish services.</li> <li>Automatic Deployment and Scaling.</li> <li>Supports Windows &amp; Linux-based solutions.</li> <li>Services run in an App Service Plan which has a type and size.</li> <li>Number of different services including web apps, API apps and mobile apps.</li> <li>Support for Deployment slots for reliable testing and promotion.</li> </ul>"},{"location":"90DaysOfDevOps/day31/#serverless-computing","title":"Serverless Computing","text":"<p>Serverless for me is an exciting next step that I am extremely interested in learning more about.</p> <p>The goal with serverless is that we only pay for the runtime of the function and do not have to have running virtual machines or PaaS applications running all the time. We simply run our function when we need it and then it goes away.</p> <p>Azure Functions - Provides serverless code. If we remember back to our first look into the public cloud we will remember the abstraction layer of management, with serverless functions you are only going to be managing the code.</p> <p>Event-Driven with massive scale, I have a plan to build something when I get some hands-on here hopefully later on.</p> <p>Provides input and output binding to many Azure and 3rd Party Services.</p> <p>Supports many different programming languages. (C#, NodeJS, Python, PHP, batch, bash, Golang and Rust. Or any Executable)</p> <p>Azure Event Grid enables logic to be triggered from services and events.</p> <p>Azure Logic App provides a graphical-based workflow and integration.</p> <p>We can also look at Azure Batch which can run large-scale jobs on both Windows and Linux nodes with consistent management &amp; scheduling.</p>"},{"location":"90DaysOfDevOps/day31/#resources","title":"Resources","text":"<ul> <li>Hybrid Cloud and MultiCloud</li> <li>Microsoft Azure Fundamentals</li> <li>Google Cloud Digital Leader Certification Course</li> <li>AWS Basics for Beginners - Full Course</li> </ul> <p>See you on Day 32</p>"},{"location":"90DaysOfDevOps/day32/","title":"#90DaysOfDevOps - Microsoft Azure Storage Models - Day 32","text":""},{"location":"90DaysOfDevOps/day32/#microsoft-azure-storage-models","title":"Microsoft Azure Storage Models","text":""},{"location":"90DaysOfDevOps/day32/#storage-services","title":"Storage Services","text":"<ul> <li>Azure storage services are provided by storage accounts.</li> <li>Storage accounts are primarily accessed via REST API.</li> <li>A storage account must have a unique name that is part of a DNS name <code>&lt;Storage Account name&gt;.core.windows.net</code></li> <li>Various replication and encryption options.</li> <li>Sits within a resource group</li> </ul> <p>We can create our storage group by simply searching for Storage Group in the search bar at the top of the Azure Portal.</p> <p></p> <p>We can then run through the steps to create our storage account remembering that this name needs to be unique and it also needs to be all lower case, with no spaces but can include numbers.</p> <p></p> <p>We can also choose the level of redundancy we would like against our storage account and anything we store here. The further down the list the more expensive option but also the spread of your data.</p> <p>Even the default redundancy option gives us 3 copies of our data.</p> <p>Azure Storage Redundancy</p> <p>Summary of the above link down below:</p> <ul> <li>Locally-redundant storage - replicates your data three times within a single data centre in the primary region.</li> <li>Geo-redundant storage - copies your data synchronously three times within a single physical location in the primary region using LRS.</li> <li>Zone-redundant storage - replicates your Azure Storage data synchronously across three Azure availability zones in the primary region.</li> <li>Geo-zone-redundant storage - combines the high availability provided by redundancy across availability zones with protection from regional outages provided by geo-replication. Data in a GZRS storage account is copied across three Azure availability zones in the primary region and is also replicated to a second geographic region for protection from regional disasters.</li> </ul> <p></p> <p>Just moving back up to performance options. We have Standard and Premium to choose from. We have chosen Standard in our walkthrough but premium gives you some specific options.</p> <p></p> <p>Then in the drop-down, you can see we have these three options to choose from.</p> <p></p> <p>There are lots more advanced options available for your storage account but for now, we do not need to get into these areas. These options are around encryption and data protection.</p>"},{"location":"90DaysOfDevOps/day32/#managed-disks","title":"Managed Disks","text":"<p>Storage access can be achieved in a few different ways.</p> <p>Authenticated access via:</p> <ul> <li>A shared key for full control.</li> <li>Shared Access Signature for delegated, granular access.</li> <li>Azure Active Directory (Where Available)</li> </ul> <p>Public Access:</p> <ul> <li>Public access can also be granted to enable anonymous access including via HTTP.</li> <li>An example of this could be to host basic content and files in a block blob so a browser can view and download this data.</li> </ul> <p>If you are accessing your storage from another Azure service, traffic stays within Azure.</p> <p>When it comes to storage performance we have two different types:</p> <ul> <li>Standard - Maximum number of IOPS</li> <li>Premium - Guaranteed number of IOPS</li> </ul> <p>IOPS =&gt; Input/Output operations per sec.</p> <p>There is also a difference between unmanaged and managed disks to consider when choosing the right storage for the task you have.</p>"},{"location":"90DaysOfDevOps/day32/#virtual-machine-storage","title":"Virtual Machine Storage","text":"<ul> <li>Virtual Machine OS disks are typically stored on persistent storage.</li> <li>Some stateless workloads do not require persistent storage and reduced latency is a larger benefit.</li> <li>There are VMs that support ephemeral OS-managed disks that are created on the node-local storage.</li> <li>These can also be used with VM Scale Sets.</li> </ul> <p>Managed Disks are durable block storage that can be used with Azure Virtual Machines. You can have Ultra Disk Storage, Premium SSD, Standard SSD, or Standard HDD. They also carry some characteristics.</p> <ul> <li>Snapshot and Image support</li> <li>Simple movement between SKUs</li> <li>Better availability when combined with availability sets</li> <li>Billed based on disk size not on consumed storage.</li> </ul>"},{"location":"90DaysOfDevOps/day32/#archive-storage","title":"Archive Storage","text":"<ul> <li>Cool Tier - A cool tier of storage is available to block and append blobs.</li> <li>Lower Storage cost</li> <li>Higher transaction cost.</li> <li>Archive Tier - Archive storage is available for block BLOBs.</li> <li>This is configured on a per-BLOB basis.</li> <li>Cheaper cost, Longer Data retrieval latency.</li> <li>Same Data Durability as regular Azure Storage.</li> <li>Custom Data tiering can be enabled as required.</li> </ul>"},{"location":"90DaysOfDevOps/day32/#file-sharing","title":"File Sharing","text":"<p>From the above creation of our storage account, we can now create file shares.</p> <p></p> <p>This will provide SMB2.1 and 3.0 file shares in Azure.</p> <p>Useable within the Azure and externally via SMB3 and port 445 open to the internet.</p> <p>Provides shared file storage in Azure.</p> <p>Can be mapped using standard SMB clients in addition to REST API.</p> <p>You might also notice Azure NetApp Files (SMB and NFS)</p>"},{"location":"90DaysOfDevOps/day32/#caching-media-services","title":"Caching &amp; Media Services","text":"<p>The Azure Content Delivery Network provides a cache of static web content with locations throughout the world.</p> <p>Azure Media Services, provides media transcoding technologies in addition to playback services.</p>"},{"location":"90DaysOfDevOps/day32/#microsoft-azure-database-models","title":"Microsoft Azure Database Models","text":"<p>Back on Day 28, we covered various service options. One of these was PaaS (Platform as a Service) where you abstract a large amount of the infrastructure and operating system away and you are left with the control of the application or in this case the database models.</p>"},{"location":"90DaysOfDevOps/day32/#relational-databases","title":"Relational Databases","text":"<p>Azure SQL Database provides a relational database as a service based on Microsoft SQL Server.</p> <p>This is SQL running the latest SQL branch with database compatibility level available where a specific functionality version is required.</p> <p>There are a few options on how this can be configured, we can provide a single database that provides one database in the instance, while an elastic pool enables multiple databases that share a pool of capacity and collectively scale.</p> <p>These database instances can be accessed like regular SQL instances.</p> <p>Additional managed offerings for MySQL, PostgreSQL and MariaDB.</p> <p></p>"},{"location":"90DaysOfDevOps/day32/#nosql-solutions","title":"NoSQL Solutions","text":"<p>Azure Cosmos DB is a scheme agnostic NoSQL implementation.</p> <p>99.99% SLA</p> <p>Globally distributed database with single-digit latencies at the 99th percentile anywhere in the world with automatic homing.</p> <p>Partition key leveraged for the partitioning/sharding/distribution of data.</p> <p>Supports various data models (documents, key-value, graph, column-friendly)</p> <p>Supports various APIs (DocumentDB SQL, MongoDB, Azure Table Storage and Gremlin)</p> <p></p> <p>Various consistency models are available based around CAP theorem.</p> <p></p>"},{"location":"90DaysOfDevOps/day32/#caching","title":"Caching","text":"<p>Without getting into the weeds about caching systems such as Redis I wanted to include that Microsoft Azure has a service called Azure Cache for Redis.</p> <p>Azure Cache for Redis provides an in-memory data store based on the Redis software.</p> <ul> <li>It is an implementation of the open-source Redis Cache.</li> <li>A hosted, secure Redis cache instance.</li> <li>Different tiers are available</li> <li>Application must be updated to leverage the cache.</li> <li>Aimed for an application that has high read requirements compared to writes.</li> <li>Key-Value store based.</li> </ul> <p></p> <p>I appreciate the last few days have been a lot of note-taking and theory on Microsoft Azure but I wanted to cover the building blocks before we get into the hands-on aspects of how these components come together and work.</p> <p>We have one more bit of theory remaining around networking before we can get some scenario-based deployments of services up and running. We also want to take a look at some of the different ways we can interact with Microsoft Azure vs just using the portal that we have been using so far.</p>"},{"location":"90DaysOfDevOps/day32/#resources","title":"Resources","text":"<ul> <li>Hybrid Cloud and MultiCloud</li> <li>Microsoft Azure Fundamentals</li> <li>Google Cloud Digital Leader Certification Course</li> <li>AWS Basics for Beginners - Full Course</li> </ul> <p>See you on Day 33</p>"},{"location":"90DaysOfDevOps/day33/","title":"#90DaysOfDevOps - Microsoft Azure Networking Models + Azure Management - Day 33","text":""},{"location":"90DaysOfDevOps/day33/#microsoft-azure-networking-models-azure-management","title":"Microsoft Azure Networking Models + Azure Management","text":"<p>As if today marks the anniversary of Microsoft Azure and its 12th Birthday! (1st February 2022) Anyway, we are going to cover the networking models within Microsoft Azure and some of the management options for Azure. So far we have only used the Azure portal but we have mentioned other areas that can be used to drive and create our resources within the platform.</p>"},{"location":"90DaysOfDevOps/day33/#azure-network-models","title":"Azure Network Models","text":""},{"location":"90DaysOfDevOps/day33/#virtual-networks","title":"Virtual Networks","text":"<ul> <li>A virtual network is a construct created in Azure.</li> <li>A virtual network has one or more IP ranges assigned to it.</li> <li>Virtual networks live within a subscription within a region.</li> <li>Virtual subnets are created in the virtual network to break up the network range.</li> <li>Virtual machines are placed in virtual subnets.</li> <li>All virtual machines within a virtual network can communicate.</li> <li>65,536 Private IPs per Virtual Network.</li> <li>Only pay for egress traffic from a region. (Data leaving the region)</li> <li>IPv4 &amp; IPv6 Supported.</li> <li>IPv6 for public-facing and within virtual networks.</li> </ul> <p>We can liken Azure Virtual Networks to AWS VPCs. However, there are some differences to note:</p> <ul> <li>In AWS a default VNet is created that is not the case in Microsoft Azure, you have to create your first virtual network to your requirements.</li> <li>All Virtual Machines by default in Azure have NAT access to the internet. No NAT Gateways as per AWS.</li> <li>In Microsoft Azure, there is no concept of Private or Public subnets.</li> <li>Public IPs are a resource that can be assigned to vNICs or Load Balancers.</li> <li>The Virtual Network and Subnets have their own ACLs enabling subnet level delegation.</li> <li>Subnets across Availability Zones whereas in AWS you have subnets per Availability Zones.</li> </ul> <p>We also have Virtual Network Peering. This enables virtual networks across tenants and regions to be connected using the Azure backbone. Not transitive but can be enabled via Azure Firewall in the hub virtual network. Using a gateway transit allows peered virtual networks to the connectivity of the connected network and an example of this could ExpressRoute to On-Premises.</p>"},{"location":"90DaysOfDevOps/day33/#access-control","title":"Access Control","text":"<ul> <li>Azure utilises Network Security Groups, these are stateful.</li> <li>Enable rules to be created and then assigned to a network security group</li> <li>Network security groups applied to subnets or VMs.</li> <li>When applied to a subnet it is still enforced at the Virtual Machine NIC that it is not an \"Edge\" device.</li> </ul> <ul> <li>Rules are combined in a Network Security Group.</li> <li>Based on the priority, flexible configurations are possible.</li> <li>Lower priority number means high priority.</li> <li>Most logic is built by IP Addresses but some tags and labels can also be used.</li> </ul> Description Priority Source Address Source Port Destination Address Destination Port Action Inbound 443 1005 * * * 443 Allow ILB 1010 Azure LoadBalancer * * 10000 Allow Deny All Inbound 4000 * * * * DENY <p>We also have Application Security Groups (ASGs)</p> <ul> <li>Where NSGs are focused on the IP address ranges which may be difficult to maintain for growing environments.</li> <li>ASGs enable real names (Monikers) for different application roles to be defined (Webservers, DB servers, WebApp1 etc.)</li> <li>The Virtual Machine NIC is made a member of one or more ASGs.</li> </ul> <p>The ASGs can then be used in rules that are part of Network Security Groups to control the flow of communication and can still use NSG features like service tags.</p> Action Name Source Destination Port Allow AllowInternettoWeb Internet WebServers 443(HTTPS) Allow AllowWebToApp WebServers AppServers 443(HTTPS) Allow AllowAppToDB AppServers DbServers 1443 (MSSQL) Deny DenyAllinbound Any Any Any"},{"location":"90DaysOfDevOps/day33/#load-balancing","title":"Load Balancing","text":"<p>Microsoft Azure has two separate load balancing solutions. (the first party, there are third parties available in the Azure marketplace.) Both can operate with externally facing or internally facing endpoints.</p> <ul> <li>Load Balancer (Layer 4) supporting hash-based distribution and port-forwarding.</li> <li>App Gateway (Layer 7) supports features such as SSL offload, cookie-based session affinity and URL-based content routing.</li> </ul> <p>Also with the App Gateway, you can optionally use the Web Application firewall component.</p>"},{"location":"90DaysOfDevOps/day33/#azure-management-tools","title":"Azure Management Tools","text":"<p>We have spent most of our theory time walking through the Azure Portal, I would suggest that when it comes to following a DevOps culture and process a lot of these tasks, especially around provisioning will be done via an API or a command-line tool. I wanted to touch on some of those other management tools that we have available to us as we need to know this for when we are automating the provisioning of our Azure environments.</p>"},{"location":"90DaysOfDevOps/day33/#azure-portal","title":"Azure Portal","text":"<p>The Microsoft Azure Portal is a web-based console, that provides an alternative to command-line tools. You can manage your subscriptions within the Azure Portal. Build, Manage, and Monitor everything from a simple web app to complex cloud deployments. Another thing you will find within the portal are these breadcrumbs, JSON as mentioned before is the underpinning of all Azure Resources, It might be that you start in the Portal to understand the features, services and functionality but then later understand the JSON underneath to incorporate into your automated workflows.</p> <p></p> <p>There is also the Azure Preview portal, this can be used to view and test new and upcoming services and enhancements.</p> <p></p>"},{"location":"90DaysOfDevOps/day33/#powershell","title":"PowerShell","text":"<p>Before we get into Azure PowerShell it is worth introducing PowerShell first. PowerShell is a task automation and configuration management framework, a command-line shell and a scripting language. We might and dare I say this liken this to what we have covered in the Linux section around shell scripting. PowerShell was very much first found on Windows OS but it is now cross-platform.</p> <p>Azure PowerShell is a set of cmdlets for managing Azure resources directly from the PowerShell command line.</p> <p>We can see below that you can connect to your subscription using the PowerShell command <code>Connect-AzAccount</code></p> <p></p> <p>Then if we wanted to find some specific commands associated with Azure VMs we can run the following command. You could spend hours learning and understanding more about this PowerShell programming language.</p> <p></p> <p>There are some great quickstarts from Microsoft on getting started and provisioning services from PowerShell here</p>"},{"location":"90DaysOfDevOps/day33/#visual-studio-code","title":"Visual Studio Code","text":"<p>Like many, and as you have all seen my go-to IDE is Visual Studio Code.</p> <p>Visual Studio Code is a free source-code editor made by Microsoft for Windows, Linux and macOS.</p> <p>You will see below that there are lots of integrations and tools built into Visual Studio Code that you can use to interact with Microsoft Azure and the services within.</p> <p></p>"},{"location":"90DaysOfDevOps/day33/#cloud-shell","title":"Cloud Shell","text":"<p>Azure Cloud Shell is an interactive, authenticated, browser-accessible shell for managing Azure resources. It provides the flexibility of choosing the shell experience that best suits the way you work.</p> <p></p> <p>You can see from the below when we first launch Cloud Shell within the portal we can choose between Bash and PowerShell.</p> <p></p> <p>To use the cloud shell you will have to provide a bit of storage in your subscription.</p> <p>When you select to use the cloud shell it is spinning up a machine, these machines are temporary but your files are persisted in two ways; through a disk image and a mounted file share.</p> <p></p> <ul> <li>Cloud Shell runs on a temporary host provided on a per-session, per-user basis</li> <li>Cloud Shell times out after 20 minutes without interactive activity</li> <li>Cloud Shell requires an Azure file share to be mounted</li> <li>Cloud Shell uses the same Azure file share for both Bash and PowerShell</li> <li>Cloud Shell is assigned one machine per user account</li> <li>Cloud Shell persists $HOME using a 5-GB image held in your file share</li> <li>Permissions are set as a regular Linux user in Bash</li> </ul> <p>The above was copied from Cloud Shell Overview</p>"},{"location":"90DaysOfDevOps/day33/#azure-cli","title":"Azure CLI","text":"<p>Finally, I want to cover the Azure CLI, The Azure CLI can be installed on Windows, Linux and macOS. Once installed you can type <code>az</code> followed by other commands to create, update, delete and view Azure resources.</p> <p>When I initially came into my Azure learning I was a little confused by there being Azure PowerShell and the Azure CLI.</p> <p>I would love some feedback from the community on this as well. But the way I see it is that Azure PowerShell is a module added to Windows PowerShell or PowerShell Core (Also available on other OS but not all) Whereas Azure CLI is a cross-platform command-line program that connects to Azure and executes those commands.</p> <p>Both of these options have a different syntax, although they can from what I can see and what I have done do very similar tasks.</p> <p>For example, creating a virtual machine from PowerShell would use the <code>New-AzVM</code> cmdlet whereas Azure CLI would use <code>az VM create</code>.</p> <p>You saw previously that I have the Azure PowerShell module installed on my system but then I also have the Azure CLI installed that can be called through PowerShell on my Windows machine.</p> <p></p> <p>The takeaway here as we already mentioned is about choosing the right tool. Azure runs on automation. Every action you take inside the portal translates somewhere to code being executed to read, create, modify, or delete resources.</p> <p>Azure CLI</p> <ul> <li>Cross-platform command-line interface, installable on Windows, macOS, Linux</li> <li>Runs in Windows PowerShell, Cmd, Bash and other Unix shells.</li> </ul> <p>Azure PowerShell</p> <ul> <li>Cross-platform PowerShell module, runs on Windows, macOS, Linux</li> <li>Requires Windows PowerShell or PowerShell</li> </ul> <p>If there is a reason you cannot use PowerShell in your environment but you can use .mdor bash then the Azure CLI is going to be your choice.</p> <p>Next up we take all the theories we have been through and create some scenarios and get hands-on in Azure.</p>"},{"location":"90DaysOfDevOps/day33/#resources","title":"Resources","text":"<ul> <li>Hybrid Cloud and MultiCloud</li> <li>Microsoft Azure Fundamentals</li> <li>Google Cloud Digital Leader Certification Course</li> <li>AWS Basics for Beginners - Full Course</li> </ul> <p>See you on Day 34</p>"},{"location":"90DaysOfDevOps/day34/","title":"#90DaysOfDevOps - Microsoft Azure Hands-On Scenarios - Day 34","text":""},{"location":"90DaysOfDevOps/day34/#microsoft-azure-hands-on-scenarios","title":"Microsoft Azure Hands-On Scenarios","text":"<p>The last 6 days have been focused on Microsoft Azure and the public cloud in general, a lot of this foundation had to contain a lot of theory to understand the building blocks of Azure but also this will nicely translate to the other major cloud providers as well.</p> <p>I mentioned at the very beginning about getting a foundational knowledge of the public cloud and choosing one provider to at least begin with, if you are dancing between different clouds then I believe you can get lost quite easily whereas choosing one you get to understand the fundamentals and when you have those it is quite easy to jump into the other clouds and accelerate your learning.</p> <p>In this final session, I am going to be picking and choosing my hands-on scenarios from this page here which is a reference created by Microsoft and is used for preparations for the AZ-104 Microsoft Azure Administrator</p> <p>There are some here such as Containers and Kubernetes that we have not covered in any detail as of yet so I don't want to jump in there just yet.</p> <p>In previous posts, we have created most of Modules 1,2 and 3.</p>"},{"location":"90DaysOfDevOps/day34/#virtual-networking","title":"Virtual Networking","text":"<p>Following Module 04:</p> <p>I went through the above and changed a few namings for #90DaysOfDevOps. I also instead of using the Cloud Shell went ahead and logged in with my new user created on previous days with the Azure CLI on my Windows machine.</p> <p>You can do this using the <code>az login</code> which will open a browser and let you authenticate to your account.</p> <p>I have then created a PowerShell script and some references from the module to use to build out some of the tasks below. You can find the associated files in this folder. (Cloud\\01VirtualNetworking)</p> <p>Please make sure you change the file location in the script to suit your environment.</p> <p>At this first stage, we have no virtual network or virtual machines created in our environment, I only have a cloud shell storage location configured in my resource group.</p> <p>I first of all run my PowerShell script</p> <p></p> <ul> <li>Task 1: Create and configure a virtual network</li> </ul> <p></p> <ul> <li>Task 2: Deploy virtual machines into the virtual network</li> </ul> <p></p> <ul> <li>Task 3: Configure private and public IP addresses of Azure VMs</li> </ul> <p></p> <ul> <li>Task 4: Configure network security groups</li> </ul> <p> </p> <ul> <li>Task 5: Configure Azure DNS for internal name resolution</li> </ul> <p> </p>"},{"location":"90DaysOfDevOps/day34/#network-traffic-management","title":"Network Traffic Management","text":"<p>Following Module 06:</p> <p>Next walkthrough, from the last one we have gone into our resource group and deleted our resources, if you had not set up the user account like me to only have access to that one resource group you could follow the module changing the name to <code>90Days*</code> this will delete all resources and resource group. This will be my process for each of the following labs.</p> <p>For this lab, I have also created a PowerShell script and some references from the module to use to build out some of the tasks below. You can find the associated files in this folder. (Cloud\\02TrafficManagement)</p> <ul> <li>Task 1: Provision of the lab environment</li> </ul> <p>I first of all run my PowerShell script</p> <p></p> <ul> <li>Task 2: Configure the hub and spoke network topology</li> </ul> <p></p> <ul> <li>Task 3: Test transitivity of virtual network peering</li> </ul> <p>For this my 90DaysOfDevOps group did not have access to the Network Watcher because of permissions, I expect this is because Network Watchers are one of those resources that are not tied to a resource group which is where our RBAC was covered for this user. I added the East US Network Watcher contributor role to the 90DaysOfDevOps group.</p> <p> </p> <p>^ This is expected since the two spoke virtual networks do not peer with each other (virtual network peering is not transitive).</p> <ul> <li>Task 4: Configure routing in the hub and spoke topology</li> </ul> <p>I had another issue here with my account not being able to run the script as my user within the group 90DaysOfDevOps which I am unsure of so I did jump back into my main admin account. The 90DaysOfDevOps group is an owner of everything in the 90DaysOfDevOps Resource Group so would love to understand why I cannot run a command inside the VM?</p> <p> </p> <p>I then was able to go back into my michael.cade@90DaysOfDevOps.com account and continue this section. Here we are running the same test again but now with the result being reachable.</p> <p></p> <ul> <li>Task 5: Implement Azure Load Balancer</li> </ul> <p> </p> <ul> <li>Task 6: Implement Azure Application Gateway</li> </ul> <p> </p>"},{"location":"90DaysOfDevOps/day34/#azure-storage","title":"Azure Storage","text":"<p>Following Module 07:</p> <p>For this lab, I have also created a PowerShell script and some references from the module to use to build out some of the tasks below. You can find the associated files in this folder. (Cloud\\03Storage)</p> <ul> <li>Task 1: Provision of the lab environment</li> </ul> <p>I first of all run my PowerShell script</p> <p></p> <ul> <li>Task 2: Create and configure Azure Storage accounts</li> </ul> <p></p> <ul> <li>Task 3: Manage blob storage</li> </ul> <p></p> <ul> <li>Task 4: Manage authentication and authorization for Azure Storage</li> </ul> <p> </p> <p>I was a little impatient waiting for this to be allowed but it did work eventually.</p> <p></p> <ul> <li>Task 5: Create and configure an Azure Files shares</li> </ul> <p>On the run command, this would not work with michael.cade@90DaysOfDevOps.com so I used my elevated account.</p> <p> </p> <ul> <li>Task 6: Manage network access for Azure Storage</li> </ul> <p></p>"},{"location":"90DaysOfDevOps/day34/#serverless-implement-web-apps","title":"Serverless (Implement Web Apps)","text":"<p>Following Module 09a:</p> <ul> <li>Task 1: Create an Azure web app</li> </ul> <p></p> <ul> <li>Task 2: Create a staging deployment slot</li> </ul> <p></p> <ul> <li>Task 3: Configure web app deployment settings</li> </ul> <p></p> <ul> <li>Task 4: Deploy code to the staging deployment slot</li> </ul> <p></p> <ul> <li>Task 5: Swap the staging slots</li> </ul> <p></p> <ul> <li>Task 6: Configure and test autoscaling of the Azure web app</li> </ul> <p>This script I am using can be found in (Cloud/04Serverless)</p> <p></p> <p>This wraps up the section on Microsoft Azure and the public cloud in general. I will say that I had lots of fun attacking and working through these scenarios.</p>"},{"location":"90DaysOfDevOps/day34/#resources","title":"Resources","text":"<ul> <li>Hybrid Cloud and MultiCloud</li> <li>Microsoft Azure Fundamentals</li> <li>Google Cloud Digital Leader Certification Course</li> <li>AWS Basics for Beginners - Full Course</li> </ul> <p>Next, we will be diving into version control systems, specifically around git and then also code repository overviews and we will be choosing GitHub as this is my preferred option.</p> <p>See you on Day 35</p>"},{"location":"90DaysOfDevOps/day35/","title":"#90DaysOfDevOps - The Big Picture: Git - Version Control - Day 35","text":""},{"location":"90DaysOfDevOps/day35/#the-big-picture-git-version-control","title":"The Big Picture: Git - Version Control","text":"<p>Before we get into git, we need to understand what version control is and why? In this opener for Git, we will take a look at what version control is, and the basics of git.</p>"},{"location":"90DaysOfDevOps/day35/#what-is-version-control","title":"What is Version Control?","text":"<p>Git is not the only version control system so here we want to cover what options and what methodologies are available around version control.</p> <p>The most obvious and a big benefit of Version Control is the ability to track a project's history. We can look back over this repository using <code>git log</code> and see that we have many commits and many comments and what has happened so far in the project. Don't worry we will get into the commands later. Now think if this was an actual software project full of source code and multiple people are committing to our software at different times, different authors and then reviewers all are logged here so that we know what has happened, when, by whom and who reviewed.</p> <p></p> <p>Version Control before it was cool, would have been something like manually creating a copy of your version before you made changes. It might be that you also comment out old useless code with the just-in-case mentality.</p> <p></p> <p>I have started using version control over not just source code but pretty much anything, talks about projects like this (90DaysOfDevOps). Why not accept the features that rollback and log of everything that has gone on. </p> <p>However, a big disclaimer Version Control is not a Backup!</p> <p>Another benefit of Version Control is the ability to manage multiple versions of a project, Let's create an example, we have a free app that is available on all operating systems and then we have a paid-for app also available on all operating systems. The majority of the code is shared between both applications. We could copy and paste our code each commit to each app but that is going to be very messy especially as you scale your development to more than just one person, also mistakes will be made.</p> <p>The premium app is where we are going to have additional features, let's call them premium commits, the free edition will just contain the normal commits.</p> <p>The way this is achieved in Version Control is through branching.</p> <p></p> <p>Branching allows for two code streams for the same app as we stated above. But we will still want new features that land in our source code-free version to be in our premium and to achieve this we have something called merging.</p> <p></p> <p>Now, this seems easy but merging can be complicated because you could have a team working on the free edition and you could have another team working on the premium paid-for version and what if both change code that affects aspects of the overall code. Maybe a variable gets updated and breaks something. Then you have a conflict that breaks one of the features. Version Control cannot fix the conflicts that are down to you. But version control allows this to be easily managed.</p> <p>The primary reason if you have not picked up so far for version control, in general, is the ability to collaborate. The ability to share code amongst developers and when I say code as I said before more and more we are seeing much more use cases for other reasons to use source control, maybe its a joint presentation you are working on with a colleague or a 90DaysOfDevOps challenge where you have the community offering their corrections and updates throughout the project.</p> <p>Without version control how did teams of software developers even handle this? I find it hard enough when I am working on my projects to keep track of things. I expect they would split out the code into each functional module. Maybe a little part of the puzzle then was bringing the pieces together and then problems and issues before anything would get released.</p> <p>With version control, we have a single source of truth. We might all still work on different modules but it enables us to collaborate better.</p> <p></p> <p>Another thing to mention here is that it's not just developers that can benefit from Version Control, it's all members of the team to have visibility but also tools all having awareness or leverage, Project Management tools can be linked here, tracking the work. We might also have a build machine for example Jenkins which we will talk about in another module. A tool that Builds and Packages the system, automating the deployment tests and metrics.</p>"},{"location":"90DaysOfDevOps/day35/#what-is-git","title":"What is Git?","text":"<p>Git is a tool that tracks changes to source code or any file, or we could also say Git is an open-source distributed version control system.</p> <p>There are many ways in which git can be used on our systems, most commonly or at least for me I have seen it at the command line, but we also have graphical user interfaces and tools like Visual Studio Code that have git-aware operations we can take advantage of.</p> <p>Now we are going to run through a high-level overview before we even get Git installed on our local machine.</p> <p>Let's take the folder we created earlier.</p> <p></p> <p>To use this folder with version control we first need to initiate this directory using the <code>git init</code> command. For now, just think that this command puts our directory as a repository in a database somewhere on our computer. </p> <p></p> <p>Now we can create some files and folders and our source code can begin or maybe it already has and we have something in here already. We can use the <code>git add .</code> command which puts all files and folders in our directory into a snapshot but we have not yet committed anything to that database. We are just saying all files with the <code>.</code> are ready to be added.</p> <p></p> <p>Then we want to go ahead and commit our files, we do this with the <code>git commit -m \"My First Commit\"</code> command. We can give a reason for our commit and this is suggested so we know what has happened for each commit.</p> <p></p> <p>We can now see what has happened within the history of the project. Using the <code>git log</code> command.</p> <p></p> <p>If we create an additional file called <code>samplecode.ps1</code>, the status would become different. We can also check the status of our repository by using <code>git status</code> this shows we have nothing to commit and we can add a new file called samplecode.ps1. If we then run the same <code>git status</code> you will see that we file to be committed. </p> <p></p> <p>Add our new file using the <code>git add samplecode.ps1</code> command and then we can run <code>git status</code> again and see our file is ready to be committed.</p> <p></p> <p>Then issue <code>git commit -m \"My Second Commit\"</code> command.</p> <p></p> <p>Another <code>git status</code> now shows everything is clean again.</p> <p></p> <p>We can then use the <code>git log</code> command which shows the latest changes and first commit.</p> <p></p> <p>If we wanted to see the changes between our commits i.e what files have been added or modified we can use the <code>git diff b8f8 709a</code></p> <p></p> <p>Which then displays what has changed in our case we added a new file.</p> <p></p> <p>We will go deeper into this later on but we can jump around our commits i.e we can go time travelling! By using our commit number we can use the <code>git checkout 709a</code> command to jump back in time without losing our new file. </p> <p></p> <p>But then equally we will want to move forward as well and we can do this the same way with the commit number or you can see here we are using the <code>git switch -</code> command to undo our operation.</p> <p></p> <p>The TLDR;</p> <ul> <li>Tracking a project's history</li> <li>Managing multiple versions of a project</li> <li>Sharing code amongst developers and a wider scope of teams and tools</li> <li>Coordinating teamwork</li> <li>Oh and there is some time travel!</li> </ul> <p>This might have seemed a jump around but hopefully, you can see without really knowing the commands used the powers and the big picture behind Version Control.</p> <p>Next up we will be getting git installed and set up on your local machine and diving a little deeper into some other use cases and commands that we can achieve in Git.</p>"},{"location":"90DaysOfDevOps/day35/#resources","title":"Resources","text":"<ul> <li>What is Version Control?</li> <li>Types of Version Control System</li> <li>Git Tutorial for Beginners</li> <li>Git for Professionals Tutorial</li> <li>Git and GitHub for Beginners - Crash Course</li> <li>Complete Git and GitHub Tutorial</li> </ul> <p>See you on Day 36</p>"},{"location":"90DaysOfDevOps/day36/","title":"#90DaysOfDevOps - Installing & Configuring Git - Day 36","text":""},{"location":"90DaysOfDevOps/day36/#installing-configuring-git","title":"Installing &amp; Configuring Git","text":"<p>Git is an open source, cross-platform tool for version control. If you are like me, using Ubuntu or most Linux environments you might find that you already have git installed but we are going to run through the install and configuration.</p> <p>Even if you already have git installed on your system it is also a good idea to make sure we are up to date.</p>"},{"location":"90DaysOfDevOps/day36/#installing-git","title":"Installing Git","text":"<p>As already mentioned Git is cross-platform, we will be running through Windows and Linux but you can find macOS also listed here</p> <p>For Windows we can grab our installers from the official site.</p> <p>You could also use <code>winget</code> on your Windows machine, think of this as your Windows Application Package Manager.</p> <p>Before we install anything let's see what version we have on our Windows Machine. Open a PowerShell window and run <code>git --version</code></p> <p></p> <p>We can also check our WSL Ubuntu version of Git as well.</p> <p></p> <p>At the time of writing the latest Windows release is <code>2.35.1</code> so we have some updating to do there which I will run through. I expect the same for Linux.</p> <p>I went ahead and downloaded the latest installer and ran through the wizard and will document that here. The important thing to note is that git will uninstall previous versions before installing the latest.</p> <p>Meaning that the process shown below is also the same process for the most part as if you were installing from no git.</p> <p>It is a very simple installation. Once downloaded double click and get started. Read through the GNU license agreement. But remember this is free and open-source software.</p> <p></p> <p>Now we can choose additional components that we would like to also install but also associate with git. On Windows, I always make sure I install Git Bash as this allows us to run bash scripts on Windows.</p> <p></p> <p>We can then choose which SSH Executable we wish to use. I leave this as the bundled OpenSSH that you might have seen in the Linux section.</p> <p></p> <p>We then have experimental features that we may wish to enable, for me I don't need them so I don't enable them, you can always come back in through the installation and enable these later on.</p> <p></p> <p>Installation complete, we can now choose to open Git Bash and or the latest release notes.</p> <p></p> <p>The final check is to take a look in our PowerShell window at what version of git we have now.</p> <p></p> <p>Super simple stuff and now we are on the latest version. On our Linux machine, we seemed to be a little behind so we can also walk through that update process.</p> <p>I simply run the <code>sudo apt-get install git</code> command.</p> <p></p> <p>You could also run the following which will add the git repository for software installations.</p> <pre><code>sudo add-apt-repository ppa:git-core/ppa -y\nsudo apt-get update\nsudo apt-get install git -y\ngit --version\n</code></pre>"},{"location":"90DaysOfDevOps/day36/#configuring-git","title":"Configuring Git","text":"<p>When we first use git we have to define some settings,</p> <ul> <li>Name</li> <li>Email</li> <li>Default Editor</li> <li>Line Ending</li> </ul> <p>This can be done at three levels</p> <ul> <li>System = All users</li> <li>Global = All repositories of the current user</li> <li>Local = The current repository</li> </ul> <p>Example: <code>git config --global user.name \"Michael Cade\"</code> <code>git config --global user.email Michael.Cade@90DaysOfDevOPs.com\"</code> Depending on your Operating System will determine the default text editor. In my Ubuntu machine without setting the next command is using nano. The below command will change this to visual studio code.</p> <p><code>git config --global core.editor \"code --wait\"</code></p> <p>now if we want to be able to see all git configurations then we can use the following command.</p> <p><code>git config --global -e</code></p> <p></p> <p>On any machine this file will be named <code>.gitconfig</code> on my Windows machine you will find this in your user account directory.</p> <p></p>"},{"location":"90DaysOfDevOps/day36/#git-theory","title":"Git Theory","text":"<p>I mentioned in the post yesterday that there were other version control types and we can split these down into two different types. One is Client Server and the other is Distributed.</p>"},{"location":"90DaysOfDevOps/day36/#client-server-version-control","title":"Client-Server Version Control","text":"<p>Before git was around Client-Server was the defacto method for version control. An example of this would be Apache Subversion which is an open source version control system founded in 2000.</p> <p>In this model of Client-Server version control, the first step the developer downloads the source code and the actual files from the server. This doesn't remove the conflicts but it does remove the complexity of the conflicts and how to resolve them.</p> <p></p> <p>Now for example let's say we have two developers working on the same files and one wins the race and commits or uploads their file back to the server first with their new changes. When the second developer goes to update they have a conflict.</p> <p></p> <p>So now the Dev needs to pull down the first devs code change next to their check and then commit once those conflicts have been settled.</p> <p></p>"},{"location":"90DaysOfDevOps/day36/#distributed-version-control","title":"Distributed Version Control","text":"<p>Git is not the only distributed version control system. But it is very much the defacto.</p> <p>Some of the major benefits of Git are:</p> <ul> <li>Fast</li> <li>Smart</li> <li>Flexible</li> <li>Safe &amp; Secure</li> </ul> <p>Unlike the Client-Server version control model, each developer downloads the source repository meaning everything. History of commits, all the branches etc.</p> <p></p>"},{"location":"90DaysOfDevOps/day36/#resources","title":"Resources","text":"<ul> <li>What is Version Control?</li> <li>Types of Version Control System</li> <li>Git Tutorial for Beginners</li> <li>Git for Professionals Tutorial</li> <li>Git and GitHub for Beginners - Crash Course</li> <li>Complete Git and GitHub Tutorial</li> </ul> <p>See you on Day 37</p>"},{"location":"90DaysOfDevOps/day37/","title":"#90DaysOfDevOps - Gitting to know Git - Day 37","text":""},{"location":"90DaysOfDevOps/day37/#gitting-to-know-git","title":"Gitting to know Git","text":"<p>Apologies for the terrible puns in the title and throughout. I am surely not the first person to turn Git into a dad joke!</p> <p>In the last two posts we learnt about version control systems, and some of the fundamental workflows of git as a version control system Day 35 Then we got git installed on our system, updated and configured. We also went a little deeper into the theory between the Client-Server version control system and Git which is a distributed version control system Day 36.</p> <p>Now we are going to run through some of the commands and use cases that we will all commonly see with git.</p>"},{"location":"90DaysOfDevOps/day37/#where-to-git-help-with-git","title":"Where to git help with git?","text":"<p>There are going to be times when you just cannot remember or just don't know the command you need to get things done with git. You are going to need help.</p> <p>Google or any search engine is likely to be your first port of call when searching for help.</p> <p>Secondly, the next place is going to be the official git site and the documentation. git-scm.com/docs Here you will find not only a solid reference to all the commands available but also lots of different resources.</p> <p></p> <p>We can also access this same documentation which is super useful if you are without connectivity from the terminal. If we chose the <code>git add</code> command for example we can run <code>git add --help</code> and we see below the manual.</p> <p></p> <p>We can also in the shell use <code>git add -h</code> which is going to give us a summary of the options we have available.</p> <p></p>"},{"location":"90DaysOfDevOps/day37/#myths-surrounding-git","title":"Myths surrounding Git","text":"<p>\"Git has no access control\" - You can empower a leader to maintain source code.</p> <p>\"Git is too heavy\" - Git can provide shallow repositories which means a reduced amount of history if you have large projects.</p>"},{"location":"90DaysOfDevOps/day37/#real-shortcomings","title":"Real shortcomings","text":"<p>Not ideal for Binary files. Great for source code but not great for executable files or videos for example.</p> <p>Git is not user-friendly, the fact that we have to spend time talking about commands and functions of the tool is probably a key sign of that.</p> <p>Overall though, git is hard to learn but easy to use.</p>"},{"location":"90DaysOfDevOps/day37/#the-git-ecosystem","title":"The git ecosystem","text":"<p>I want to briefly cover the ecosystem around git but not deep dive into some of these areas but I think it's important to note these here at a high level.</p> <p>Almost all modern development tools support Git.</p> <ul> <li>Developer tools - We have already mentioned visual studio code but you will find git plugins and integrations into sublime text and other text editors and IDEs.</li> <li> <p>Team tools - Also mentioned around tools like Jenkins from a CI/CD point of view, Slack from a messaging framework and Jira for project management and issue tracking.</p> </li> <li> <p>Cloud Providers - All the large cloud providers support git, Microsoft Azure, Amazon AWS, and Google Cloud Platform.</p> </li> <li>Git-Based services - Then we have GitHub, GitLab and BitBucket which we will cover in more detail later on. I have heard of these services as the social network for code!</li> </ul>"},{"location":"90DaysOfDevOps/day37/#the-git-cheatsheet","title":"The Git Cheatsheet","text":"<p>We have not covered most of these commands but having looked at some cheat sheets available online I wanted to document some of the git commands and what their purpose is. We don't need to remember these all, and with more hands-on practice and use you will pick at least the git basics.</p> <p>I have taken these from atlassian but writing them down and reading the description is a good way to get to know what the commands are as well as getting hands-on in everyday tasks.</p>"},{"location":"90DaysOfDevOps/day37/#git-basics","title":"Git Basics","text":"Command Example Description git init <code>git init &lt;directory&gt;</code> Create an empty git repository in the specified directory. git clone <code>git clone &lt;repo&gt;</code> Clone repository located at \\ onto local machine. git config <code>git config user.name</code> Define author name to be used for all commits in current repository <code>system</code>, <code>global</code>, <code>local</code> flag to set config options. git add <code>git add &lt;directory&gt;</code> Stage all changes in \\ for the next commit. We can also add \\ and \\&lt;.&gt; for everything. git commit -m <code>git commit -m \"&lt;message&gt;\"</code> Commit the staged snapshot, use \\ to detail what is being committed. git status <code>git status</code> List files that are staged, unstaged and untracked. git log <code>git log</code> Display all commit history using the default format. There are additional options with this command. git diff <code>git diff</code> Show unstaged changes between your index and working directory."},{"location":"90DaysOfDevOps/day37/#git-undoing-changes","title":"Git Undoing Changes","text":"Command Example Description git revert <code>git revert &lt;commit&gt;</code> Create a new commit that undoes all of the changes made in \\ then apply it to the current branch. git reset <code>git reset &lt;file&gt;</code> Remove \\ from the staging area, but leave the working directory unchanged. This unstaged a file without overwriting any changes. git clean <code>git clean -n</code> Shows which files would be removed from the working directory. Use <code>-f</code> in place of <code>-n</code> to execute the clean."},{"location":"90DaysOfDevOps/day37/#git-rewriting-history","title":"Git Rewriting History","text":"Command Example Description git commit <code>git commit --amend</code> Replace the last commit with the staged changes and the last commit combined. Use with nothing staged to edit the last commit\u2019s message. git rebase <code>git rebase &lt;base&gt;</code> Rebase the current branch onto \\. \\ can be a commit ID, branch name, a tag, or a relative reference to HEAD. git reflog <code>git reflog</code> Show a log of changes to the local repository\u2019s HEAD. Add --relative-date flag to show date info or --all to show all refs."},{"location":"90DaysOfDevOps/day37/#git-branches","title":"Git Branches","text":"Command Example Description git branch <code>git branch</code> List all of the branches in your repo. Add a \\ argument to create a new branch with the name \\. git checkout <code>git checkout -b &lt;branch&gt;</code> Create and check out a new branch named \\. Drop the -b flag to checkout an existing branch. git merge <code>git merge &lt;branch&gt;</code> Merge \\ into the current branch."},{"location":"90DaysOfDevOps/day37/#git-remote-repositories","title":"Git Remote Repositories","text":"Command Example Description git remote add <code>git remote add &lt;name&gt; &lt;url&gt;</code> Create a new connection to a remote repo. After adding a remote, you can use \\ as a shortcut for \\ in other commands. git fetch <code>git fetch &lt;remote&gt; &lt;branch&gt;</code> Fetches a specific \\, from the repo. Leave off \\ to fetch all remote refs. git pull <code>git pull &lt;remote&gt;</code> Fetch the specified remote\u2019s copy of current branch and immediately merge it into the local copy. git push <code>git push &lt;remote&gt; &lt;branch&gt;</code> Push the branch to \\, along with necessary commits and objects. Creates named branch in the remote repo if it doesn\u2019t exist."},{"location":"90DaysOfDevOps/day37/#git-diff","title":"Git Diff","text":"Command Example Description git diff HEAD <code>git diff HEAD</code> Show the difference between the working directory and the last commit. git diff --cached <code>git diff --cached</code> Show difference between staged changes and last commit"},{"location":"90DaysOfDevOps/day37/#git-config","title":"Git Config","text":"Command Example Description git config --global user.name \\ <code>git config --global user.name &lt;name&gt;</code> Define the author name to be used for all commits by the current user. git config --global user.email \\ <code>git config --global user.email &lt;email&gt;</code> Define author email to be used for all commits by the current user. git config --global alias \\ \\ <code>git config --global alias &lt;alias-name&gt; &lt;git-command&gt;</code> Create shortcut for a git command . git config --system core.editor \\ <code>git config --system core.editor &lt;editor&gt;</code> Set the text editor to be used by commands for all users on the machine. \\ arg should be the comamnd that launches the desired editor. git config --global --edit <code>git config --global --edit</code> Open the global configuration file in a text editor for manual editing."},{"location":"90DaysOfDevOps/day37/#git-rebase","title":"Git Rebase","text":"Command Example Description git rebase -i \\ <code>git rebase -i &lt;base&gt;</code> Interactively rebase current branch onto \\. Launches editor to enter commands for how each commit will be transferred to the new base."},{"location":"90DaysOfDevOps/day37/#git-pull","title":"Git Pull","text":"Command Example Description git pull --rebase \\ <code>git pull --rebase &lt;remote&gt;</code> Fetch the remote\u2019s copy of current branch and rebases it into the local copy. Uses git rebase instead of the merge to integrate the branches."},{"location":"90DaysOfDevOps/day37/#git-reset","title":"Git Reset","text":"Command Example Description git reset <code>git reset</code> Reset the staging area to match the most recent commit but leave the working directory unchanged. git reset --hard <code>git reset --hard</code> Reset staging area and working directory to match most recent commit and overwrites all changes in the working directory git reset \\ <code>git reset &lt;commit&gt;</code> Move the current branch tip backwards to \\, reset the staging area to match, but leave the working directory alone git reset --hard \\ <code>git reset --hard &lt;commit&gt;</code> Same as previous, but resets both the staging area &amp; working directory to match. Deletes uncommitted changes, and all commits after \\."},{"location":"90DaysOfDevOps/day37/#git-push","title":"Git Push","text":"Command Example Description git push \\ --force <code>git push &lt;remote&gt; --force</code> Forces the git push even if it results in a non-fast-forward merge. Do not use the --force flag unless you\u2019re sure you know what you\u2019re doing. git push \\ --all <code>git push &lt;remote&gt; --all</code> Push all of your local branches to the specified remote. git push \\ --tags <code>git push &lt;remote&gt; --tags</code> Tags aren\u2019t automatically pushed when you push a branch or use the --all flag. The --tags flag sends all of your local tags to the remote repo."},{"location":"90DaysOfDevOps/day37/#resources","title":"Resources","text":"<ul> <li>What is Version Control?</li> <li>Types of Version Control System</li> <li>Git Tutorial for Beginners</li> <li>Git for Professionals Tutorial</li> <li>Git and GitHub for Beginners - Crash Course</li> <li>Complete Git and GitHub Tutorial</li> <li>Git cheatsheet</li> </ul> <p>See you on Day 38</p>"},{"location":"90DaysOfDevOps/day38/","title":"#90DaysOfDevOps - Staging & Changing - Day 38","text":""},{"location":"90DaysOfDevOps/day38/#staging-changing","title":"Staging &amp; Changing","text":"<p>We have already covered some of the basics but putting things into a walkthrough makes it better for me to learn and understand how and why we are doing it this way. Before we get into any git-based services such as GitHub, git has its powers that we can take advantage of on our local workstation.</p> <p>We are going to take the project folder we created at the start of the git session and we are going to walk through some of the simple steps we can do with git. We created a folder on our local machine and we initialised it with the <code>git init</code> command</p> <p></p> <p>We can also see now that we have initialised the folder we have a hidden folder in our directory.</p> <p></p> <p>This is where the details of the git repository are stored as well as the information regarding our branches and commits.</p>"},{"location":"90DaysOfDevOps/day38/#staging-files","title":"Staging Files","text":"<p>We then start working on our empty folder and maybe we add some source code on the first days of work. We create our readme.mdfile and we can see that file in the directory, next we check our <code>git status</code> and it knows about the new readme.mdfile but we have not committed the file yet.</p> <p></p> <p>We can stage our readme.mdfile with the <code>git add README.md</code> command then we can see changes to be committed that we did not have before and a green new file.</p> <p></p> <p>Next up we want to commit this, our first commit or our first snapshot of our project. We can do this by using the <code>git commit -m \"Meaningful message\"</code> command so that we can easily see what has changed for each commit. Also, notice the yellow cross changes now to a green tick. This is something I have within my terminal with the theme I use, something we covered in the Linux section.</p> <p></p>"},{"location":"90DaysOfDevOps/day38/#committing-changes","title":"Committing Changes","text":"<p>We are going to most likely want to add more files or even change the files we have in our directory. We have already done our first commit above. But now we are going to add more details and more files.</p> <p>We could repeat our process from before, create or edit our file &gt; <code>git add .</code> to add all files to the staging area then <code>git commit -m \"meaningful message\"</code> and this would work just fine. But to be able to offer a meaningful message on commit of what has changed you might not want to write something out like <code>git commit -m \"Well, I changed some code because it did not work and when I fixed that I also added something new to the readme.mdto ensure everyone knew about the user experience and then I made a tea.\"</code> I mean this would work as well although probably make it descriptive but the preferred way here is to add this with a text editor.</p> <p>If we run <code>git commit</code> after running <code>git add</code> it will open our default text editor which in my case here is nano. Here are the steps I took to add some changes to the file, ran <code>git status</code> to show what is and what is not staged. Then I used <code>git add</code> to add the file to the staging area, then ran <code>git commit</code> which opened nano.</p> <p></p> <p>When nano opens you can then add your short and long description and then save the file.</p> <p></p>"},{"location":"90DaysOfDevOps/day38/#committing-best-practices","title":"Committing Best Practices","text":"<p>There is a balance here between when to commit and commit often. We don't want to wait to the end of the project before committing, each commit should be meaningful and they also should not be coupled with non-relevant tasks with each other. If you have a bug fix and a typo make sure they are two separate commits as a best practice.</p> <p>Make the commit message mean something.</p> <p>In terms of wording, the team or yourself should be sticking to the same wording for each commit.</p>"},{"location":"90DaysOfDevOps/day38/#skipping-the-staging-area","title":"Skipping the Staging Area","text":"<p>Do we always have to stage our changes before committing them?</p> <p>The answer is yes but don't see this as a shortcut, you have to be sure 100% that you are not needing that snapshot to roll back to, it is a risky thing to do.</p> <p></p>"},{"location":"90DaysOfDevOps/day38/#removing-files","title":"Removing Files","text":"<p>What about removing files from our project, maybe we have another file in our directory that we have committed but now the project no longer needs or using it, as a best practice we should remove it.</p> <p>Just because we remove the file from the directory, git is still aware of this file and we also need to remove it from the repository. You can see the workflow for this below.</p> <p></p> <p>That could be a bit of a pain to either remember or have to deal with if you have a large project which has many moving files and folders. We can do this with one command with <code>git rm oldcode.ps1</code></p> <p></p>"},{"location":"90DaysOfDevOps/day38/#renaming-or-moving-files","title":"Renaming or Moving Files","text":"<p>Within our operating system, we can rename and move our files. We will no doubt need to do this from time to time with our projects. Similar to removing though there is a two-step process, we change our files on our OS and then we have to modify and make sure that the staging area or that the files are added correctly. Steps as follows:</p> <p></p> <p>However, like removing files from the operating system and then the git repository we can perform this rename using a git command too.</p> <p></p>"},{"location":"90DaysOfDevOps/day38/#ignoring-files","title":"Ignoring Files","text":"<p>We may have the requirement to ignore files or folders within our project that we might be using locally or that will be just wasted space if we were to share with the overall project, a good example of this could be logs. I also think using this for secrets that you do not want to be shared out in public or across teams.</p> <p>We can ignore files by adding folders or files to the <code>.gitignore</code> file in our project directory.</p> <p></p> <p>You can then open the <code>.gitignore</code> file and see that we have the logs/ directory present. But we could also add additional files and folders here to ignore.</p> <p></p> <p>We can then see <code>git status</code> and then see what has happened.</p> <p></p> <p>There are also ways in which you might need to go back and ignore files and folders, maybe you did want to share the logs folder but then later realised that you didn't want to. You will have to use <code>git rm --cached</code> to remove files and folders from the staging area if you have a previously tracked folder that you now want to ignore.</p>"},{"location":"90DaysOfDevOps/day38/#short-status","title":"Short Status","text":"<p>We have been using <code>git status</code> a lot to understand what we have in our staging area and what we do not, it's a very comprehensive command with lots of detail. Most of the time you will just want to know what has been modified or what is new? We can use <code>git status -s</code> for a short status of this detail. I would usually set an alias on my system to just use <code>git status -s</code> vs the more detailed command.</p> <p></p> <p>In the post tomorrow we will continue to look through these short examples of these common git commands.</p>"},{"location":"90DaysOfDevOps/day38/#resources","title":"Resources","text":"<ul> <li>What is Version Control?</li> <li>Types of Version Control System</li> <li>Git Tutorial for Beginners</li> <li>Git for Professionals Tutorial</li> <li>Git and GitHub for Beginners - Crash Course</li> <li>Complete Git and GitHub Tutorial</li> <li>Git cheatsheet</li> </ul> <p>See you on Day 39</p>"},{"location":"90DaysOfDevOps/day39/","title":"#90DaysOfDevOps - Viewing, unstaging, discarding & restoring - Day 39","text":""},{"location":"90DaysOfDevOps/day39/#viewing-unstaging-discarding-restoring","title":"Viewing, unstaging, discarding &amp; restoring","text":"<p>Continuing from where we finished yesterday around some of the commands that we have with git and how to leverage git with your projects. Remember we have not touched GitHub or any other git-based services yet this is all to help you keep control of your projects locally at the moment, but they will all become useful when we start to integrate into those tools.</p>"},{"location":"90DaysOfDevOps/day39/#viewing-the-staged-and-unstaged-changes","title":"Viewing the Staged and Unstaged Changes","text":"<p>It is good practice to make sure you view the staged and unstaged code before committing. We can do this by running the <code>git diff --staged</code> command</p> <p></p> <p>This then shows us all the changes we have made and all new files we have added or deleted.</p> <p>changes in the modified files are indicated with <code>---</code> or <code>+++</code> you can see below that we just added +add some text below which means they are new lines.</p> <p></p> <p>We can also run <code>git diff</code> to compare our staging area with our working directory. If we make some changes to our newly added file code.txt and add some lines of text.</p> <p></p> <p>If we then run <code>git diff</code> we compare and see the output below.</p> <p></p>"},{"location":"90DaysOfDevOps/day39/#visual-diff-tools","title":"Visual Diff Tools","text":"<p>For me, the above is more confusing so I would much rather use a visual tool,</p> <p>To name a few visual diff tools:</p> <ul> <li>KDiff3</li> <li>P4Merge</li> <li>WinMerge (Windows Only)</li> <li>VSCode</li> </ul> <p>To set this in git we run the following command <code>git config --global diff.tool vscode</code></p> <p>We are going to run the above and we are going to set some parameters when we launch VScode.</p> <p></p> <p>We can also check our configuration with <code>git config --global -e</code></p> <p></p> <p>We can then use <code>git difftool</code> to now open our diff visual tool.</p> <p></p> <p>Which then opens our VScode editor on the diff page and compares the two, we have only modified one file from nothing to now adding a line of code on the right side.</p> <p></p> <p>I find this method much easier to track changes and this is something similar to what we will see when we look into git-based services such as GitHub.</p> <p>We can also use <code>git difftool --staged</code> to compare stage with committed files.</p> <p></p> <p>Then we can cycle through our changed files before we commit.</p> <p></p> <p>I am using VScode as my IDE and like most IDEs they have this functionality built in it is very rare you would need to run these commands from the terminal, although helpful if you don't have an IDE installed for some reason.</p>"},{"location":"90DaysOfDevOps/day39/#viewing-the-history","title":"Viewing the History","text":"<p>We previously touched on <code>git log</code> which will provide us with a comprehensive view of all commits we have made in our repository.</p> <p></p> <p>Each commit has its hexadecimal string, unique to the repository. Here you can see which branch we are working on and then also the author, date and commit message.</p> <p>We also have <code>git log --oneline</code> and this gives us a much smaller version of the hexadecimal string which we can use in other <code>diff</code> commands. We also only have the one-line description or commit message.</p> <p></p> <p>We can reverse this into a start with the first commit by running <code>git log --oneline --reverse</code> and now we see our first commit at the top of our page.</p> <p></p>"},{"location":"90DaysOfDevOps/day39/#viewing-a-commit","title":"Viewing a Commit","text":"<p>Being able to look at the commit message is great if you have been conscious about following best practices and you have added a meaningful commit message, however, there is also <code>git show</code> command which allows us to inspect and view a commit.</p> <p>We can use <code>git log --oneline --reverse</code> to get a list of our commits. and then we can take those and run <code>git show &lt;commit ID&gt;</code></p> <p></p> <p>The output of that command will look like below with the detail of the commit, author and what changed.</p> <p></p> <p>We can also use <code>git show HEAD~1</code> where 1 is how many steps back from the current version we want to get back to.</p> <p>This is great if you want some detail on your files, but if we want to list all the files in a tree for the whole snapshot directory. We can achieve this by using the <code>git ls-tree HEAD~1</code> command, again going back one snapshot from the last commit. We can see below we have two blobs, these indicate files whereas the tree would indicate a directory. You can also see commits and tags in this information.</p> <p></p> <p>We can then use the above to drill in and see the contents of our file (blobs) using the <code>git show</code> command.</p> <p></p> <p>Then the contents of that specific version of the file will be shown.</p> <p></p>"},{"location":"90DaysOfDevOps/day39/#unstaging-files","title":"Unstaging Files","text":"<p>There will be a time when you have maybe used <code>git add .</code> but there are files you do not wish to commit to that snapshot just yet. In this example below I have added newfile.txt to my staging area but I am not ready to commit this file so I am going to use the <code>git restore --staged newfile.txt</code> to undo the <code>git add</code> step.</p> <p></p> <p>We can also do the same to modified files such as main.js and unstage the commit, see above we have a green M for modified and then below we are unstaging those changes.</p> <p></p> <p>I have found this command quite useful during the 90DaysOfDevOps as I sometimes work ahead of the days where I feel I want to make notes for the following day but I don't want to commit and push to the public GitHub repository.</p>"},{"location":"90DaysOfDevOps/day39/#discarding-local-changes","title":"Discarding Local Changes","text":"<p>Sometimes we might make changes but we are not happy with those changes and we want to throw them away. We are going to use the <code>git restore</code> command again and we are going to be able to restore files from our snapshots or previous versions. We can run <code>git restore .</code> against our directory and we will restore everything from our snapshot but notice that our untracked file is still present. There is no previous file being tracked called newfile.txt.</p> <p></p> <p>Now to remove newfile.txt or any untracked files. We can use <code>git clean</code> we will get a warning alone.</p> <p></p> <p>Or if we know the consequences then we might want to run <code>git clean -fd</code> to force and remove all directories.</p> <p></p>"},{"location":"90DaysOfDevOps/day39/#restoring-a-file-to-an-earlier-version","title":"Restoring a File to an Earlier Version","text":"<p>As we have alluded to throughout a big portion of what Git can help with is being able to restore copies of your files from your snapshots (this is not a backup but it is a very fast restore point) My advice is that you also save copies of your code in other locations using a backup solution for this.</p> <p>As an example let's go and delete our most important file in our directory, notice we are using Unix-based commands to remove this from the directory, not git commands.</p> <p></p> <p>Now we have no readme.md in our working directory. We could have used <code>git rm readme.md</code> and this would then be reflected in our git database. Let's also delete it from here to simulate it being removed completely.</p> <p></p> <p>Let's now commit this with a message and prove that we no longer have anything in our working directory or staging area.</p> <p></p> <p>Mistakes were made and we now need this file back!</p> <p>We could use the <code>git undo</code> command which will undo the last commit, but what if it was a while back? We can use our <code>git log</code> command to find our commits and then we find that our file is in the last commit but we don't all of those commits to be undone so we can then use this command <code>git restore --source=HEAD~1 README.md</code> to specifically find the file and restore it from our snapshot.</p> <p>You can see using this process we now have the file back in our working directory.</p> <p></p> <p>We now have a new untracked file and we can use our commands previously mentioned to track, stage and commit our files and changes.</p>"},{"location":"90DaysOfDevOps/day39/#rebase-vs-merge","title":"Rebase vs Merge","text":"<p>This seems to be the biggest headache when it comes to Git and when to use rebase vs using merge on your git repositories.</p> <p>The first thing to know is that both <code>git rebase</code> and <code>git merge</code> solve the same problem. Both are to integrate changes from one branch into another branch. However, they do this in different ways.</p> <p>Let's start with a new feature in a new dedicated branch. The Main branch continues with new commits.</p> <p></p> <p>The easy option here is to use <code>git merge feature main</code> which will merge the main branch into the feature branch.</p> <p></p> <p>Merging is easy because it is non-destructive. The existing branches are not changed in any way. However, this also means that the feature branch will have an irrelevant merge commit every time you need to incorporate upstream changes. If the main is very busy or active this will or can pollute the feature branch history.</p> <p>As an alternate option, we can rebase the feature branch onto the main branch using</p> <pre><code>git checkout feature\ngit rebase main\n</code></pre> <p>This moves the feature branch (the entire feature branch) effectively incorporating all of the new commits in the main. But, instead of using a merge commit, rebasing re-writes the project history by creating brand new commits for each commit in the original branch.</p> <p></p> <p>The biggest benefit of rebasing is a much cleaner project history. It also eliminates unnecessary merge commits. and as you compare the last two images, you can follow arguably a much cleaner linear project history.</p> <p>Although it's still not a foregone conclusion, choosing the cleaner history also comes with tradeoffs, If you do not follow the The Golden rule of rebasing re-writing project history can be potentially catastrophic for your collaboration workflow. And, less importantly, rebasing loses the context provided by a merge commit\u2014you can\u2019t see when upstream changes were incorporated into the feature.</p>"},{"location":"90DaysOfDevOps/day39/#resources","title":"Resources","text":"<ul> <li>What is Version Control?</li> <li>Types of Version Control System</li> <li>Git Tutorial for Beginners</li> <li>Git for Professionals Tutorial</li> <li>Git and GitHub for Beginners - Crash Course</li> <li>Complete Git and GitHub Tutorial</li> <li>Git cheatsheet</li> <li>Exploring the Git command line \u2013 A getting started guide</li> </ul> <p>See you on Day40</p>"},{"location":"90DaysOfDevOps/day40/","title":"#90DaysOfDevOps - Social Network for code - Day 40","text":""},{"location":"90DaysOfDevOps/day40/#social-network-for-code","title":"Social Network for code","text":"<p>Exploring GitHub | GitLab | BitBucket</p> <p>Today I want to cover some of the git-based services that we have likely all heard of and expect we also use daily.</p> <p>We will then use some of our prior session knowledge to move copies of our data to each of the main services.</p> <p>I called this section \"Social Network for Code\" let me explain why?</p>"},{"location":"90DaysOfDevOps/day40/#github","title":"GitHub","text":"<p>Most common at least for me is GitHub, GitHub is a web-based hosting service for git. It is most commonly used by software developers to store their code. Source Code Management with the git version control features as well as a lot of additional features. It allows for teams or open contributors to easily communicate and provides a social aspect to coding. (hence the social networking title) Since 2018 GitHub is part of Microsoft.</p> <p>GitHub has been around for quite some time and was founded in 2007/2008. With Over 40 million users on the platform today.</p> <p>GitHub Main Features</p> <ul> <li>Code Repository</li> <li>Pull Requests</li> <li>Project Management toolset - Issues</li> <li>CI / CD Pipeline - GitHub Actions</li> </ul> <p>In terms of pricing, GitHub has different levels of pricing for its users. More can be found on Pricing</p> <p>For this, we will cover the free tier.</p> <p>I am going to be using my already created GitHub account during this walkthrough, if you do not have an account then on the opening GitHub page there is a sign-up option and some easy steps to get set up.</p>"},{"location":"90DaysOfDevOps/day40/#github-opening-page","title":"GitHub opening page","text":"<p>When you first log in to your GitHub account you get a page containing a lot of widgets giving you options of where and what you would like to see or do. First up we have the \"All Activity\" this is going to give you a look into what is happening with your repositories or activity in general associated with your organisation or account.</p> <p></p> <p>Next, we have our Code Repositories, either our own or repositories that we have interacted with recently. We can also quickly create new repositories or search repositories.</p> <p></p> <p>We then have our recent activity, these for me are issues and pull requests that I have created or contributed to recently.</p> <p></p> <p>Over on the right side of the page, we have some referrals for repositories that we might be interested in, most likely based on your recent activity or own projects.</p> <p></p> <p>To be honest I am very rarely on my home page that we just saw and described, although I now see that the feed could be really useful to help interact with the community a little better on certain projects.</p> <p>Next up if we want to head into our GitHub Profile we can navigate to the top right corner and on your image, there is a drop-down which allows you to navigate through your account. From here to access your Profile select \"Your Profile\"</p> <p></p> <p>Next, your profile page will appear, by default, unless you change your configuration you are not going to see what I have, I have added some functionality that shows my recent blog posts over on vZilla and then also my latest videos on my YouTube Channel.</p> <p>You are not going to be spending much time looking at your profile, but this is a good profile page to share around your network so they can see the cool projects you are working on.</p> <p></p> <p>We can then drill down into the building block of GitHub, the repositories. Here you are going to see your repositories and if you have private repositories they are also going to be shown in this long list.</p> <p></p> <p>As the repository is so important to GitHub let me choose a pretty busy one of late and run through some of the core functionality that we can use here on top of everything I am already using when it comes to editing our \"code\" in git on my local system.</p> <p>First of all, from the previous window, I have selected the 90DaysOfDevOps repository and we get to see this view. You can see from this view we have a lot of information, we have our main code structure in the middle showing our files and folders that are stored in our repository. We have our readme. mdbeing displayed down at the bottom. Over to the right of the page, we have an about section where the repository has a description and purpose. Then we have a lot of information underneath this showing how many people have starred in the project, forked, and watched.</p> <p></p> <p>If we scroll down a little further you will also see that we have Released, these are from the golang part of the challenge. We do not have any packages in our project, we have our contributors listed here. (Thank you community for assisting in my spelling and fact checking) We then have languages used again these are from different sections in the challenge.</p> <p></p> <p>A the top of the page you are going to see a list of tabs. These may vary and these can be modified to only show the ones you require. You will see here that I am not using all of these and I should remove them to make sure my whole repository is tidy.</p> <p>First up we had the code tab which we just discussed but these tabs are always available when navigating through a repository which is super useful so we can jump between sections quickly and easily. Next, we have the issues tab.</p> <p>Issues let you track your work on GitHub, where development happens. In this specific repository you can see I have some issues focused on adding diagrams or typos but also we have an issue stating a need or requirement for a Chinese version of the repository.</p> <p>If this was a code repository then this is a great place to raise concerns or issues with the maintainers, but remember to be mindful and detailed about what you are reporting, and give as much detail as possible.</p> <p></p> <p>The next tab is Pull Requests, Pull requests let you tell others about changes you've pushed to a branch in a repository. This is where someone may have forked your repository, made changes such as bug fixes or feature enhancements or just typos in a lot of the cases in this repository.</p> <p>We will cover forking later on.</p> <p></p> <p>I believe the next tab is quite new? But I thought for a project like #90DaysOfDevOps this could help guide the content journey but also help the community as they walk through their learning journey. I have created some discussion groups for each section of the challenge so people can jump in and discuss.</p> <p></p> <p>The Actions tab is going to enable you to build, test and deploy code and a lot more right from within GitHub. GitHub Actions will be something we cover in the CI/CD section of the challenge but this is where we can set some configuration here to automate steps for us.</p> <p>On my main GitHub Profile, I am using GitHub Actions to fetch the latest blog posts and YouTube videos to keep things up to date on that home screen.</p> <p></p> <p>I mentioned above how GitHub is not just a source code repository but is also a project management tool, The Project tab enables us to build out project tables kanban type boards so that we can link issues and PRs to better collaborate on the project and have visibility of those tasks.</p> <p></p> <p>I know that issues to me seem like a good place to log feature requests and they are but the wiki page allows for a comprehensive roadmap for the project to be outlined with the current status and in general better document your project is it troubleshooting or how-to type content.</p> <p></p> <p>Not so applicable to this project but the Security tab is there to make sure that contributors know how to deal with certain tasks, we can define a policy here but also code scanning add-ons to make sure your code for example does not contain secret environment variables.</p> <p></p> <p>For me the insights tab is great, it provides so much information about the repository from how much activity has been going on down to commits and issues, but it also reports on traffic to the repository. You can see a list on the left side that allows you to go into great detail about metrics on the repository.</p> <p></p> <p>Finally, we have the Settings tab, this is where we can get into the details of how we run our repository, I am currently the only maintainer of the repository but we could share this responsibility here. We can define integrations and other such tasks here.</p> <p></p> <p>This was a super quick overview of GitHub, I think there are some other areas that I might have mentioned that need explaining in a little more detail. As mentioned GitHub houses millions of repositories mostly these are holding source code and these can be public or privately accessible.</p>"},{"location":"90DaysOfDevOps/day40/#forking","title":"Forking","text":"<p>I am going to get more into Open-Source in the session tomorrow but a big part of any code repository is the ability to collaborate with the community. Let's think of the scenario I want a copy of a repository because I want to make some changes to it, maybe I want to fix a bug or maybe I want to change something to use it for a use case that I have that was maybe not the intended use case for the original maintainer of the code. This is what we would call forking a repository. A fork is a copy of a repository. Forking a repository allows you to freely experiment with changes without affecting the original project.</p> <p>Let me head back to the opening page after login and see one of those suggested repositories.</p> <p></p> <p>If we click on that repository we are going to get the same look as we have just walked through on the 90DaysOfDevOps repository.</p> <p></p> <p>If we notice below we have 3 options, we have watch, fork and star.</p> <ul> <li>Watch - Updates when things happen to the repository.</li> <li>Fork - a copy of a repository.</li> <li>Star - \"I think your project is cool\"</li> </ul> <p></p> <p>Given our scenario of wanting a copy of this repository to work on we are going to hit the fork option. If you are a member of multiple organisations then you will have to choose where the fork will take place, I am going to choose my profile.</p> <p></p> <p>Now we have our copy of the repository that we can freely work on and change as we see fit. This would be the start of the pull request process that we mentioned briefly before but we will cover it in more detail tomorrow.</p> <p></p> <p>Ok, I hear you say, but how do I make changes to this repository and code if it's on a website, well you can go through and edit on the website but it's not going to be the same as using your favourite IDE on your local system with your favourite colour theme. For us to get a copy of this repository on our local machine we will perform a clone of the repository. This will allow us to work on things locally and then push our changes back into our forked copy of the repository.</p> <p>We have several options when it comes to getting a copy of this code as you can see below.</p> <p>There is a local version available of GitHub Desktop which gives you a visual desktop application to track changes and push and pull changes between local and GitHub.</p> <p>For this little demo, I am going to use the HTTPS URL we see on there.</p> <p></p> <p>Now on our local machine, I am going to navigate to a directory I am happy to download this repository to and then run <code>git clone url</code></p> <p></p> <p>Now we could take it to VScode to make some changes to this.</p> <p></p> <p>Let's now make some changes, I want to make a change to all those links and replace that with something else.</p> <p></p> <p>Now if we check back on GitHub and we find our readme.mdin that repository, you should be able to see a few changes that I made to the file.</p> <p></p> <p>At this stage, this might be complete and we might be happy with our change as we are the only people going to use our new change but maybe it was a bug change and if that is the case then we will want to contribute via a Pull Request to notify the original repository maintainers of our change and see if they accept our changes.</p> <p>We can do this by using the contribute button highlighted below. I will cover more on this tomorrow when we look into Open-Source workflows.</p> <p></p> <p>I have spent a long time looking through GitHub and I hear some of you cry but what about other options!</p> <p>Well, there are and I am going to find some resources that cover the basics for some of those as well. You are going to come across GitLab and BitBucket amongst others in your travels and whilst they are git-based services they have their differences.</p> <p>You will also come across hosted options. Most commonly here I have seen GitLab as a hosted version vs GitHub Enterprise (Don't believe there is a free hosted GitHub?)</p>"},{"location":"90DaysOfDevOps/day40/#resources","title":"Resources","text":"<ul> <li>Learn GitLab in 3 Hours | GitLab Complete Tutorial For Beginners</li> <li>BitBucket Tutorials Playlist</li> <li>What is Version Control?</li> <li>Types of Version Control System</li> <li>Git Tutorial for Beginners</li> <li>Git for Professionals Tutorial</li> <li>Git and GitHub for Beginners - Crash Course</li> <li>Complete Git and GitHub Tutorial</li> <li>Git cheatsheet</li> </ul> <p>See you on Day 41</p>"},{"location":"90DaysOfDevOps/day41/","title":"#90DaysOfDevOps - The Open Source Workflow - Day 41","text":""},{"location":"90DaysOfDevOps/day41/#the-open-source-workflow","title":"The Open Source Workflow","text":"<p>Hopefully, through the last 7 sections of Git, we have a better understanding of what git is and then how a git-based service such as GitHub integrates with git to provide a source code repository but also a way in which the wider community can collaborate on code and projects together.</p> <p>When we went through the GitHub fundamentals we went through the process of forking a random project and making a change to our local repository. Here we want to go one step further and contribute to an open-source project. Remember that contributing doesn't need to be bug fixes or coding features but it could also be documentation. Every little helps and it also allows you to get hands-on with some of the git functionality we have covered.</p>"},{"location":"90DaysOfDevOps/day41/#fork-a-project","title":"Fork a Project","text":"<p>The first thing we have to do is find a project we can contribute to. I have recently been presenting on the Kanister Project and I would like to share my presentations that are now on YouTube to the main readme.mdfile in the project.</p> <p>First of all, we need to fork the project. Let's run through that process. I am going to navigate to the link shared above and fork the repository.</p> <p></p> <p>We now have our copy of the whole repository.</p> <p></p> <p>For reference on the Readme.mdfile the original Presentations listed are just these two so we need to fix this with our process.</p> <p></p>"},{"location":"90DaysOfDevOps/day41/#clones-to-a-local-machine","title":"Clones to a local machine","text":"<p>Now we have our fork we can bring that down to our local and we can then start making our edits to the files. Using the code button on our repo we can grab the URL and then use <code>git clone url</code> in a directory we wish to place the repository.</p> <p></p>"},{"location":"90DaysOfDevOps/day41/#make-our-changes","title":"Make our changes","text":"<p>We have our project local so we can open VSCode or an IDE or text editor of your choice to add your modifications.</p> <p></p> <p>The readme.mdfile is written in markdown language and because I am modifying someone else's project I am going to follow the existing project formatting to add our content.</p> <p></p>"},{"location":"90DaysOfDevOps/day41/#test-your-changes","title":"Test your changes","text":"<p>We must as a best practice test our changes, this makes total sense if this was a code change to an application you would want to ensure that the application still functions after a code change, well we also must make sure that documentation is formatted and looks correct.</p> <p>In vscode we can add a lot of plugins one of these is the ability to preview markdown pages.</p> <p></p>"},{"location":"90DaysOfDevOps/day41/#push-changes-back-to-our-forked-repository","title":"Push changes back to our forked repository","text":"<p>We do not have the authentication to push our changes directly back to the Kanister repository so we have to take this route. Now that I am happy with our changes we can run through some of those now well-known git commands.</p> <p></p> <p>Now we go back to GitHub to check the changes once more and then contribute back to the master project.</p> <p>Looks good.</p> <p></p> <p>Now we can go back to the top of our forked repository for Kanister and we can see that we are 1 commit ahead of the kanisterio:master branch.</p> <p></p> <p>Next, we hit that contribute button highlighted above. We see the option to \"Open Pull Request\"</p> <p></p>"},{"location":"90DaysOfDevOps/day41/#open-a-pull-request","title":"Open a pull request","text":"<p>There is quite a bit going on in this next image, top left you can now see we are in the original or the master repository. then you can see what we are comparing and that is the original master and our forked repository. We then have a create pull request button which we will come back to shortly. We have our single commit but if this was more changes you might have multiple commits here. then we have the changes we have made in the readme.mdfile.</p> <p></p> <p>We have reviewed the above changes and we are ready to create a pull request by hitting the green button.</p> <p>Then depending on how the maintainer of a project has set out their Pull Request functionality on their repository you may or may not have a template that gives you pointers on what the maintainer wants to see.</p> <p>This is again where you want to make a meaningful description of what you have done, clear and concise but with enough detail. You can see I have made a simple change overview and I have ticked documentation.</p> <p></p>"},{"location":"90DaysOfDevOps/day41/#create-a-pull-request","title":"Create a pull request","text":"<p>We are now ready to create our pull request. After hitting the \"Create Pull Request\" at the top of the page you will get a summary of your pull request.</p> <p></p> <p>Scrolling down you are likely to see some automation taking place, in this instance, we require a review and some checks are taking place. We can see that Travis CI is in progress and a build has started and this will check our update, making sure that before anything is merged we are not breaking things with our additions.</p> <p></p> <p>Another thing to note here is that the red in the screenshot above, can look a little daunting and look as if you have made mistakes! Don't worry you have not broken anything, my biggest tip here is this process is there to help you and the maintainers of the project. If you have made a mistake at least from my experience the maintainer will contact and advise on what to do next.</p> <p>This pull request is now public for everyone to see added Kanister presentation/resource #1237</p> <p>I am going to publish this before the merge and pull requests are accepted so maybe we can get a little prize for anyone that is still following along and can add a picture of the successful PR?</p> <ol> <li>Fork this repository to your own GitHub account</li> <li>Add your picture and possibly text</li> <li>Push the changes to your forked repository</li> <li>Create a PR that I will see and approve.</li> <li>I will think of some sort of prize</li> </ol> <p>This then wraps up our look into Git and GitHub, next we are diving into containers which starts with a big picture look into how, and why containers and also a look into virtualisation and how we got here.</p>"},{"location":"90DaysOfDevOps/day41/#resources","title":"Resources","text":"<ul> <li>Learn GitLab in 3 Hours | GitLab Complete Tutorial For Beginners</li> <li>BitBucket Tutorials Playlist</li> <li>What is Version Control?</li> <li>Types of Version Control System</li> <li>Git Tutorial for Beginners</li> <li>Git for Professionals Tutorial</li> <li>Git and GitHub for Beginners - Crash Course</li> <li>Complete Git and GitHub Tutorial</li> <li>Git cheatsheet</li> </ul> <p>See you on Day 42</p>"},{"location":"90DaysOfDevOps/day42/","title":"#90DaysOfDevOps - The Big Picture: Containers - Day 42","text":""},{"location":"90DaysOfDevOps/day42/#the-big-picture-containers","title":"The Big Picture: Containers","text":"<p>We are now starting the next section and this section is going to be focused on containers in particular we are going to be looking into Docker getting into some of the key areas to understand more about Containers.</p> <p>I will also be trying to get some hands-on here to create the container that we can use during this section but also in future sections later on in the challenge.</p> <p>As always this first post is going to be focused on the big picture of how we got here and what it all means.</p>"},{"location":"90DaysOfDevOps/day42/#history-of-platforms-and-application-development","title":"History of platforms and application development","text":""},{"location":"90DaysOfDevOps/day42/#do-we-want-to-talk-about-virtualisation-containerisation","title":"do we want to talk about Virtualisation &amp; Containerisation","text":""},{"location":"90DaysOfDevOps/day42/#why-another-way-to-run-applications","title":"Why another way to run applications?","text":"<p>The first thing we have to take a look at is why do we need another way to run our software or applications? Well it is just that choice is great, we can run our applications in many different forms, we might see applications deployed on physical hardware with an operating system and a single application deployed there, and we might see the virtual machine or cloud-based IaaS instances running our application which then integrate into a database again in a VM or as PaaS offering in the public cloud. Or we might see our applications running in containers.</p> <p>None of the above options is wrong or right, but they each have their reasons to exist and I also strongly believe that none of these is going away. I have seen a lot of content that walks into Containers vs Virtual Machines and there really should not be an argument as that is more like apples vs pears argument where they are both fruit (ways to run our applications) but they are not the same.</p> <p>I would also say that if you were starting and you were developing an application you should lean towards containers simply because we will get into some of these areas later, but it's about efficiency, speed and size. But that also comes with a price, if you have no idea about containers then it's going to be a learning curve to force yourself to understand the why and get into that mindset. If you have developed your applications a particular way or you are not in a greenfield environment then you might have more pain points to deal with before even considering containers.</p> <p>We have many different choices then when it comes to downloading a given piece of software, there are a variety of different operating systems that we might be using. And specific instructions for what we need to do to install our applications.</p> <p></p> <p>More and more recently I am finding that the applications we might have once needed a full server OS, A VM, Physical or cloud instance are now releasing container-based versions of their software. I find this interesting as this opens the world of containers and then Kubernetes to everyone and not just a focus on application developers.</p> <p></p> <p>As you can probably tell as I have said before, I am not going to advocate that the answer is containers, what's the question! But I would like to discuss how this is another option for us to be aware of when we deploy our applications.</p> <p></p> <p>We have had container technology for a long time, so why now over the last say 10 years has this become popular, I would say even more popular in the last 5. We have had containers for decades. It comes down to the challenge of containers or should I say images as well, to how we distribute our software, because if we just have container technology, then we still will have many of the same problems we've had with software management.</p> <p>If we think about Docker as a tool, the reason that it took off, is because of the ecosystem of images that are easy to find and use. Simple to get on your systems and get up and running. A major part of this is consistency across the entire space, of all these different challenges that we face with software. It doesn't matter if it's MongoDB or nodeJS, the process to get either of those up and running will be the same. The process to stop either of those is the same. All of these issues will still exist, but the nice thing is, when we bring good container and image technology together, we now have a single set of tools to help us tackle all of these different problems. Some of those issues are listed below:</p> <ul> <li>We first have to find software on the internet.</li> <li>We then have to download this software.</li> <li>Do we trust the source?</li> <li>Do we then need a license? Which License?</li> <li>Is it compatible with different platforms?</li> <li>What is the package? binary? Executable? Package manager?</li> <li>How do we configure the software?</li> <li>Dependencies? Did the overall download have us covered or do we need them as well?</li> <li>Dependencies of Dependencies?</li> <li>How do we start the application?</li> <li>How do we stop the application?</li> <li>Will it auto-restart?</li> <li>Start on boot?</li> <li>Resource conflicts?</li> <li>Conflicting libraries?</li> <li>Port Conflicts</li> <li>Security for the software?</li> <li>Software updates?</li> <li>How can I remove the software?</li> </ul> <p>We can split the above into 3 areas of the complexity of the software that containers and images do help with these.</p> Distribution Installation Operation Find Install Start Download Configuration Security License Uninstall Ports Package Dependencies Resource Conflicts Trust Platform Auto-Restart Find Libraries Updates <p>Containers and images are going to help us remove some of these challenges that we have with possibly other software and applications.</p> <p>At a high level we could move installation and operation into the same list, Images are going to help us from a distribution point of view and containers help with the installation and operations.</p> <p>Ok, probably sounds great and exciting but we still need to understand what is a container and now I have mentioned images so let's cover those areas next.</p> <p>Another thing you might have seen a lot when we talk about Containers for software development is the analogy used alongside shipping containers, shipping containers are used to ship various goods across the seas using large vessels.</p> <p></p> <p>What does this have to do with our topic of containers? Think about the code that software developers write, how can we ship that particular code from one machine to another machine?</p> <p>If we think about what we touched on before about software distribution, installation and operations but now we start to build this out into an environment visual. We have hardware and an operating system where you will run multiple applications. For example, nodejs has certain dependencies and needs certain libraries. If you then want to install MySQL then it needs its required libraries and dependencies. Each software application will have its library and dependency. We might be massively lucky and not have any conflicts between any of our applications where specific libraries and dependencies are clashing causing issues but the more applications the more chance or risk of conflicts. However, this is not about that one deployment when everything fixes your software applications are going to be updated and then we can also introduce these conflicts.</p> <p></p> <p>Containers can help solve this problem. Containers help build your application, ship the application, deploy and scale these applications with ease independently. let's look at the architecture, you will have hardware and operating system then on top of it you will have a container engine like docker which we will cover later. The container engine software helps create containers that package the libraries and dependencies along with it so that you can move this container seamlessly from one machine to another machine without worrying about the libraries and dependencies since they come as a part of a package which is nothing but the container so you can have different containers this container can be moved across the systems without worrying about the underlying dependencies that the application needs to run because everything the application needs to run is packaged as a container that you can move.</p> <p></p>"},{"location":"90DaysOfDevOps/day42/#the-advantages-of-these-containers","title":"The advantages of these containers","text":"<ul> <li> <p>Containers help package all the dependencies within the container and   isolate it.</p> </li> <li> <p>It is easy to manage the containers</p> </li> <li> <p>The ability to move from one system to another.</p> </li> <li> <p>Containers help package the software and you can easily ship it without any duplicate efforts</p> </li> <li> <p>Containers are easily scalable.</p> </li> </ul> <p>Using containers you can scale independent containers and use a load balancer or a service which helps split the traffic and you can scale the applications horizontally. Containers offer a lot of flexibility and ease in how you manage your applications</p>"},{"location":"90DaysOfDevOps/day42/#what-is-a-container","title":"What is a container?","text":"<p>When we run applications on our computer, this could be the web browser or VScode that you are using to read this post. That application is running as a process or what is known as a process. On our laptops or systems, we tend to run multiple applications or as we said processes. When we open a new application or click on the application icon this is an application we would like to run, sometimes this application might be a service that we just want to run in the background, our operating system is full of services that are running in the background providing you with the user experience you get with your system.</p> <p>That application icon represents a link to an executable somewhere on your file system, the operating system then loads that executable into memory. Interestingly, that executable is sometimes referred to as an image when we're talking about a process.</p> <p>Containers are processes, A container is a standard unit of software that packages up code and all its dependencies so the application runs quickly and reliably from one computing environment to another.</p> <p>Containerised software will always run the same, regardless of the infrastructure. Containers isolate software from its environment and ensure that it works uniformly despite differences for instance between development and staging.</p> <p>I mentioned images in the last section when it comes to how and why containers and images combined made containers popular in our ecosystem.</p>"},{"location":"90DaysOfDevOps/day42/#what-is-an-image","title":"What is an Image?","text":"<p>A container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings. Container images become containers at runtime.</p>"},{"location":"90DaysOfDevOps/day42/#resources","title":"Resources","text":"<ul> <li>TechWorld with Nana - Docker Tutorial for Beginners</li> <li>Programming with Mosh - Docker Tutorial for Beginners</li> <li>Docker Tutorial for Beginners - What is Docker? Introduction to Containers</li> <li>Introduction to Container By Red Hat</li> </ul> <p>See you on Day 43</p>"},{"location":"90DaysOfDevOps/day43/","title":"#90DaysOfDevOps - What is Docker & Getting installed - Day 43","text":""},{"location":"90DaysOfDevOps/day43/#what-is-docker-getting-installed","title":"What is Docker &amp; Getting installed","text":"<p>In the previous post, I mentioned Docker at least once and that is because Docker is innovative in making containers popular even though they have been around for such a long time.</p> <p>We are going to be using and explaining docker here but we should also mention the Open Container Initiative (OCI) which is an industry standards organization that encourages innovation while avoiding the danger of vendor lock-in. Thanks to the OCI, we have a choice when choosing a container toolchain, including Docker, CRI-O, Podman, LXC, and others.</p> <p>Docker is a software framework for building, running, and managing containers. The term \"docker\" may refer to either the tools (the commands and a daemon) or the Dockerfile file format.</p> <p>We are going to be using Docker Personal here which is free (for education and learning). This includes all the essentials that we need to cover to get a good foundation of knowledge of containers and tooling.</p> <p>It is probably worth breaking down some of the \"docker\" tools that we will be using and what they are used for. The term docker can be referring to the docker project overall, which is a platform for devs and admins to develop, ship and run applications. It might also be a reference to the docker daemon process running on the host which manages images and containers also called Docker Engine.</p>"},{"location":"90DaysOfDevOps/day43/#docker-engine","title":"Docker Engine","text":"<p>Docker Engine is an open-source containerization technology for building and containerizing your applications. Docker Engine acts as a client-server application with:</p> <ul> <li>A server with a long-running daemon process dockerd.</li> <li>APIs specify interfaces that programs can use to talk to and instruct the Docker daemon.</li> <li>A command line interface (CLI) client docker.</li> </ul> <p>The above was taken from the official Docker documentation and the specific Docker Engine Overview</p>"},{"location":"90DaysOfDevOps/day43/#docker-desktop","title":"Docker Desktop","text":"<p>We have a docker desktop for both Windows and macOS systems. An easy-to-install, lightweight docker development environment. A native OS application that leverages virtualisation capabilities on the host operating system.</p> <p>It\u2019s the best solution if you want to build, debug, test, package, and ship Dockerized applications on Windows or macOS.</p> <p>On Windows, we can also take advantage of WSL2 and Microsoft Hyper-V. We will cover some of the WSL2 benefits as we go through.</p> <p>Because of the integration with hypervisor capabilities on the host operating system docker provides the ability to run your containers with Linux Operating systems.</p>"},{"location":"90DaysOfDevOps/day43/#docker-compose","title":"Docker Compose","text":"<p>Docker compose is a tool that allows you to run more complex apps over multiple containers. With the benefit of being able to use a single file and command to spin up your application.</p>"},{"location":"90DaysOfDevOps/day43/#docker-hub","title":"Docker Hub","text":"<p>A centralised resource for working with Docker and its components. Most commonly known as a registry to host docker images. But there are a lot of additional services here which can be used in part with automation or integrated into GitHub as well as security scanning.</p>"},{"location":"90DaysOfDevOps/day43/#dockerfile","title":"Dockerfile","text":"<p>A dockerfile is a text file that contains commands you would normally execute manually to build a docker image. Docker can build images automatically by reading the instructions we have in our dockerfile.</p>"},{"location":"90DaysOfDevOps/day43/#installing-docker-desktop","title":"Installing Docker Desktop","text":"<p>The docker documenation is amazing and if you are only just diving in then you should take a look and have a read-through. We will be using Docker Desktop on Windows with WSL2. I had already run through the installation on the machine we are using here.</p> <p></p> <p>Take note before you go ahead and install at the system requirements, Install Docker Desktop on Windows if you are using macOS including the M1-based CPU architecture you can also take a look at Install Docker Desktop on macOS</p> <p>I will run through the Docker Desktop installation for Windows on another Windows Machine and log the process down below.</p>"},{"location":"90DaysOfDevOps/day43/#windows","title":"Windows","text":"<ul> <li>Select windows as the operating system of your device.</li> </ul> <ul> <li> <p>Navigate to the folder where you want to save the installer and save.</p> </li> <li> <p>Run the installer and wait for a few seconds and grant access for WSL.     </p> </li> <li> <p>Click ok and the installation will begin.     </p> </li> <li> <p>Docker Desktop has been successfully installed on your device. You can now run the command \"docker\" on the terminal to check if the installation was successfull.     </p> </li> </ul>"},{"location":"90DaysOfDevOps/day43/#resources","title":"Resources","text":"<ul> <li>TechWorld with Nana - Docker Tutorial for Beginners</li> <li>Programming with Mosh - Docker Tutorial for Beginners</li> <li>Docker Tutorial for Beginners - What is Docker? Introduction to Containers</li> <li>WSL 2 with Docker getting started</li> </ul> <p>See you on Day 44</p>"},{"location":"90DaysOfDevOps/day44/","title":"#90DaysOfDevOps - Docker Images & Hands-On with Docker Desktop - Day 44","text":""},{"location":"90DaysOfDevOps/day44/#docker-images-hands-on-with-docker-desktop","title":"Docker Images &amp; Hands-On with Docker Desktop","text":"<p>We now have Docker Desktop installed on our system. (If you are running Linux then you still have options but no GUI but docker does work on Linux.)Install Docker Engine on Ubuntu (Other distributions also available.)</p> <p>In this post, we are going to get started with deploying some images into our environment. A recap on what a Docker Image is - A Docker image is a file used to execute code in a Docker container. Docker images act as a set of instructions to build a Docker container, like a template. Docker images also act as the starting point when using Docker.</p> <p>Now is a good time to go and create your account on DockerHub</p> <p></p> <p>DockerHub is a centralised resource for working with Docker and its components. Most commonly known as a registry to host docker images. But there are a lot of additional services here which can be used in part with automation or integrated into GitHub as well as security scanning.</p> <p>If you scroll down once logged in you are going to see a list of container images, You might see database images for MySQL, hello-world etc. Think of these as great baseline images or you might just need a database image and you are best to use the official one which means you don't need to create your own.</p> <p></p> <p>We can drill deeper into the view of available images and search across categories, operating systems and architectures. The one thing I highlight below is the Official Image, this should give you peace of mind about the origin of this container image.</p> <p></p> <p>We can also search for a specific image, for example, WordPress might be a good base image that we want we can do that at the top and find all container images related to WordPress. Below are notices that we also have verified publisher.</p> <ul> <li> <p>Official Image - Docker Official images are a curated set of Docker open source and \"drop-in\" solution repositories.</p> </li> <li> <p>Verified Publisher - High-quality Docker content from verified publishers. These products are published and maintained directly by a commercial entity.</p> </li> </ul> <p></p>"},{"location":"90DaysOfDevOps/day44/#exploring-docker-desktop","title":"Exploring Docker Desktop","text":"<p>We have Docker Desktop installed on our system and if you open this I expect unless you had this already installed you will see something similar to the image below. As you can see we have no containers running and our docker engine is running.</p> <p></p> <p>Because this was not a fresh install for me, I do have some images already downloaded and available on my system. You will likely see nothing in here.</p> <p></p> <p>Under remote repositories, this is where you will find any container images you have stored in your docker hub. You can see from the below I do not have any images.</p> <p></p> <p>We can also clarify this on our dockerhub site and confirm that we have no repositories there.</p> <p></p> <p>Next, we have the Volumes tab, If you have containers that require persistence then this is where we can add these volumes to your local file system or a shared file system.</p> <p></p> <p>At the time of writing, there is also a Dev Environments tab, this is going to help you collaborate with your team instead of moving between different git branches. We won't be covering this.</p> <p></p> <p>Going back to the first tab you can see that there is a command we can run which is a getting started container. Let's run <code>docker run -d -p 80:80 docker/getting-started</code> in our terminal.</p> <p></p> <p>If we go and check our docker desktop window again, we are going to see that we have a running container.</p> <p></p> <p>You might have noticed that I am using WSL2 and for you to be able to use that you will need to make sure this is enabled in the settings.</p> <p></p> <p>If we now go and check our Images tab again, you should now see an in-use image called docker/getting-started.</p> <p></p> <p>Back to the Containers/Apps tab, click on your running container. You are going to see the logs by default and along the top, you have some options to choose from, in our case I am pretty confident that this is going to be a web page running in this container so we are going to choose the open in the browser.</p> <p></p> <p>When we hit that button above sure enough a web page should open hitting your localhost and display something similar to below.</p> <p>This container also has some more detail on our containers and images.</p> <p></p> <p>We have now run our first container. Nothing too scary just yet. What about if we wanted to pull one of the container images down from DockerHub? Maybe there is a <code>hello world</code> docker container we could use.</p> <p>I went ahead and stopped the getting started container not that it's taking up any mass amount of resources but for tidiness, as we walk through some more steps.</p> <p>Back in our terminal let's go ahead and run <code>docker run hello-world</code> and see what happens.</p> <p>You can see we did not have the image locally so we pulled that down and then we got a message that is written into the container image with some information on what it did to get up and running and some links to reference points.</p> <p></p> <p>However, if we go and look in Docker Desktop now we have no running containers but we do have an exited container that used the hello-world message, meaning it came up, delivered the message and then is terminated.</p> <p></p> <p>And for the last time, let's just go and check the images tab and see that we have a new hello-world image locally on our system, meaning that if we run the <code>docker run hello-world</code> command again in our terminal we would not have to pull anything unless a version changes.</p> <p></p> <p>The message from the hello-world container set down the challenge of running something a little more ambitious.</p> <p>Challenge Accepted!</p> <p></p> <p>In running <code>docker run -it ubuntu bash</code> in our terminal we are going to run a containerised version of Ubuntu well not a full copy of the Operating system. You can find out more about this particular image on DockerHub</p> <p>You can see below when we run the command we now have an interactive prompt (<code>-it</code>) and we have a bash shell into our container.</p> <p></p> <p>We have a bash shell but we don't have much more which is why this container image is less than 30MB.</p> <p></p> <p>But we can still use this image and we can still install software using our apt package manager, we can update our container image and upgrade also.</p> <p></p> <p>Or maybe we want to install some software into our container, I have chosen a really bad example here as pinta is an image editor and it's over 200MB but hopefully you get where I am going with this. This would increase the size of our container considerably but still, we are going to be in the MB and not in the GB.</p> <p></p> <p>I wanted that to hopefully give you an overview of Docker Desktop and the not-so-scary world of containers when you break it down with simple use cases, we do need to cover some networking, security and other options we have vs just downloading container images and using them like this. By the end of the section, we want to have made something and uploaded it to our DockerHub repository and be able to deploy it.</p>"},{"location":"90DaysOfDevOps/day44/#resources","title":"Resources","text":"<ul> <li>TechWorld with Nana - Docker Tutorial for Beginners</li> <li>Programming with Mosh - Docker Tutorial for Beginners</li> <li>Docker Tutorial for Beginners - What is Docker? Introduction to Containers</li> <li>WSL 2 with Docker getting started</li> </ul> <p>See you on Day 45</p>"},{"location":"90DaysOfDevOps/day45/","title":"#90DaysOfDevOps - The anatomy of a Docker Image - Day 45","text":""},{"location":"90DaysOfDevOps/day45/#the-anatomy-of-a-docker-image","title":"The anatomy of a Docker Image","text":"<p>In the last session, we covered some basics of how we can use Docker Desktop combined with DockerHub to deploy and run some verified images. A recap on what an image is, you won't forget things if I keep mentioning them. </p> <p>A Docker image is a read-only template that contains a set of instructions for creating a container that can run on the Docker platform. It provides a convenient way to package up applications and preconfigured server environments, which you can use for your private use or share publicly with other Docker users. Docker images are also the starting point for anyone using Docker for the first time.</p> <p>What happens if we want to create our own Docker image? For us to do this we would create a Dockerfile. You saw how we could take that Ubuntu container image and we could add our software and we would have our container image with the software that we wanted and everything is good, however, if that container is shut down or thrown away then all those software updates and installations go away there is no repeatable version of what we had done. So that is great for showing off the capabilities but it doesn't help with the transport of images across multiple environments with the same set of software installed each time the container is run. </p>"},{"location":"90DaysOfDevOps/day45/#what-is-a-dockerfile","title":"What is a Dockerfile","text":"<p>A dockerfile is a text file that contains commands you would normally execute manually to build a docker image. Docker can build images automatically by reading the instructions we have in our dockerfile.</p> <p>Each of the files that make up a docker image is known as a layer. these layers form a series of images, built on top of each other in stages. Each layer is dependent on the layer immediately below it. The order of your layers is key to the efficiency of the lifecycle management of your docker images. </p> <p>We should organise our layers that change most often as high in the stack as possible, this is because when you make changes to a layer in your image, Docker not only rebuilds that particular layer but all layers built from it. Therefore a change to a layer at the top involves the least amount of work to rebuild the entire image. </p> <p>Each time docker launches a container from an image (like we ran yesterday) it adds a writeable layer, known as the container layer. This stores all changes to the container throughout its runtime. This layer is the only difference between a live operational container and the source image itself. Any number of like-for-like containers can share access to the same underlying image while maintaining their state. </p> <p>Back to the example, we used yesterday with the Ubuntu image. We could run that same command multiple times and on the first container we could go and install pinta and on the second we could install figlet with two different applications, different purposes, different sizes etc. Each container that we deployed shares the same image but not the same state and then that state is then gone when we remove the container. </p> <p></p> <p>Following the example above with the Ubuntu image, but also many other ready-built container images available on DockerHub and other third-party repositories. These images are generally known as the parent image. It is the foundation upon which all other layers are built and provides the basic building blocks for our container environments. </p> <p>Together with a set of individual layer files, a Docker image also includes an additional file known as a manifest. This is essentially a description of the image in JSON format and comprises information such as image tags, a digital signature, and details on how to configure the container for different types of host platforms.</p> <p></p>"},{"location":"90DaysOfDevOps/day45/#how-to-create-a-docker-image","title":"How to create a docker image","text":"<p>There are two ways we can create a docker image. We can do it a little on the fly with the process that we started yesterday, we pick our base image spin up that container, and install all of the software and dependencies that we wish to have on our container. </p> <p>Then we can use the <code>docker commit container name</code> then we have a local copy of this image under docker images and in our docker desktop images tab. </p> <p>Super simple, I would not recommend this method unless you want to understand the process, it is going to be very difficult to manage lifecycle management this way and a lot of manual configuration/reconfiguration. But it is the quickest and most simple way to build a docker image. Great for testing, troubleshooting, validating dependencies etc. </p> <p>The way we intend to build our image is through a dockerfile. Which gives us a clean, compact and repeatable way to create our images. Much easier lifecycle management and easy integration into Continous Integration and Continuous delivery processes. But as you might gather it is a little more difficult than the first mentioned process. </p> <p>Using the dockerfile method is much more in tune with real-world, enterprise-grade container deployments. </p> <p>A dockerfile is a three-step process whereby you create the dockerfile and add the commands you need to assemble the image. </p> <p>The following table shows some of the dockerfile statements we will be using or that you will most likely be using. </p> Command Purpose FROM To specify the parent image. WORKDIR To set the working directory for any commands that follow in the Dockerfile. RUN To install any applications and packages required for your container. COPY To copy over files or directories from a specific location. ADD As COPY, but also able to handle remote URLs and unpack compressed files. ENTRYPOINT Command that will always be executed when the container starts. If not specified, the default is /bin/sh -c CMD Arguments passed to the entrypoint. If ENTRYPOINT is not set (defaults to /bin/sh -c), the CMD will be the commands the container executes. EXPOSE To define which port through which to access your container application. LABEL To add metadata to the image. <p>Now we have the detail on how to build our first dockerfile we can create a working directory and create our dockerfile. I have created a working directory within this repository where you can see the files and folders I have to walk through. Containers</p> <p>In this directory, I am going to create a .dockerignore file similar to the .gitignore we used in the last section. This file will list any files that would otherwise be created during the Docker build process, which you want to exclude from the final build.</p> <p>Remember everything about containers is about being compact, as fast as possible with no bloat. </p> <p>I want to create a very simple Dockerfile with the below layout also can be found in the folder linked above. </p> <pre><code># Use the official Ubuntu 18.04 as base\nFROM ubuntu:18.04\n# Install nginx and curl\nRUN apt-get update &amp;&amp; apt-get upgrade -y\nRUN apt-get install -y nginx curl\nRUN rm -rf /var/lib/apt/lists/*\n</code></pre> <p>Navigate to this directory in your terminal, and then run <code>docker build -t 90daysofdevops:0.1 .</code> we are using the <code>-t</code> and then setting an image name and tag. </p> <p></p> <p>Now we have created our image we can then go and run our image using Docker Desktop or we could use the docker command line. I have used Docker Desktop I have fired up a container and you can see that we have <code>curl</code> available to us in the cli of the container. </p> <p></p> <p>Whilst in Docker Desktop there is also the ability to leverage the UI to do some more tasks with this new image. </p> <p></p> <p>We can inspect our image, in doing so you see very much of the dockerfile and the lines of code that we wanted to run within our container. </p> <p></p> <p>We have a pull option, now this would fail for us because this image is not hosted anywhere so we would get that as an error. However, we do have a Push to hub which would enable us to push our image to DockerHub. </p> <p>If you are using the same <code>docker build</code> we ran earlier then this would not work either, you would need the build command to be <code>docker build -t {{username}}/{{imagename}}:{{version}}</code></p> <p></p> <p>Then if we go and take a look in our DockerHub repository you can see that we just pushed a new image. Now in Docker Desktop, we would be able to use that pull tab. </p> <p></p>"},{"location":"90DaysOfDevOps/day45/#resources","title":"Resources","text":"<ul> <li>TechWorld with Nana - Docker Tutorial for Beginners</li> <li>Programming with Mosh - Docker Tutorial for Beginners</li> <li>Docker Tutorial for Beginners - What is Docker? Introduction to Containers</li> <li>WSL 2 with Docker getting started</li> <li>Blog on gettng started building a docker image</li> <li>Docker documentation for building an image</li> </ul> <p>See you on Day 46</p>"},{"location":"90DaysOfDevOps/day46/","title":"#90DaysOfDevOps - Docker Compose - Day 46","text":""},{"location":"90DaysOfDevOps/day46/#docker-compose","title":"Docker Compose","text":"<p>The ability to run one container could be great if you have a self-contained image that has everything you need for your single use case, where things get interesting is when you are looking to build multiple applications between different container images. For example, if I had a website front end but required a backend database I could put everything in one container but better and more efficient would be to have its container for the database.</p> <p>This is where Docker compose comes in which is a tool that allows you to run more complex apps over multiple containers. With the benefit of being able to use a single file and command to spin up your application. The example I am going to the walkthrough in this post is from the Docker QuickStart sample apps (Quickstart: Compose and WordPress).</p> <p>In this first example we are going to:</p> <ul> <li>Use Docker compose to bring up WordPress and a separate MySQL instance.</li> <li>Use a YAML file which will be called <code>docker-compose.yml</code></li> <li>Build the project</li> <li>Configure WordPress via a Browser</li> <li>Shutdown and Clean up</li> </ul>"},{"location":"90DaysOfDevOps/day46/#install-docker-compose","title":"Install Docker Compose","text":"<p>As mentioned Docker Compose is a tool, If you are on macOS or Windows then compose is included in your Docker Desktop installation. However, you might be wanting to run your containers on a Windows server host or Linux server and in which case you can install using these instructions Install Docker Compose</p> <p>To confirm we have <code>docker-compose</code> installed on our system we can open a terminal and simply type the above command.</p> <p></p>"},{"location":"90DaysOfDevOps/day46/#docker-composeyml-yaml","title":"Docker-Compose.yml (YAML)","text":"<p>The next thing to talk about is the docker-compose.yml which you can find in the container folder of the repository. But more importantly, we need to discuss YAML, in general, a little.</p> <p>YAML could almost have its session as you are going to find it in so many different places. But for the most part</p> <p>\"YAML is a human-friendly data serialization language for all programming languages.\"</p> <p>It is commonly used for configuration files and in some applications where data is being stored or transmitted. You have no doubt come across XML files that tend to offer that same configuration file. YAML provides a minimal syntax but is aimed at those same use cases.</p> <p>YAML Ain't Markup Language (YAML) is a serialisation language that has steadily increased in popularity over the last few years. The object serialisation abilities make it a viable replacement for languages like JSON.</p> <p>The YAML acronym was shorthand for Yet Another Markup Language. But the maintainers renamed it to YAML Ain't Markup Language to place more emphasis on its data-oriented features.</p> <p>Anyway, back to the docker-compose.yml file. This is a configuration file of what we want to do when it comes to multiple containers being deployed on our single system.</p> <p>Straight from the tutorial linked above you can see the contents of the file looks like this:</p> <pre><code>version: \"3.9\"\n\nservices:\n  DB:\n    image: mysql:5.7\n    volumes:\n      - db_data:/var/lib/mysql\n    restart: always\n    environment:\n      MYSQL_ROOT_PASSWORD: somewordpress\n      MYSQL_DATABASE: wordpress\n      MYSQL_USER: wordpress\n      MYSQL_PASSWORD: wordpress\n\n  wordpress:\n    depends_on:\n      - db\n    image: wordpress:latest\n    volumes:\n      - wordpress_data:/var/www/html\n    ports:\n      - \"8000:80\"\n    restart: always\n    environment:\n      WORDPRESS_DB_HOST: db\n      WORDPRESS_DB_USER: wordpress\n      WORDPRESS_DB_PASSWORD: wordpress\n      WORDPRESS_DB_NAME: wordpress\nvolumes:\n  db_data: {}\n  wordpress_data: {}\n</code></pre> <p>We declare a version and then a large part of this docker-compose.yml file is made up of our services, we have a DB service and a WordPress service. You can see each of those has an image defined with a version tag associated. We are now also introducing state into our configuration unlike our first walkthroughs, but now we are going to create volumes so we can store our databases there.</p> <p>We then have some environmental variables such as passwords and usernames. These files can get very complicated but the YAML configuration file simplifies what these look like overall.</p>"},{"location":"90DaysOfDevOps/day46/#build-the-project","title":"Build the project","text":"<p>Next up we can head back into our terminal and we can use some commands with our docker-compose tool. Navigate to your directory, where your docker-compose.yml file is located.</p> <p>From the terminal, we can simply run <code>docker-compose up -d</code> this will start the process of pulling those images and standing up your multi-container application.</p> <p>The <code>-d</code> in this command means detached mode, which means that the Run command is or will be in the background.</p> <p></p> <p>If we now run the <code>docker ps</code> command, you can see we have 2 containers running, one being WordPress and the other being MySQL.</p> <p></p> <p>Next, we can validate that we have WordPress up and running by opening a browser and going to <code>http://localhost:8000</code> and you should see the WordPress set-up page.</p> <p></p> <p>We can run through the setup of WordPress, and then we can start building our website as we see fit in the console below.</p> <p></p> <p>If we then open a new tab and navigate to that same address we did before <code>http://localhost:8000</code> we will now see a simple default theme with our site title \"90DaysOfDevOps\" and then a sample post.</p> <p></p> <p>Before we make any changes, open Docker Desktop and navigate to the volumes tab and here you will see two volumes associated with our containers, one for WordPress and one for DB.</p> <p></p> <p>My Current wordpress theme is \"Twenty Twenty-Two\" and I want to change this to \"Twenty Twenty\" Back in the dashboard we can make those changes.</p> <p></p> <p>I am also going to add a new post to my site, and here below you see the latest version of our new site.</p> <p></p>"},{"location":"90DaysOfDevOps/day46/#clean-up-or-not","title":"Clean Up or not","text":"<p>If we were now to use the command <code>docker-compose down</code> this would bring down our containers. But will leave our volumes in place.</p> <p></p> <p>We can just confirm in Docker Desktop that our volumes are still there though.</p> <p></p> <p>If we then want to bring things back up then we can issue the <code>docker up -d</code> command from within the same directory and we have our application back up and running.</p> <p></p> <p>We then navigate in our browser to that same address of <code>http://localhost:8000</code> and notice that our new post and our theme change are all still in place.</p> <p></p> <p>If we want to get rid of the containers and those volumes then issuing the <code>docker-compose down --volumes</code> will also destroy the volumes.</p> <p></p> <p>Now when we use <code>docker-compose up -d</code> again we will be starting, however, the images will still be local on our system so you won't need to re-pull them from the DockerHub repository.</p> <p>I know that when I started diving into docker-compose and its capabilities I was then confused as to where this sits alongside or with Container Orchestration tools such as Kubernetes, well everything we have done here in this short demo is focused on one host we have WordPress and DB running on the local desktop machine. We don't have multiple virtual machines or multiple physical machines, we also can't easily scale up and down the requirements of our application.</p> <p>Our next section is going to cover Kubernetes but we have a few more days of Containers in general first.</p> <p>This is also a great resource for samples of docker-compose applications with multiple integrations. Awesome-Compose</p> <p>In the above repository, there is a great example which will deploy an Elasticsearch, Logstash, and Kibana (ELK) in single-node.</p> <p>I have uploaded the files to the Containers folder When you have this folder locally, navigate there and you can simply use <code>docker-compose up -d</code></p> <p></p> <p>We can then check we have those running containers with <code>docker ps</code></p> <p></p> <p>Now we can open a browser for each of the containers:</p> <p></p> <p>To remove everything we can use the <code>docker-compose down</code> command.</p>"},{"location":"90DaysOfDevOps/day46/#resources","title":"Resources","text":"<ul> <li>TechWorld with Nana - Docker Tutorial for Beginners</li> <li>Programming with Mosh - Docker Tutorial for Beginners</li> <li>Docker Tutorial for Beginners - What is Docker? Introduction to Containers</li> <li>WSL 2 with Docker getting started</li> <li>Blog on getting started building a docker image</li> <li>Docker documentation for building an image</li> <li>YAML Tutorial: Everything You Need to Get Started in Minute</li> </ul> <p>See you on Day 47</p>"},{"location":"90DaysOfDevOps/day47/","title":"#90DaysOfDevOps - Docker Networking & Security - Day 47","text":""},{"location":"90DaysOfDevOps/day47/#docker-networking-security","title":"Docker Networking &amp; Security","text":"<p>During this container session so far we have made things happen but we have not looked at how things have worked behind the scenes either from a networking point of view also we have not touched on security, that is the plan for this session.</p>"},{"location":"90DaysOfDevOps/day47/#docker-networking-basics","title":"Docker Networking Basics","text":"<p>Open a terminal, and type the command <code>docker network</code> this is the main command for configuring and managing container networks.</p> <p>From the below, you can see this is how we can use the command, and all of the sub-commands available. We can create new networks, list existing ones, and inspect and remove networks.</p> <p></p> <p>Let's take a look at the existing networks we have since our installation, so the out-of-box Docker networking looks like using the <code>docker network list</code> command.</p> <p>Each network gets a unique ID and NAME. Each network is also associated with a single driver. Notice that the \"bridge\" network and the \"host\" network have the same name as their respective drivers.</p> <p></p> <p>Next, we can take a deeper look into our networks with the <code>docker network inspect</code> command.</p> <p>With me running <code>docker network inspect bridge</code> I can get all the configuration details of that specific network name. This includes name, ID, drivers, connected containers and as you can see quite a lot more.</p> <p></p>"},{"location":"90DaysOfDevOps/day47/#docker-bridge-networking","title":"Docker: Bridge Networking","text":"<p>As you have seen above a standard installation of Docker Desktop gives us a pre-built network called <code>bridge</code> If you look back up to the <code>docker network list</code> command, you will see that the network called bridge is associated with the <code>bridge</code> driver. Just because they have the same name doesn't they are the same thing. Connected but not the same thing.</p> <p>The output above also shows that the bridge network is scoped locally. This means that the network only exists on this Docker host. This is true of all networks using the bridge driver - the bridge driver provides single-host networking.</p> <p>All networks created with the bridge driver are based on a Linux bridge (a.k.a. a virtual switch).</p>"},{"location":"90DaysOfDevOps/day47/#connect-a-container","title":"Connect a Container","text":"<p>By default the bridge network is assigned to new containers, meaning unless you specify a network all containers will be connected to the bridge network.</p> <p>Let's create a new container with the command <code>docker run -dt ubuntu sleep infinity</code></p> <p>The sleep command above is just going to keep the container running in the background so we can mess around with it.</p> <p></p> <p>If we then check our bridge network with <code>docker network inspect bridge</code> you will see that we have a container matching what we have just deployed because we did not specify a network.</p> <p></p> <p>We can also dive into the container using <code>docker exec -it 3a99af449ca2 bash</code> you will have to use <code>docker ps</code> to get your container ID.</p> <p>From here our image doesn't have anything to ping so we need to run the following command.<code>apt-get update &amp;&amp; apt-get install -y iputils-ping</code> then ping an external interfacing address. <code>ping -c5 www.90daysofdevops.com</code></p> <p></p> <p>To clear this up we can run <code>docker stop 3a99af449ca2</code> again and use <code>docker ps</code> to find your container ID but this will remove our container.</p>"},{"location":"90DaysOfDevOps/day47/#configure-nat-for-external-connectivity","title":"Configure NAT for external connectivity","text":"<p>In this step, we'll start a new NGINX container and map port 8080 on the Docker host to port 80 inside of the container. This means that traffic that hits the Docker host on port 8080 will be passed on to port 80 inside the container.</p> <p>Start a new container based on the official NGINX image by running <code>docker run --name web1 -d -p 8080:80 nginx</code></p> <p></p> <p>Review the container status and port mappings by running <code>docker ps</code></p> <p></p> <p>The top line shows the new web1 container running NGINX. Take note of the command the container is running as well as the port mapping - <code>0.0.0.0:8080-&gt;80/tcp</code> maps port 8080 on all host interfaces to port 80 inside the web1 container. This port mapping is what effectively makes the container's web service accessible from external sources (via the Docker hosts IP address on port 8080).</p> <p>Now we need our IP address for our actual host, we can do this by going into our WSL terminal and using the <code>IP addr</code> command.</p> <p></p> <p>Then we can take this IP and open a browser and head to <code>http://172.25.218.154:8080/</code> Your IP might be different. This confirms that NGINX is accessible.</p> <p></p> <p>I have taken these instructions from this site from way back in 2017 DockerCon but they are still relevant today. However, the rest of the walkthrough goes into Docker Swarm and I am not going to be looking into that here. Docker Networking - DockerCon 2017</p>"},{"location":"90DaysOfDevOps/day47/#securing-your-containers","title":"Securing your containers","text":"<p>Containers provide a secure environment for your workloads vs a full server configuration. They offer the ability to break up your applications into much smaller, loosely coupled components each isolated from one another which helps reduce the attack surface overall.</p> <p>But they are not immune from hackers that are looking to exploit systems. We still need to understand the security pitfalls of the technology and maintain best practices.</p>"},{"location":"90DaysOfDevOps/day47/#move-away-from-root-permission","title":"Move away from root permission","text":"<p>All of the containers we have deployed have been using the root permission to the process within your containers. This means they have full administrative access to your container and host environments. Now to walk through we knew these systems were not going to be up and running for long. But you saw how easy it was to get up and running.</p> <p>We can add a few steps to our process to enable non-root users to be our preferred best practice. When creating our dockerfile we can create user accounts. You can find this example also in the containers folder in the repository.</p> <pre><code># Use the official Ubuntu 18.04 as base\nFROM ubuntu:18.04\nRUN apt-get update &amp;&amp; apt-get upgrade -y\nRUN groupadd -g 1000 basicuser &amp;&amp; useradd -r -u 1000 -g basicuser basicuser\nUSER basicuser\n</code></pre> <p>We can also use <code>docker run --user 1009 ubuntu</code> the Docker run command overrides any user specified in your Dockerfile. Therefore, in the following example, your container will always run with the least privilege\u2014provided user identifier 1009 also has the lowest permission level.</p> <p>However, this method doesn\u2019t address the underlying security flaw of the image itself. Therefore it\u2019s better to specify a non-root user in your Dockerfile so your containers always run securely.</p>"},{"location":"90DaysOfDevOps/day47/#private-registry","title":"Private Registry","text":"<p>Another area we have used heavily in public registries in DockerHub, with a private registry of container images set up by your organisation means that you can host where you wish or there are managed services for this as well, but all in all, this gives you complete control of the images available for you and your team.</p> <p>DockerHub is great to give you a baseline, but it's only going to be providing you with a basic service where you have to put a lot of trust into the image publisher.</p>"},{"location":"90DaysOfDevOps/day47/#lean-clean","title":"Lean &amp; Clean","text":"<p>Have mentioned this throughout, although not related to security. But the size of your container can also affect security in terms of attack surface if you have resources you do not use in your application then you do not need them in your container.</p> <p>This is also my major concern with pulling the <code>latest</code> images because that can bring a lot of bloat to your images as well. DockerHub does show the compressed size for each of the images in a repository.</p> <p>Checking <code>docker image</code> is a great command to see the size of your images.</p> <p></p>"},{"location":"90DaysOfDevOps/day47/#resources","title":"Resources","text":"<ul> <li>TechWorld with Nana - Docker Tutorial for Beginners</li> <li>Programming with Mosh - Docker Tutorial for Beginners</li> <li>Docker Tutorial for Beginners - What is Docker? Introduction to Containers</li> <li>WSL 2 with Docker getting started</li> <li>Blog on getting started building a docker image</li> <li>Docker documentation for building an image</li> <li>YAML Tutorial: Everything You Need to Get Started in Minute</li> </ul> <p>See you on Day 48</p>"},{"location":"90DaysOfDevOps/day48/","title":"#90DaysOfDevOps - Alternatives to Docker - Day 48","text":""},{"location":"90DaysOfDevOps/day48/#alternatives-to-docker","title":"Alternatives to Docker","text":"<p>I did say at the very beginning of this section that we were going to be using Docker, simply because resource wise there is so much and the community is very big, but also this was really where the indents to making containers popular came from. I would encourage you to go and watch some of the history around Docker and how it came to be, I found it very useful.</p> <p>But as I have alluded to there are other alternatives to Docker. If we think about what Docker is and what we have covered. It is a platform for developing, testing, deploying, and managing applications.</p> <p>I want to highlight a few alternatives to Docker that you might or will in the future see out in the wild.</p>"},{"location":"90DaysOfDevOps/day48/#podman","title":"Podman","text":"<p>What is Podman? Podman is a daemon-less container engine for developing, managing, and running OCI Containers on your Linux System. Containers can either be run as root or in rootless mode.</p> <p>I am going to be looking at this from a Windows point of view but know that like Docker there is no requirement for virtualisation there as it will use the underlying OS which is cannot do in the Windows world.</p> <p>Podman can be run under WSL2 although not as sleek as the experience with Docker Desktop. There is also a Windows remote client where you can connect to a Linux VM where your containers will run.</p> <p>My Ubuntu on WSL2 is the 20.04 release. Following the next steps will enable you to install Podman on your WSL instance.</p> <pre><code>echo \"deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/xUbuntu_20.04/ /\" |\nsudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list\n</code></pre> <p>Add the GPG Key</p> <pre><code>curl -L \"https://download.opensuse.org/repositories/devel:/kubic:\\\n/libcontainers:/stable/xUbuntu_20.04/Release.key\" | sudo apt-key add -\n</code></pre> <p>Run a system update and upgrade with the <code>sudo apt-get update &amp;&amp; sudo apt-get upgrade</code> command. Finally, we can install podman using <code>sudo apt install podman</code></p> <p>We can now use a lot of the same commands we have been using for docker, note that we do not have that nice docker desktop UI. You can see below I used <code>podman images</code> and I have nothing after installation then I used <code>podman pull ubuntu</code> to pull down the ubuntu container image.</p> <p></p> <p>We can then run our Ubuntu image using <code>podman run -dit ubuntu</code> and <code>podman ps</code> to see our running image.</p> <p></p> <p>To then get into that container we can run <code>podman attach dazzling_darwin</code> your container name will most likely be different.</p> <p></p> <p>If you are moving from docker to podman it is also common to change your config file to have <code>alias docker=podman</code> that way any command you run with docker will use podman.</p>"},{"location":"90DaysOfDevOps/day48/#lxc","title":"LXC","text":"<p>LXC is a containerisation engine that enables users again to create multiple isolated Linux container environments. Unlike Docker, LXC acts as a hypervisor for creating multiple Linux machines with separate system files, and networking features. Was around before Docker and then made a short comeback due to Docker's shortcomings.</p> <p>LXC is as lightweight though as docker and easily deployed.</p>"},{"location":"90DaysOfDevOps/day48/#containerd","title":"Containerd","text":"<p>A standalone container runtime. Containerd brings simplicity and robustness as well as of course portability. Containerd was formerly a tool that runs as part of Docker container services until Docker decided to graduate its components into standalone components.</p> <p>A project in the Cloud Native Computing Foundation, placing it in the same class as popular container tools like Kubernetes, Prometheus, and CoreDNS.</p>"},{"location":"90DaysOfDevOps/day48/#other-docker-tooling","title":"Other Docker tooling","text":"<p>We could also mention toolings and options around Rancher, and VirtualBox but we can cover them in more detail another time.</p> <p>Gradle</p> <ul> <li>Build scans allow teams to collaboratively debug their scripts and track the history of all builds.</li> <li>Execution options give teams the ability to continuously build so that whenever changes are inputted, the task is automatically executed.</li> <li>The custom repository layout gives teams the ability to treat any file directory structure as an artefact repository.</li> </ul> <p>Packer</p> <ul> <li>Ability to create multiple machine images in parallel to save developer time and increase efficiency.</li> <li>Teams can easily debug builds using Packer\u2019s debugger, which inspects failures and allows teams to try out solutions before restarting builds.</li> <li>Support with many platforms via plugins so teams can customize their builds.</li> </ul> <p>Logspout</p> <ul> <li>Logging tool - The tool\u2019s customizability allows teams to ship the same logs to multiple destinations.</li> <li>Teams can easily manage their files because the tool only requires access to the Docker socket.</li> <li>Completely open-sourced and easy to deploy.</li> </ul> <p>Logstash</p> <ul> <li>Customize your pipeline using Logstash\u2019s pluggable framework.</li> <li>Easily parse and transform your data for analysis and to deliver business value.</li> <li>Logstash\u2019s variety of outputs lets you route your data where you want.</li> </ul> <p>Portainer</p> <ul> <li>Utilise pre-made templates or create your own to deploy applications.</li> <li>Create teams and assign roles and permissions to team members.</li> <li>Know what is running in each environment using the tool\u2019s dashboard.</li> </ul>"},{"location":"90DaysOfDevOps/day48/#resources","title":"Resources","text":"<ul> <li>TechWorld with Nana - Docker Tutorial for Beginners</li> <li>Programming with Mosh - Docker Tutorial for Beginners</li> <li>Docker Tutorial for Beginners - What is Docker? Introduction to Containers</li> <li>WSL 2 with Docker getting started</li> <li>Blog on getting started building a docker image</li> <li>Docker documentation for building an image</li> <li>YAML Tutorial: Everything You Need to Get Started in Minute</li> <li>Podman | Daemonless Docker | Getting Started with Podman</li> <li>LXC - Guide to building an LXC Lab</li> </ul> <p>See you on Day 49</p>"},{"location":"90DaysOfDevOps/day49/","title":"#90DaysOfDevOps - The Big Picture: Kubernetes - Day 49","text":""},{"location":"90DaysOfDevOps/day49/#the-big-picture-kubernetes","title":"The Big Picture: Kubernetes","text":"<p>In the last section we covered Containers, Containers fall short when it comes to scale and orchestration alone. The best we can do is use docker-compose to bring up multiple containers together. When it comes to Kubernetes which is a Container Orchestrator, this gives us the ability to scale up and down in an automated way or based on a load of your applications and services.</p> <p>As a platform Kubernetes offers the ability to orchestrate containers according to your requirements and desired state. We are going to cover Kubernetes in this section as it is growing rapidly as the next wave of infrastructure. I would also suggest that from a DevOps perspective Kubernetes is just one platform that you will need to have a basic understanding of, you will also need to understand bare metal, virtualisation and most likely cloud-based services as well. Kubernetes is just another option to run our applications.</p>"},{"location":"90DaysOfDevOps/day49/#what-is-container-orchestration","title":"What is Container Orchestration?","text":"<p>I have mentioned Kubernetes and I have mentioned Container Orchestration, Kubernetes is the technology whereas container orchestration is the concept or the process behind the technology. Kubernetes is not the only Container Orchestration platform we also have Docker Swarm, HashiCorp Nomad and others. But Kubernetes is going from strength to strength so I want to cover Kubernetes but wanted to say that it is not the only one out there.</p>"},{"location":"90DaysOfDevOps/day49/#what-is-kubernetes","title":"What is Kubernetes?","text":"<p>The first thing you should read if you are new to Kubernetes is the official documentation, My experience of really deep diving into Kubernetes a little over a year ago was that this is going to be a steep learning curve. Coming from a virtualisation and storage background I was thinking about how daunting this felt.</p> <p>But the community, free learning resources and documentation are amazing. Kubernetes.io</p> <p>Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.</p> <p>Important things to note from the above quote, Kubernetes is Open-Source with a rich history that goes back to Google who donated the project to the Cloud Native Computing Foundation (CNCF) and it has now been progressed by the open-source community as well as large enterprise vendors contributing to making Kubernetes what it is today.</p> <p>I mentioned above that containers are great and in the previous section, we spoke about how containers and container images have changed and accelerated the adoption of cloud-native systems. But containers alone are not going to give you the production-ready experience you need from your application. Kubernetes gives us the following:</p> <ul> <li> <p>Service discovery and load balancing Kubernetes can expose a container using the DNS name or using their IP address. If traffic to a container is high, Kubernetes can load balance and distribute the network traffic so that the deployment is stable.</p> </li> <li> <p>Storage orchestration Kubernetes allows you to automatically mount a storage system of your choice, such as local storage, public cloud providers, and more.</p> </li> <li> <p>Automated rollouts and rollbacks You can describe the desired state for your deployed containers using Kubernetes, and it can change the actual state to the desired state at a controlled rate. For example, you can automate Kubernetes to create new containers for your deployment, remove existing containers and adopt all their resources to the new container.</p> </li> <li> <p>Automatic bin packing You provide Kubernetes with a cluster of nodes that it can use to run containerized tasks. You tell Kubernetes how much CPU and memory (RAM) each container needs. Kubernetes can fit containers onto your nodes to make the best use of your resources.</p> </li> <li> <p>Self-healing Kubernetes restarts containers that fail, replaces containers, kills containers that don't respond to your user-defined health check, and doesn't advertise them to clients until they are ready to serve.</p> </li> <li> <p>Secret and configuration management Kubernetes lets you store and manage sensitive information, such as passwords, OAuth tokens, and SSH keys. You can deploy and update secrets and application configuration without rebuilding your container images, and without exposing secrets in your stack configuration.</p> </li> </ul> <p>Kubernetes provides you with a framework to run distributed systems resiliently.</p> <p>Container Orchestration manages the deployment, placement, and lifecycle of containers.</p> <p>It also has many other responsibilities:</p> <ul> <li> <p>Cluster management federates hosts into one target.</p> </li> <li> <p>Schedule management distributes containers across nodes through the scheduler.</p> </li> <li> <p>Service discovery knows where containers are located and distributes client requests across them.</p> </li> <li> <p>Replication ensures that the right number of nodes and containers are available for the requested workload.</p> </li> <li> <p>Health management detects and replaces unhealthy containers and nodes.</p> </li> </ul>"},{"location":"90DaysOfDevOps/day49/#main-kubernetes-components","title":"Main Kubernetes Components","text":"<p>Kubernetes is a container orchestrator to provision, manage, and scale apps. You can use it to manage the lifecycle of containerized apps in a cluster of nodes, which is a collection of worker machines such as VMs or physical machines.</p> <p>Your apps might need many other resources to run, such as volumes, networks, and secrets that can help you connect to databases, talk to firewalled back ends, and secure keys. With Kubernetes, you can add those resources to your app. Infrastructure resources that your apps need are managed declaratively.</p> <p>The key paradigm of Kubernetes is its declarative model. You provide the state that you want and Kubernetes makes it happen. If you need five instances, you don't start five separate instances on your own. Instead, you tell Kubernetes that you need five instances, and Kubernetes automatically reconciles the state. If something goes wrong with one of your instances and it fails, Kubernetes still knows the state that you want and creates instances on an available node.</p>"},{"location":"90DaysOfDevOps/day49/#node","title":"Node","text":""},{"location":"90DaysOfDevOps/day49/#control-plane","title":"Control Plane","text":"<p>Every Kubernetes cluster requires a Control Plane node, the control plane's components make global decisions about the cluster (for example, scheduling), as well as detecting and responding to cluster events.</p> <p></p>"},{"location":"90DaysOfDevOps/day49/#worker-node","title":"Worker Node","text":"<p>A worker machine that runs Kubernetes workloads. It can be a physical (bare metal) machine or a virtual machine (VM). Each node can host one or more pods. Kubernetes nodes are managed by a control plane</p> <p></p> <p>There are other node types but I won't be covering them here.</p>"},{"location":"90DaysOfDevOps/day49/#kubelet","title":"kubelet","text":"<p>An agent that runs on each node in the cluster. It makes sure that containers are running in a Pod.</p> <p>The kubelet takes a set of PodSpecs that are provided through various mechanisms and ensures that the containers described in those PodSpecs are running and healthy. The kubelet doesn't manage containers which were not created by Kubernetes.</p> <p></p>"},{"location":"90DaysOfDevOps/day49/#kube-proxy","title":"kube-proxy","text":"<p>kube-proxy is a network proxy that runs on each node in your cluster, implementing part of the Kubernetes Service concept.</p> <p>kube-proxy maintains network rules on nodes. These network rules allow network communication to your Pods from network sessions inside or outside of your cluster.</p> <p>kube-proxy uses the operating system packet filtering layer if there is one and it's available. Otherwise, kube-proxy forwards the traffic itself.</p> <p></p>"},{"location":"90DaysOfDevOps/day49/#container-runtime","title":"Container runtime","text":"<p>The container runtime is the software that is responsible for running containers.</p> <p>Kubernetes supports several container runtimes: Docker, containerd, CRI-O, and any implementation of the Kubernetes CRI (Container Runtime Interface).</p> <p></p>"},{"location":"90DaysOfDevOps/day49/#cluster","title":"Cluster","text":"<p>A cluster is a group of nodes, where a node can be a physical machine or a virtual machine. Each of the nodes will have the container runtime (Docker) and will also be running a kubelet service, which is an agent that takes in the commands from the Master controller (more on that later) and a Proxy, that is used to proxy connections to the Pods from another component (Services, that we will see later).</p> <p>Our control plane which can be made highly available will contain some unique roles compared to the worker nodes, the most important will be the kube API server, this is where any communication will take place to get information or push information to our Kubernetes cluster.</p>"},{"location":"90DaysOfDevOps/day49/#kube-api-server","title":"Kube API-Server","text":"<p>The Kubernetes API server validates and configures data for the API objects which include pods, services, replication controllers, and others. The API Server services REST operations and provide the frontend to the cluster's shared state through which all other components interact.</p>"},{"location":"90DaysOfDevOps/day49/#scheduler","title":"Scheduler","text":"<p>The Kubernetes scheduler is a control plane process which assigns Pods to Nodes. The scheduler determines which Nodes are valid placements for each Pod in the scheduling queue according to constraints and available resources. The scheduler then ranks each valid Node and binds the Pod to a suitable Node.</p>"},{"location":"90DaysOfDevOps/day49/#controller-manager","title":"Controller Manager","text":"<p>The Kubernetes controller manager is a daemon that embeds the core control loops shipped with Kubernetes. In applications of robotics and automation, a control loop is a non-terminating loop that regulates the state of the system. In Kubernetes, a controller is a control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.</p>"},{"location":"90DaysOfDevOps/day49/#etcd","title":"etcd","text":"<p>Consistent and highly-available key value store used as Kubernetes' backing store for all cluster data.</p> <p></p>"},{"location":"90DaysOfDevOps/day49/#kubectl","title":"kubectl","text":"<p>To manage this from a CLI point of view we have kubectl, kubectl interacts with the API server.</p> <p>The Kubernetes command-line tool, kubectl, allows you to run commands against Kubernetes clusters. You can use kubectl to deploy applications, inspect and manage cluster resources, and view logs.</p> <p></p>"},{"location":"90DaysOfDevOps/day49/#pods","title":"Pods","text":"<p>A Pod is a group of containers that form a logical application. E.g. If you have a web application that is running a NodeJS container and also a MySQL container, then both these containers will be located in a single Pod. A Pod can also share common data volumes and they also share the same networking namespace. Remember that Pods are ephemeral and they could be brought up and down by the Master Controller. Kubernetes uses a simple but effective means to identify the Pods via the concepts of Labels (name \u2013 values).</p> <ul> <li> <p>Pods handle Volumes, Secrets, and configuration for containers.</p> </li> <li> <p>Pods are ephemeral. They are intended to be restarted automatically when they die.</p> </li> <li> <p>Pods are replicated when the app is scaled horizontally by the ReplicationSet. Each Pod will run the same container code.</p> </li> <li> <p>Pods live on Worker Nodes.</p> </li> </ul> <p></p>"},{"location":"90DaysOfDevOps/day49/#deployments","title":"Deployments","text":"<ul> <li> <p>You can just decide to run Pods but when they die they die.</p> </li> <li> <p>A Deployment will enable your pod to run continuously.</p> </li> <li> <p>Deployments allow you to update a running app without downtime.</p> </li> <li> <p>Deployments also specify a strategy to restart Pods when they die</p> </li> </ul> <p></p>"},{"location":"90DaysOfDevOps/day49/#replicasets","title":"ReplicaSets","text":"<ul> <li> <p>The Deployment can also create the ReplicaSet</p> </li> <li> <p>A ReplicaSet ensures your app has the desired number of Pods</p> </li> <li> <p>ReplicaSets will create and scale Pods based on the Deployment</p> </li> <li> <p>Deployments, ReplicaSets, and Pods are not exclusive but can be</p> </li> </ul>"},{"location":"90DaysOfDevOps/day49/#statefulsets","title":"StatefulSets","text":"<ul> <li> <p>Does your App require you to keep information about its state?</p> </li> <li> <p>A database needs state</p> </li> <li> <p>A StatefulSet\u2019s Pods are not interchangeable.</p> </li> <li> <p>Each pod has a unique, persistent identifier that the controller maintains over any rescheduling.</p> </li> </ul> <p></p>"},{"location":"90DaysOfDevOps/day49/#daemonsets","title":"DaemonSets","text":"<ul> <li> <p>DaemonSets are for continuous process</p> </li> <li> <p>They run one Pod per Node.</p> </li> <li> <p>Each new node added to the cluster gets a pod started</p> </li> <li> <p>Useful for background tasks such as monitoring and log collection</p> </li> <li> <p>Each pod has a unique, persistent identifier that the controller maintains over any rescheduling.</p> </li> </ul> <p></p>"},{"location":"90DaysOfDevOps/day49/#services","title":"Services","text":"<ul> <li> <p>A single endpoint to access Pods</p> </li> <li> <p>a unified way to route traffic to a cluster and eventually to a list of Pods.</p> </li> <li> <p>By using a Service, Pods can be brought up and down without affecting anything.</p> </li> </ul> <p>This is just a quick overview and notes around the fundamental building blocks of Kubernetes, we can take this knowledge and add in some other areas around Storage and Ingress to enhance our applications but we then also have a lot of choices on where our Kubernetes cluster runs. The next session will focus on those options on where can I run a Kubernetes cluster, whilst also exploring some specifics around Storage.</p> <p></p>"},{"location":"90DaysOfDevOps/day49/#what-we-will-cover-in-the-series-on-kubernetes","title":"What we will cover in the series on Kubernetes","text":"<ul> <li>Kubernetes Architecture</li> <li>Kubectl Commands</li> <li>Kubernetes YAML</li> <li>Kubernetes Ingress</li> <li>Kubernetes Services</li> <li>Helm Package Manager</li> <li>Persistent Storage</li> <li>Stateful Apps</li> </ul>"},{"location":"90DaysOfDevOps/day49/#resources","title":"Resources","text":"<ul> <li>Kubernetes Documentation</li> <li>TechWorld with Nana - Kubernetes Tutorial for Beginners [FULL COURSE in 4 Hours]</li> <li>TechWorld with Nana - Kubernetes Crash Course for Absolute Beginners</li> <li>Kunal Kushwaha - Kubernetes Tutorial for Beginners | What is Kubernetes? Architecture Simplified!</li> </ul> <p>See you on Day 50</p>"},{"location":"90DaysOfDevOps/day50/","title":"#90DaysOfDevOps - Choosing your Kubernetes platform - Day 50","text":""},{"location":"90DaysOfDevOps/day50/#choosing-your-kubernetes-platform","title":"Choosing your Kubernetes platform","text":"<p>I wanted to use this session to break down some of the platforms or maybe distributions is a better term to use here, one thing that has been a challenge in the Kubernetes world is removing complexity.</p> <p>Kubernetes the hard way walks through how to build out from nothing to a full-blown functional Kubernetes cluster this is to the extreme but more and more at least the people I am speaking to are wanting to remove that complexity and run a managed Kubernetes cluster. The issue there is that it costs more money but the benefits could be if you use a managed service do you need to know the underpinning node architecture and what is happening from a Control Plane node point of view when generally you do not have access to this.</p> <p>Then we have the local development distributions that enable us to use our systems and run a local version of Kubernetes so developers can have the full working environment to run their apps in the platform they are intended for.</p> <p>The general basis of all of these concepts is that they are all a flavour of Kubernetes which means we should be able to freely migrate and move our workloads where we need them to suit our requirements.</p> <p>A lot of our choice will also depend on what investments have been made. I mentioned the developer experience as well but some of those local Kubernetes environments that run our laptops are great for getting to grips with the technology without spending any money.</p>"},{"location":"90DaysOfDevOps/day50/#bare-metal-clusters","title":"Bare-Metal Clusters","text":"<p>An option for many could be running your Linux OS straight onto several physical servers to create our cluster, it could also be Windows but I have not heard much about the adoption rate around Windows, Containers and Kubernetes. If you are a business and you have made a CAPEX decision to buy your physical servers then this might be how you go when building out your Kubernetes cluster, the management and admin side here means you are going to have to build yourself and manage everything from the ground up.</p>"},{"location":"90DaysOfDevOps/day50/#virtualisation","title":"Virtualisation","text":"<p>Regardless of test and learning environments or enterprise-ready Kubernetes clusters virtualisation is a great way to go, typically the ability to spin up virtual machines to act as your nodes and then cluster those together. You have the underpinning architecture, efficiency and speed of virtualisation as well as leveraging that existing spend. VMware for example offers a great solution for both Virtual Machines and Kubernetes in various flavours.</p> <p>My first ever Kubernetes cluster was built based on Virtualisation using Microsoft Hyper-V on an old server that I had which was capable of running a few VMs as my nodes.</p>"},{"location":"90DaysOfDevOps/day50/#local-desktop-options","title":"Local Desktop options","text":"<p>There are several options when it comes to running a local Kubernetes cluster on your desktop or laptop. This as previously said gives developers the ability to see what their app will look like without having to have multiple costly or complex clusters. Personally, this has been one that I have used a lot and in particular, I have been using minikube. It has some great functionality and adds-ons which changes the way you get something up and running.</p>"},{"location":"90DaysOfDevOps/day50/#kubernetes-managed-services","title":"Kubernetes Managed Services","text":"<p>I have mentioned virtualisation, and this can be achieved with hypervisors locally but we know from previous sections we could also leverage VMs in the public cloud to act as our nodes. What I am talking about here with Kubernetes managed services are the offerings we see from the large hyperscalers but also from MSPs removing layers of management and control away from the end user, this could be removing the control plane from the end user this is what happens with Amazon EKS, Microsoft AKS and Google Kubernetes Engine. (GKE)</p>"},{"location":"90DaysOfDevOps/day50/#overwhelming-choice","title":"Overwhelming choice","text":"<p>I mean the choice is great but there is a point where things become overwhelming and this is not a depth look into all options within each category listed above. On top of the above, we also have OpenShift which is from Red Hat and this option can be run across the options above in all the major cloud providers and probably today gives the best overall useability to the admins regardless of where clusters are deployed.</p> <p>So where do you start from your learning perspective, as I said I started with the virtualisation route but that was because I had access to a physical server which I could use for the purpose, I appreciate and in fact, since then I no longer have this option.</p> <p>My actual advice now would be to use Minikube as a first option or Kind (Kubernetes in Docker) but Minikube gives us some additional benefits which almost abstracts the complexity out as we can just use add-ons and get things built out quickly and we can then blow it away when we are finished, we can run multiple clusters, we can run it almost anywhere, cross-platform and hardware agnostic.</p> <p>I have been through a bit of a journey with my learning around Kubernetes so I am going to leave the platform choice and specifics here to list out the options that I have tried to give me a better understanding of Kubernetes the platform and where it can run. What I might do with the below blog posts is take another look at these update them and bring them more into here vs them being linked to blog posts.</p> <ul> <li>Kubernetes playground \u2013 How to choose your platform</li> <li>Kubernetes playground \u2013 Setting up your cluster</li> <li>Getting started with Amazon Elastic Kubernetes Service (Amazon EKS)</li> <li>Getting started with Microsoft Azure Kubernetes Service (AKS)</li> <li>Getting Started with Microsoft AKS \u2013 Azure PowerShell Edition</li> <li>Getting started with Google Kubernetes Service (GKE)</li> <li>Kubernetes, How to \u2013 AWS Bottlerocket + Amazon EKS</li> <li>Getting started with CIVO Cloud</li> <li>Minikube - Kubernetes Demo Environment For Everyone</li> </ul>"},{"location":"90DaysOfDevOps/day50/#what-we-will-cover-in-the-series-on-kubernetes","title":"What we will cover in the series on Kubernetes","text":"<ul> <li>Kubernetes Architecture</li> <li>Kubectl Commands</li> <li>Kubernetes YAML</li> <li>Kubernetes Ingress</li> <li>Kubernetes Services</li> <li>Helm Package Manager</li> <li>Persistent Storage</li> <li>Stateful Apps</li> </ul>"},{"location":"90DaysOfDevOps/day50/#resources","title":"Resources","text":"<ul> <li>Kubernetes Documentation</li> <li>TechWorld with Nana - Kubernetes Tutorial for Beginners [FULL COURSE in 4 Hours]</li> <li>TechWorld with Nana - Kubernetes Crash Course for Absolute Beginners</li> <li>Kunal Kushwaha - Kubernetes Tutorial for Beginners | What is Kubernetes? Architecture Simplified!</li> </ul> <p>See you on Day 51</p>"},{"location":"90DaysOfDevOps/day51/","title":"#90DaysOfDevOps - Deploying your first Kubernetes Cluster - Day 51","text":""},{"location":"90DaysOfDevOps/day51/#deploying-your-first-kubernetes-cluster","title":"Deploying your first Kubernetes Cluster","text":"<p>In this post we are going to get a Kubernetes cluster up and running on our local machine using minikube, this will give us a baseline Kubernetes cluster for the rest of the Kubernetes section, although we will look at deploying a Kubernetes cluster also in VirtualBox later on. The reason for choosing this method vs spinning a managed Kubernetes cluster up in the public cloud is that this is going to cost money even with the free tier, I shared some blogs though if you would like to spin up that environment in the previous section Day 50.</p>"},{"location":"90DaysOfDevOps/day51/#what-is-minikube","title":"What is Minikube?","text":"<p>\u201cminikube quickly sets up a local Kubernetes cluster on macOS, Linux, and Windows. We proudly focus on helping application developers and new Kubernetes users.\u201d</p> <p>You might not fit into the above but I have found minikube is a great little tool if you just want to test something out in a Kubernetes fashion, you can easily deploy an app and they have some amazing add-ons which I will also cover.</p> <p>To begin with, regardless of your workstation OS, you can run minikube. First, head over to the project page here. The first option you have is choosing your installation method. I did not use this method, but you might choose to vs my way (my way is coming up).</p> <p>mentioned below it states that you need to have a \u201cContainer or virtual machine managers, such as Docker, Hyper kit, Hyper-V, KVM, Parallels, Podman, VirtualBox, or VMware\u201d this is where MiniKube will run and the easy option and unless stated in the repository I am using Docker. You can install Docker on your system using the following link.</p> <p></p>"},{"location":"90DaysOfDevOps/day51/#my-way-of-installing-minikube-and-other-prereqs","title":"My way of installing minikube and other prereqs\u2026","text":"<p>I have been using arkade for some time now to get all those Kubernetes tools and CLIs, you can see the installation steps on this github repository for getting started with Arkade. I have also mentioned this in other blog posts where I needed something installed. The simplicity of just hitting arkade get and then seeing if your tool or cli is available in handy. In the Linux section, we spoke about package manager and the process for getting our software, you can think about Arkade as that marketplace for all your apps and CLIs for Kubernetes. A very handy little tool to have on your systems, written in Golang and cross-platform.</p> <p></p> <p>As part of the long list of available apps within arkade minikube is one of them so with a simple <code>arkade get minikube</code> command we are now downloading the binary and we are good to go.</p> <p></p> <p>We will also need kubectl as part of our tooling so you can also get this via arkade or I believe that the minikube documentation brings this down as part of the curl commands mentioned above. We will cover more on kubectl later on in the post.</p>"},{"location":"90DaysOfDevOps/day51/#getting-a-kubernetes-cluster-up-and-running","title":"Getting a Kubernetes cluster up and running","text":"<p>For this particular section, I want to cover the options available to us when it comes to getting a Kubernetes cluster up and running on your local machine. We could simply run the following command and it would spin up a cluster for you to use.</p> <p>minikube is used on the command line, and simply put once you have everything installed you can run <code>minikube start</code> to deploy your first Kubernetes cluster. You will see below that the Docker Driver is the default as to where we will be running our nested virtualisation node. I mentioned at the start of the post the other options available, the other options help when you want to expand what this local Kubernetes cluster needs to look like.</p> <p>A single Minikube cluster is going to consist of a single docker container in this instance which will have the control plane node and worker node in one instance. Whereas typically you would separate those nodes. Something we will cover in the next section where we look at still home lab type Kubernetes environments but a little closer to production architecture.</p> <p></p> <p>I have mentioned this a few times now, I like minikube because of the add-ons available, the ability to deploy a cluster with a simple command including all the required addons from the start helps me deploy the same required setup every time.</p> <p>Below you can see a list of those add-ons, I generally use the <code>CSI-host path-driver</code> and the <code>volumesnapshots</code> addons but you can see the long list below. Sure these addons can generally be deployed using Helm again something we will cover later on in the Kubernetes section but this makes things much simpler.</p> <p></p> <p>I am also defining in our project some additional configuration, apiserver is set to 6433 instead of a random API port, and I define the container runtime also to containerd however docker is default and CRI-O is also available. I am also setting a specific Kubernetes version.</p> <p></p> <p>Now we are ready to deploy our first Kubernetes cluster using minikube. I mentioned before though that you will also need <code>kubectl</code> to interact with your cluster. You can get kubectl installed using arkade with the command <code>arkade get kubectl</code></p> <p></p> <p>or you can download cross-platform from the following</p> <ul> <li>Linux</li> <li>macOS</li> <li>Windows</li> </ul> <p>Once you have kubectl installed we can then interact with our cluster with a simple command like <code>kubectl get nodes</code></p> <p></p>"},{"location":"90DaysOfDevOps/day51/#what-is-kubectl","title":"What is kubectl?","text":"<p>We now have our minikube | Kubernetes cluster up and running and I have asked you to install both Minikube where I have explained at least what it does but I have not explained what kubectl is and what it does.</p> <p>kubectl is a cli that is used or allows you to interact with Kubernetes clusters, we are using it here for interacting with our minikube cluster but we would also use kubectl for interacting with our enterprise clusters across the public cloud.</p> <p>We use kubectl to deploy applications and inspect and manage cluster resources. A much better Overview of kubectl can be found here on the Kubernetes official documentation.</p> <p>kubectl interacts with the API server found on the Control Plane node which we briefly covered in an earlier post.</p>"},{"location":"90DaysOfDevOps/day51/#kubectl-cheat-sheet","title":"kubectl cheat sheet","text":"<p>Along with the official documentation, I have also found myself with this page open all the time when looking for kubectl commands. Unofficial Kubernetes</p> Listing Resources kubectl get nodes List all nodes in cluster kubectl get namespaces List all namespaces in cluster kubectl get pods List all pods in default namespace cluster kubectl get pods -n name List all pods in the \"name\" namespace Creating Resources kubectl create namespace name Create a namespace called \"name\" kubectl create -f [filename] Create a resource from a JSON or YAML file: Editing Resources kubectl edit svc/servicename To edit a service More detail on Resources kubectl describe nodes display the state of any number of resources in detail, Delete Resources kubectl delete pod Remove resources, this can be from stdin or file <p>You will find yourself wanting to know the short names for some of the kubectl commands, for example <code>-n</code> is the short name for <code>namespace</code> which makes it easier to type a command but also if you are scripting anything you can have much tidier code.</p> Short name Full name csr certificatesigningrequests cs componentstatuses cm configmaps ds daemonsets deploy deployments ep endpoints ev events hpa horizontalpodautoscalers ing ingresses limits limitranges ns namespaces no nodes pvc persistentvolumeclaims pv persistentvolumes po pods pdb poddisruptionbudgets psp podsecuritypolicies rs replicasets rc replicationcontrollers quota resourcequotas sa serviceaccounts svc services <p>The final thing to add here is that I created another project around minikube to help me quickly spin up demo environments to display data services and protect those workloads with Kasten K10, Project Pace can be found there and would love your feedback or interaction, it also displays or includes some automated ways of deploying your minikube clusters and creating different data services applications.</p> <p>Next up, we will get into deploying multiple nodes into virtual machines using VirtualBox but we are going to hit the easy button there as we did in the Linux section where we used vagrant to quickly spin up the machines and deploy our software how we want them.</p> <p>I added this list to the post yesterday which are walkthrough blogs I have done around different Kubernetes clusters being deployed.</p> <ul> <li>Kubernetes playground \u2013 How to choose your platform</li> <li>Kubernetes playground \u2013 Setting up your cluster</li> <li>Getting started with Amazon Elastic Kubernetes Service (Amazon EKS)</li> <li>Getting started with Microsoft Azure Kubernetes Service (AKS)</li> <li>Getting Started with Microsoft AKS \u2013 Azure PowerShell Edition</li> <li>Getting started with Google Kubernetes Service (GKE)</li> <li>Kubernetes, How to \u2013 AWS Bottlerocket + Amazon EKS</li> <li>Getting started with CIVO Cloud</li> <li>Minikube - Kubernetes Demo Environment For Everyone</li> <li>Minikube - Deploy Minikube Using Vagrant and Ansible on VirtualBox</li> </ul>"},{"location":"90DaysOfDevOps/day51/#what-we-will-cover-in-the-series-on-kubernetes","title":"What we will cover in the series on Kubernetes","text":"<p>We have started covering some of these mentioned below but we are going to get more hands-on tomorrow with our second cluster deployment then we can start deploying applications into our clusters.</p> <ul> <li>Kubernetes Architecture</li> <li>Kubectl Commands</li> <li>Kubernetes YAML</li> <li>Kubernetes Ingress</li> <li>Kubernetes Services</li> <li>Helm Package Manager</li> <li>Persistent Storage</li> <li>Stateful Apps</li> </ul>"},{"location":"90DaysOfDevOps/day51/#resources","title":"Resources","text":"<p>If you have FREE resources that you have used then please feel free to add them here via a PR to the repository and I will be happy to include them.</p> <ul> <li>Kubernetes Documentation</li> <li>TechWorld with Nana - Kubernetes Tutorial for Beginners [FULL COURSE in 4 Hours]</li> <li>TechWorld with Nana - Kubernetes Crash Course for Absolute Beginners</li> <li>Kunal Kushwaha - Kubernetes Tutorial for Beginners | What is Kubernetes? Architecture Simplified!</li> <li>Techbeatly - Deploy Minikube Using Vagrant and Ansible on VirtualBox</li> </ul> <p>See you on Day 52</p>"},{"location":"90DaysOfDevOps/day52/","title":"#90DaysOfDevOps - Setting up a multinode Kubernetes Cluster - Day 52","text":""},{"location":"90DaysOfDevOps/day52/#setting-up-a-multinode-kubernetes-cluster","title":"Setting up a multinode Kubernetes Cluster","text":"<p>I wanted this title to be \"Setting up a multinode Kubernetes cluster with Vagrant\" but thought it might be a little too long!</p> <p>In the session yesterday we used a cool project to deploy our first Kubernetes cluster and get a little hands-on with the most important CLI tool you will come across when using Kubernetes (kubectl).</p> <p>Here we are going to use VirtualBox as our base but as mentioned the last time we spoke about Vagrant back in the Linux section we can use any hypervisor or virtualisation tool supported. It was Day 14 when we went through and deployed an Ubuntu machine for the Linux section.</p>"},{"location":"90DaysOfDevOps/day52/#a-quick-recap-on-vagrant","title":"A quick recap on Vagrant","text":"<p>Vagrant is a CLI utility that manages the lifecycle of your virtual machines. We can use vagrant to spin up and down virtual machines across many different platforms including vSphere, Hyper-v, Virtual Box and also Docker. It does have other providers but we will stick with that we are using Virtual Box here so we are good to go.</p> <p>I am going to be using a baseline this blog and repository to walk through the configuration. I would however advise that if this is your first time deploying a Kubernetes cluster then maybe also look into how you would do this manually and then at least you know what this looks like. Although I will say that this Day 0 operations and effort is being made more efficient with every release of Kubernetes. I liken this very much to the days of VMware and ESX and how you would need at least a day to deploy 3 ESX servers now we can have that up and running in an hour. We are heading in that direction when it comes to Kubernetes.</p>"},{"location":"90DaysOfDevOps/day52/#kubernetes-lab-environment","title":"Kubernetes Lab environment","text":"<p>I have uploaded in Kubernetes folder the vagrantfile that we will be using to build out our environment. Grab this and navigate to this directory in your terminal. I am again using Windows so I will be using PowerShell to perform my workstation commands with vagrant. If you do not have vagrant then you can use arkade, we covered this yesterday when installing minikube and other tools. A simple command <code>arkade get vagrant</code> should see your download and install the latest version of vagrant.</p> <p>When you are in your directory then you can simply run <code>vagrant up</code> and if all is configured correctly then you should see the following kick-off in your terminal.</p> <p></p> <p>In the terminal, you are going to see several steps taking place, but in the meantime let's take a look at what we are building here.</p> <p></p> <p>From the above you can see that we are going to build out 3 virtual machines, we will have a control plane node and then two worker nodes. If you head back to Day 49 You will see some more descriptions of these areas we see in the image.</p> <p>Also in the image, we indicate that our kubectl access will come from outside of the cluster and hit that kube apiserver when in fact as part of the vagrant provisioning we are deploying kubectl on each of these nodes so that we can access the cluster from within each of our nodes.</p> <p>The process of building out this lab could take anything from 5 minutes to 30 minutes depending on your setup.</p> <p>I am going to cover the scripts shortly as well but you will notice if you look into the vagrant file that we are calling on 3 scripts as part of the deployment and this is really where the cluster is created. We have seen how easy it is to use vagrant to deploy our virtual machines and OS installations using vagrant boxes but having the ability to run a shell script as part of the deployment process is where it gets quite interesting around automating these lab build-outs.</p> <p>Once complete we can then ssh to one of our nodes <code>vagrant ssh master</code> from the terminal should get you access, the default username and password is <code>vagrant/vagrant</code></p> <p>You can also use <code>vagrant ssh node01</code> and <code>vagrant ssh node02</code> to gain access to the worker nodes should you wish.</p> <p></p> <p>Now we are in one of the above nodes in our new cluster we can issue <code>kubectl get nodes</code> to show our 3 node cluster and the status of this.</p> <p></p> <p>At this point, we have a running 3 node cluster, with 1 control plane node and 2 worker nodes.</p>"},{"location":"90DaysOfDevOps/day52/#vagrantfile-and-shell-script-walkthrough","title":"Vagrantfile and Shell Script walkthrough","text":"<p>If we take a look at our vagrantfile, you will see that we are defining several worker nodes, networking IP addresses for the bridged network within VirtualBox and then some naming. Another you will notice is that we are also calling upon some scripts that we want to run on specific hosts.</p> <pre><code>NUM_WORKER_NODES=2\nIP_NW=\"10.0.0.\"\nIP_START=10\n\nVagrant.configure(\"2\") do |config|\n    config.vm.provision \"shell\", inline: &lt;&lt;-SHELL\n        apt-get update -y\n        echo \"$IP_NW$((IP_START))  master-node\" &gt;&gt; /etc/hosts\n        echo \"$IP_NW$((IP_START+1))  worker-node01\" &gt;&gt; /etc/hosts\n        echo \"$IP_NW$((IP_START+2))  worker-node02\" &gt;&gt; /etc/hosts\n    SHELL\n    config.vm.box = \"bento/ubuntu-21.10\"\n    config.vm.box_check_update = true\n\n    config.vm.define \"master\" do |master|\n      master.vm.hostname = \"master-node\"\n      master.vm.network \"private_network\", ip: IP_NW + \"#{IP_START}\"\n      master.vm.provider \"virtualbox\" do |vb|\n          vb.memory = 4048\n          vb.cpus = 2\n          vb.customize [\"modifyvm\", :id, \"--natdnshostresolver1\", \"on\"]\n      end\n      master.vm.provision \"shell\", path: \"scripts/common.sh\"\n      master.vm.provision \"shell\", path: \"scripts/master.sh\"\n    end\n\n    (1..NUM_WORKER_NODES).each do |i|\n      config.vm.define \"node0#{i}\" do |node|\n        node.vm.hostname = \"worker-node0#{i}\"\n        node.vm.network \"private_network\", ip: IP_NW + \"#{IP_START + i}\"\n        node.vm.provider \"virtualbox\" do |vb|\n            vb.memory = 2048\n            vb.cpus = 1\n            vb.customize [\"modifyvm\", :id, \"--natdnshostresolver1\", \"on\"]\n        end\n        node.vm.provision \"shell\", path: \"scripts/common.sh\"\n        node.vm.provision \"shell\", path: \"scripts/node.sh\"\n      end\n    end\n  end\n</code></pre> <p>Let's break down those scripts that are being run. We have three scripts listed in the above VAGRANTFILE to run on specific nodes.</p> <p><code>master.vm.provision \"shell\", path: \"scripts/common.sh\"</code></p> <p>This script above is going to focus on getting the nodes ready, it is going to be run on all 3 of our nodes and it will remove any existing Docker components and reinstall Docker and ContainerD as well kubeadm, kubelet and kubectl. This script will also update existing software packages on the system.</p> <p><code>master.vm.provision \"shell\", path: \"scripts/master.sh\"</code></p> <p>The master.sh script will only run on the control plane node, this script is going to create the Kubernetes cluster using kubeadm commands. It will also prepare the config context for access to this cluster which we will cover next.</p> <p><code>node.vm.provision \"shell\", path: \"scripts/node.sh\"</code></p> <p>This is simply going to take the config created by the master and join our nodes to the Kubernetes cluster, this join process again uses kubeadm and another script which can be found in the config folder.</p>"},{"location":"90DaysOfDevOps/day52/#access-to-the-kubernetes-cluster","title":"Access to the Kubernetes cluster","text":"<p>Now we have two clusters deployed we have the minikube cluster that we deployed in the previous section and we have the new 3 node cluster we just deployed to VirtualBox.</p> <p>Also, that config file that you will also have access to on the machine, you ran vagrant from consists of how we can gain access to our cluster from our workstation.</p> <p>Before we show that let me touch on the context.</p> <p></p> <p>Context is important, the ability to access your Kubernetes cluster from your desktop or laptop is required. Lots of different options out there and people use different operating systems as their daily drivers.</p> <p>By default, the Kubernetes CLI client (kubectl) uses the C:\\Users\\username.kube\\config to store the Kubernetes cluster details such as endpoint and credentials. If you have deployed a cluster you will be able to see this file in that location. But if you have been using maybe the master node to run all of your kubectl commands so far via SSH or other methods then this post will hopefully help you get to grips with being able to connect with your workstation.</p> <p>We then need to grab the kubeconfig file from the cluster or we can also get this from our config file once deployed, grab the contents of this file either via SCP or just open a console session to your master node and copy to the local windows machine.</p> <p></p> <p>We then want to take a copy of that config file and move it to our <code>$HOME/.kube/config</code> location.</p> <p></p> <p>Now from your local workstation, you will be able to run <code>kubectl cluster-info</code> and <code>kubectl get nodes</code> to validate that you have access to your cluster.</p> <p></p> <p>This not only allows for connectivity and control from your windows machine but this then also allows us to do some port forwarding to access certain services from our windows machine</p> <p>If you are interested in how you would manage multiple clusters on your workstation then I have a more detailed walkthrough here.</p> <p>I have added this list which are walkthrough blogs I have done around different Kubernetes clusters being deployed.</p> <ul> <li>Kubernetes playground \u2013 How to choose your platform</li> <li>Kubernetes playground \u2013 Setting up your cluster</li> <li>Getting started with Amazon Elastic Kubernetes Service (Amazon EKS)</li> <li>Getting started with Microsoft Azure Kubernetes Service (AKS)</li> <li>Getting Started with Microsoft AKS \u2013 Azure PowerShell Edition</li> <li>Getting started with Google Kubernetes Service (GKE)</li> <li>Kubernetes, How to \u2013 AWS Bottlerocket + Amazon EKS</li> <li>Getting started with CIVO Cloud</li> <li>Minikube - Kubernetes Demo Environment For Everyone</li> </ul>"},{"location":"90DaysOfDevOps/day52/#what-we-will-cover-in-the-series-on-kubernetes","title":"What we will cover in the series on Kubernetes","text":"<p>We have started covering some of these mentioned below but we are going to get more hands-on tomorrow with our second cluster deployment then we can start deploying applications into our clusters.</p> <ul> <li>Kubernetes Architecture</li> <li>Kubectl Commands</li> <li>Kubernetes YAML</li> <li>Kubernetes Ingress</li> <li>Kubernetes Services</li> <li>Helm Package Manager</li> <li>Persistent Storage</li> <li>Stateful Apps</li> </ul>"},{"location":"90DaysOfDevOps/day52/#resources","title":"Resources","text":"<p>If you have FREE resources that you have used then please feel free to add them here via a PR to the repository and I will be happy to include them.</p> <ul> <li>Kubernetes Documentation</li> <li>TechWorld with Nana - Kubernetes Tutorial for Beginners [FULL COURSE in 4 Hours]</li> <li>TechWorld with Nana - Kubernetes Crash Course for Absolute Beginners</li> <li>Kunal Kushwaha - Kubernetes Tutorial for Beginners | What is Kubernetes? Architecture Simplified!</li> </ul> <p>See you on Day 53</p>"},{"location":"90DaysOfDevOps/day53/","title":"#90DaysOfDevOps - Rancher Overview - Hands On - Day 53","text":""},{"location":"90DaysOfDevOps/day53/#rancher-overview-hands-on","title":"Rancher Overview - Hands On","text":"<p>In this section we are going to take a look at Rancher, so far everything we have done has been in the cli and using kubectl but we have a few good UIs and multi-cluster management tools to give our operations teams good visibility into our cluster management.</p> <p>Rancher is according to their site</p> <p>Rancher is a complete software stack for teams adopting containers. It addresses the operational and security challenges of managing multiple Kubernetes clusters across any infrastructure while providing DevOps teams with integrated tools for running containerized workloads.</p> <p>Rancher enables us to deploy production-grade Kubernetes clusters from pretty much any location and then provides centralised authentication, access control and observability. I mentioned in a previous section that there is almost an overwhelming choice when it comes to Kubernetes and where you should or could run them, looking at Rancher it doesn't matter where they are.</p>"},{"location":"90DaysOfDevOps/day53/#deploy-rancher","title":"Deploy Rancher","text":"<p>The first thing we need to do is deploy Rancher on our local workstation, there are a few ways and locations you can choose to proceed with this step, for me I want to use my local workstation and run rancher as a docker container. By running the command below we will pull down a container image and then have access to the rancher UI.</p> <p>Other rancher deployment methods are available Rancher Quick-Start-Guide</p> <p><code>sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher</code></p> <p>As you can see in our Docker Desktop we have a running rancher container.</p> <p></p>"},{"location":"90DaysOfDevOps/day53/#accessing-rancher-ui","title":"Accessing Rancher UI","text":"<p>With the above container running we should be able to navigate to it via a web page. <code>https://localhost</code> will bring up a login page as per below.</p> <p></p> <p>Follow the instructions below to get the password required. Because I am using Windows I chose to use bash for Windows because of the grep command required.</p> <p></p> <p>We can then take the above password and log in, the next page is where we can define a new password.</p> <p></p> <p>Once we have done the above we will then be logged in and we can see our opening screen. As part of the Rancher deployment, we will also see a local K3s cluster provisioned.</p> <p></p>"},{"location":"90DaysOfDevOps/day53/#a-quick-tour-of-rancher","title":"A quick tour of rancher","text":"<p>The first thing for us to look at is our locally deployed K3S cluster You can see below that we get a good visual of what is happening inside our cluster. This is the default deployment and we have not yet deployed anything to this cluster. You can see it is made up of 1 node and has 5 deployments. Then you can also see that there are some stats on pods, cores and memory.</p> <p></p> <p>On the left-hand menu, we also have an Apps &amp; Marketplace tab, this allows us to choose applications we would like to run on our clusters, as mentioned previously Rancher gives us the capability of running or managing several different clusters. With the marketplace, we can deploy our applications very easily.</p> <p></p> <p>Another thing to mention is that if you did need to get access to any cluster being managed by Rancher in the top right you can open a kubectl shell to the selected cluster.</p> <p></p>"},{"location":"90DaysOfDevOps/day53/#create-a-new-cluster","title":"Create a new cluster","text":"<p>Over the past two sessions, we have created a minikube cluster locally and we have used Vagrant with VirtualBox to create a 3 node Kubernetes cluster, with Rancher we can also create clusters. In the Rancher Folder you will find additional vagrant files that will build out the same 3 nodes but without the steps for creating our Kubernetes cluster (we want Rancher to do this for us)</p> <p>We do however want docker installed and for the OS to be updated so you will still see the <code>common.sh</code> script being run on each of our nodes. This will also install Kubeadm, Kubectl etc. But it will not run the Kubeadm commands to create and join our nodes into a cluster.</p> <p>We can navigate to our vagrant folder location and we can simply run <code>vagrant up</code> and this will begin the process of creating our 3 VMs in VirtualBox.</p> <p></p> <p>Now that we have our nodes or VMs in place and ready, we can then use Rancher to create our new Kubernetes cluster. The first screen to create your cluster gives you some options as to where your cluster is, i.e are you using the public cloud managed Kubernetes services, vSphere or something else.</p> <p></p> <p>We will be choosing \"custom\" as we are not using one of the integrated platforms. The opening page is where you define your cluster name (it says local below but you cannot use local, our cluster is called vagrant.) you can define Kubernetes versions here, network providers and some other configuration options to get your Kubernetes cluster up and running.</p> <p></p> <p>The next page is going to give you the registration code that needs to be run on each of your nodes with the appropriate services to be enabled. etcd, control-plane and worker. For our master node, we want etcd and control-plane so the command can be seen below.</p> <p></p> <pre><code>sudo docker run -d --privileged --restart=unless-stopped --net=host -v /etc/kubernetes:/etc/kubernetes -v /var/run:/var/run  rancher/rancher-agent:v2.6.3 --server https://10.0.0.1 --token mpq8cbjjwrj88z4xmf7blqxcfmwdsmq92bmwjpphdkklfckk5hfwc2 --ca-checksum a81944423cbfeeb92be0784edebba1af799735ebc30ba8cbe5cc5f996094f30b --etcd --controlplane\n</code></pre> <p>If networking is configured correctly then you should pretty quickly see the following in your rancher dashboard, indicating that the first master node is now being registered and the cluster is being created.</p> <p></p> <p>We can then repeat the registration process for each of the worker nodes with the following command and after some time you will have your cluster up and running with the ability to leverage the marketplace to deploy your applications.</p> <pre><code>sudo docker run -d --privileged --restart=unless-stopped --net=host -v /etc/kubernetes:/etc/kubernetes -v /var/run:/var/run  rancher/rancher-agent:v2.6.3 --server https://10.0.0.1 --token mpq8cbjjwrj88z4xmf7blqxcfmwdsmq92bmwjpphdkklfckk5hfwc2 --ca-checksum a81944423cbfeeb92be0784edebba1af799735ebc30ba8cbe5cc5f996094f30b --worker\n</code></pre> <p></p> <p>Over the last 3 sessions, we have used a few different ways to get up and running with a Kubernetes cluster, over the remaining days we are going to look at the application side of the platform arguably the most important. We will look into services and being able to provision and use our service in Kubernetes.</p> <p>I have been told since that the requirements around bootstrapping rancher nodes require those VMs to have 4GB ram or they will crash-loop, I have since updated as our worker nodes had 2GB.</p>"},{"location":"90DaysOfDevOps/day53/#what-we-will-cover-in-the-series-on-kubernetes","title":"What we will cover in the series on Kubernetes","text":"<p>We have started covering some of these mentioned below but we are going to get more hands-on tomorrow with our second cluster deployment then we can start deploying applications into our clusters.</p> <ul> <li>Kubernetes Architecture</li> <li>Kubectl Commands</li> <li>Kubernetes YAML</li> <li>Kubernetes Ingress</li> <li>Kubernetes Services</li> <li>Helm Package Manager</li> <li>Persistent Storage</li> <li>Stateful Apps</li> </ul>"},{"location":"90DaysOfDevOps/day53/#resources","title":"Resources","text":"<p>If you have FREE resources that you have used then please feel free to add them here via a PR to the repository and I will be happy to include them.</p> <ul> <li>Kubernetes Documentation</li> <li>TechWorld with Nana - Kubernetes Tutorial for Beginners [FULL COURSE in 4 Hours]</li> <li>TechWorld with Nana - Kubernetes Crash Course for Absolute Beginners</li> <li>Kunal Kushwaha - Kubernetes Tutorial for Beginners | What is Kubernetes? Architecture Simplified!</li> </ul> <p>See you on Day 54</p>"},{"location":"90DaysOfDevOps/day54/","title":"#90DaysOfDevOps - Kubernetes Application Deployment - Day 54","text":""},{"location":"90DaysOfDevOps/day54/#kubernetes-application-deployment","title":"Kubernetes Application Deployment","text":"<p>Now we finally get to deploying some applications into our clusters, some would say this is the reason Kubernetes exists, for Application delivery.</p> <p>The idea here is that we can take our container images and now deploy these as pods into our Kubernetes cluster to take advantage of Kubernetes as a container orchestrator.</p>"},{"location":"90DaysOfDevOps/day54/#deploying-apps-into-kubernetes","title":"Deploying Apps into Kubernetes","text":"<p>There are several ways in which we can deploy our applications into our Kubernetes cluster, we will cover two of the most common approaches which will be YAML files and Helm charts.</p> <p>We will be using our minikube cluster for these application deployments. We will be walking through some of the previously mentioned components or building blocks of Kubernetes.</p> <p>All through this section and the Container section we have discussed images and the benefits of Kubernetes and how we can handle scale quite easily on this platform.</p> <p>In this first step, we are simply going to create a stateless application within our minikube cluster. We will be using the defacto standard stateless application in our first demonstration <code>nginx</code> we will configure a Deployment, which will provide us with our pods and then we will also create a service which will allow us to navigate to the simple web server hosted by the nginx pod. All of this will be contained in a namespace.</p> <p></p>"},{"location":"90DaysOfDevOps/day54/#creating-the-yaml","title":"Creating the YAML","text":"<p>In the first demo, we want to define everything we do with YAML, we could have a whole section on YAML but I am going to skim over this and leave some resources at the end that will cover YAML in more detail.</p> <p>We could create the following as one YAML file or we could break this down for each aspect of our application, i.e this could be separate files for namespace, deployment and service creation but in this file, below we separate these by using <code>---</code> in one file. You can find this file located here (File name:- nginx-stateless-demo.YAML)</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: nginx\n  \"labels\": {\n    \"name\": \"nginx\"\n  }\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  namespace: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\n  namespace: nginx\nspec:\n  selector:\n    app: nginx-deployment\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n</code></pre>"},{"location":"90DaysOfDevOps/day54/#checking-our-cluster","title":"Checking our cluster","text":"<p>Before we deploy anything we should just make sure that we have no existing namespaces called <code>nginx</code> we can do this by running the <code>kubectl get namespace</code> command and as you can see below we do not have a namespace called <code>nginx</code></p> <p></p>"},{"location":"90DaysOfDevOps/day54/#time-to-deploy-our-app","title":"Time to deploy our App","text":"<p>Now we are ready to deploy our application to our minikube cluster, this same process will work on any other Kubernetes cluster.</p> <p>We need to navigate to our YAML file location and then we can run <code>kubectl create -f nginx-stateless-demo.yaml</code> which you then see that 3 objects have been created, and we have a namespace, deployment and service.</p> <p></p> <p>Let's run the command again to see our available namespaces in our cluster <code>kubectl get namespace</code> and you can now see that we have our new namespace.</p> <p></p> <p>If we then check our namespace for pods using <code>kubectl get pods -n nginx</code> you will see that we have 1 pod in a ready and running state.</p> <p></p> <p>We can also check our service is created by running <code>kubectl get service -n nginx</code></p> <p></p> <p>Finally, we can then go and check our deployment, the deployment is where and how we keep our desired configuration.</p> <p></p> <p>The above takes a few commands that are worth knowing but you can also use <code>kubectl get all -n nginx</code> to see everything we deployed with that one YAML file.</p> <p></p> <p>You will notice in the above that we also have a replicaset, in our deployment we define how many replicas of our image we would like to deploy. This was set to 1 initially, but if we wanted to quickly scale our application then we can do these several ways.</p> <p>We can edit our file using <code>kubectl edit deployment nginx-deployment -n nginx</code> which will open a text editor within your terminal and enable you to modify your deployment.</p> <p></p> <p>Upon saving the above in your text editor within the terminal if there were no issues and the correct formatting was used then you should see additional deployed in your namespace.</p> <p></p> <p>We can also make a change to the number of replicas using kubectl and the <code>kubectl scale deployment nginx-deployment --replicas=10 -n nginx</code></p> <p></p> <p>We can equally use this method to scale our application down back to 1 again if we wish to use either method. I used the edit option but you can also use the scale command above.</p> <p></p> <p>Hopefully, here you can see the use case not only are things super fast to spin up and down but we have the ability to quickly scale up and down our applications. If this was a web server we could scale up during busy times and down when the load is quiet.</p>"},{"location":"90DaysOfDevOps/day54/#exposing-our-app","title":"Exposing our app","text":"<p>But how do we access our web server?</p> <p>If you look above at our service you will see there is no External IP available so we cannot just open a web browser and expect this to be there magically. For access, we have a few options.</p> <p>ClusterIP - The IP you do see is a ClusterIP this is on an internal network on the cluster. Only things within the cluster can reach this IP.</p> <p>NodePort - Exposes the service on the same port of each of the selected nodes in the cluster using NAT.</p> <p>LoadBalancer - Creates an external load balancer in the current cloud, we are using minikube but also if you have built your own Kubernetes cluster i.e what we did in VirtualBox you would need to deploy a LoadBalancer such as metallb into your cluster to provide this functionality.</p> <p>Port-Forward - We also have the ability to Port Forward, which allows you to access and interact with internal Kubernetes cluster processes from your localhost. This option is only for testing and fault finding.</p> <p>We now have a few options to choose from, Minikube has some limitations or differences I should say to a full-blown Kubernetes cluster.</p> <p>We could simply run the following command to port forward our access using our local workstation.</p> <p><code>kubectl port-forward deployment/nginx-deployment -n nginx 8090:80</code></p> <p></p> <p>note that when you run the above command this terminal is now unusable as this is acting as your port forward to your local machine and port.</p> <p></p> <p>We are now going to run through specifically with Minikube how we can expose our application. We can also use minikube to create a URL to connect to a service More details</p> <p>First of all, we will delete our service using <code>kubectl delete service nginx-service -n nginx</code></p> <p>Next, we are going to create a new service using <code>kubectl expose deployment nginx-deployment --name nginx-service --namespace nginx --port=80 --type=NodePort</code> notice here that we are going to use the expose and change the type to NodePort.</p> <p></p> <p>Finally in a new terminal run <code>minikube --profile='mc-demo' service nginx-service --URL -n nginx</code> to create a tunnel for our service.</p> <p></p> <p>Open a browser or control and click on the link in your terminal.</p> <p></p>"},{"location":"90DaysOfDevOps/day54/#helm","title":"Helm","text":"<p>Helm is another way in which we can deploy our applications. Known as \"The package manager for Kubernetes\" You can find out more here</p> <p>Helm is a package manager for Kubernetes. Helm could be considered the Kubernetes equivalent of yum or apt. Helm deploys charts, which you can think of like a packaged application., it is a blueprint for your pre-configured application resources which can be deployed as one easy-to-use chart. You can then deploy another version of the chart with a different set of configurations.</p> <p>They have a site where you can browse all the Helm charts available and of course, you can create your own. The documentation is also clear and concise and not as daunting as when I first started hearing the term helm amongst all of the other new words in this space.</p> <p>It is super simple to get Helm up and running or installed. Simply. You can find the binaries and download links here for pretty much all distributions including your RaspberryPi arm64 devices.</p> <p>Or you can use an installer script, the benefit here is that the latest version of the helm will be downloaded and installed.</p> <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n\nchmod 700 get_helm.sh\n\n./get_helm.sh\n</code></pre> <p>Finally, there is also the option to use a package manager for the application manager, homebrew for mac, chocolatey for windows, apt with Ubuntu/Debian, snap and pkg also.</p> <p>Helm so far seems to be the go-to way to get different test applications downloaded and installed in your cluster.</p> <p>A good resource to link here would be ArtifactHUB which is a resource to find, install and publish Kubernetes packages. I will also give a shout-out to KubeApps which is a UI to display helm charts.</p>"},{"location":"90DaysOfDevOps/day54/#what-we-will-cover-in-the-series-on-kubernetes","title":"What we will cover in the series on Kubernetes","text":"<p>We have started covering some of these mentioned below but we are going to get more hands-on tomorrow with our second cluster deployment then we can start deploying applications into our clusters.</p> <ul> <li>Kubernetes Architecture</li> <li>Kubectl Commands</li> <li>Kubernetes YAML</li> <li>Kubernetes Ingress</li> <li>Kubernetes Services</li> <li>Helm Package Manager</li> <li>Persistent Storage</li> <li>Stateful Apps</li> </ul>"},{"location":"90DaysOfDevOps/day54/#resources","title":"Resources","text":"<p>If you have FREE resources that you have used then please feel free to add them here via a PR to the repository and I will be happy to include them.</p> <ul> <li>Kubernetes Documentation</li> <li>TechWorld with Nana - Kubernetes Tutorial for Beginners [FULL COURSE in 4 Hours]</li> <li>TechWorld with Nana - Kubernetes Crash Course for Absolute Beginners</li> <li>Kunal Kushwaha - Kubernetes Tutorial for Beginners | What is Kubernetes? Architecture Simplified!</li> </ul> <p>See you on Day 55</p>"},{"location":"90DaysOfDevOps/day55/","title":"#90DaysOfDevOps - State and Ingress in Kubernetes - Day 55","text":""},{"location":"90DaysOfDevOps/day55/#state-and-ingress-in-kubernetes","title":"State and Ingress in Kubernetes","text":"<p>In this closing section of Kubernetes, we are going to take a look at State and ingress.</p> <p>Everything we have said so far is about stateless, stateless is really where our applications do not care which network it is using and does not need any permanent storage. Whereas stateful apps and databases for example for such an application to function correctly, you\u2019ll need to ensure that pods can reach each other through a unique identity that does not change (hostnames, IPs...etc.). Examples of stateful applications include MySQL clusters, Redis, Kafka, MongoDB and others. Basically, through any application that stores data.</p>"},{"location":"90DaysOfDevOps/day55/#stateful-application","title":"Stateful Application","text":"<p>StatefulSets represent a set of Pods with unique, persistent identities and stable hostnames that Kubernetes maintains regardless of where they are scheduled. The state information and other resilient data for any given StatefulSet Pod are maintained in persistent disk storage associated with the StatefulSet.</p>"},{"location":"90DaysOfDevOps/day55/#deployment-vs-statefulset","title":"Deployment vs StatefulSet","text":"<ul> <li>Replicating stateful applications is more difficult.</li> <li>Replicating our pods in a deployment (Stateless Application) is identical and interchangeable.</li> <li>Create pods in random order with random hashes</li> <li>One Service that load balances to any Pod.</li> </ul> <p>When it comes to StatefulSets or Stateful Applications the above is more difficult.</p> <ul> <li>Cannot be created or deleted at the same time.</li> <li>Can't be randomly addressed.</li> <li>replica Pods are not identical</li> </ul> <p>Something you will see in our demonstration shortly is that each pod has its own identity. With a stateless Application, you will see random names. For example <code>app-7469bbb6d7-9mhxd</code> whereas a Stateful Application would be more aligned to <code>mongo-0</code> and then when scaled it will create a new pod called <code>mongo-1</code>.</p> <p>These pods are created from the same specification, but they are not interchangeable. Each StatefulSet pod has a persistent identifier across any rescheduling. This is necessary because when we require stateful workloads such as a database where we require writing and reading to a database, we cannot have two pods writing at the same time with no awareness as this will give us data inconsistency. We need to ensure that only one of our pods is writing to the database at any given time however we can have multiple pods reading that data.</p> <p>Each pod in a StatefulSet would have access to its persistent volume and replica copy of the database to read from, this is continuously updated from the master. It's also interesting to note that each pod will also store its pod state in this persistent volume, if then <code>mongo-0</code> dies then when a new one is provisioned it will take over the pod state stored in storage.</p> <p>TLDR; StatefulSets vs Deployments</p> <ul> <li>Predictable pod name = <code>mongo-0</code></li> <li>Fixed individual DNS name</li> <li>Pod Identity - Retain State, Retain Role</li> <li>Replicating stateful apps is complex</li> <li>There are lots of things you must do:<ul> <li>Configure cloning and data synchronisation.</li> <li>Make remote shared storage available.</li> <li>Management &amp; backup</li> </ul> </li> </ul>"},{"location":"90DaysOfDevOps/day55/#persistant-volumes-claims-storageclass","title":"Persistant Volumes | Claims | StorageClass","text":"<p>How to persist data in Kubernetes?</p> <p>We mentioned above when we have a stateful application, we have to store the state somewhere and this is where the need for a volume comes in, out of the box Kubernetes does not provide persistence out of the box.</p> <p>We require a storage layer that does not depend on the pod lifecycle. This storage should be available and accessible from all of our Kubernetes nodes. The storage should also be outside of the Kubernetes cluster to be able to survive even if the Kubernetes cluster crashes.</p>"},{"location":"90DaysOfDevOps/day55/#persistent-volume","title":"Persistent Volume","text":"<ul> <li>A cluster resource (like CPU and RAM) to store data.</li> <li>Created via a YAML file</li> <li>Needs actual physical storage (NAS)</li> <li>External integration to your Kubernetes cluster</li> <li>You can have different types of storage available in your storage.</li> <li>PVs are not namespaced</li> <li>Local storage is available but it would be specific to one node in the cluster</li> <li>Database persistence should use remote storage (NAS)</li> </ul>"},{"location":"90DaysOfDevOps/day55/#persistent-volume-claim","title":"Persistent Volume Claim","text":"<p>A persistent volume alone above can be there and available but unless it is claimed by an application it is not being used.</p> <ul> <li>Created via a YAML file</li> <li>Persistent Volume Claim is used in pod configuration (volumes attribute)</li> <li>PVCs live in the same namespace as the pod</li> <li>Volume is mounted into the pod</li> <li>Pods can have multiple different volume types (ConfigMap, Secret, PVC)</li> </ul> <p>Another way to think of PVs and PVCs is that</p> <p>PVs are created by the Kubernetes Admin PVCs are created by the user or application developer</p> <p>We also have two other types of volumes that we will not get into detail on but are worth mentioning:</p>"},{"location":"90DaysOfDevOps/day55/#configmaps-secrets","title":"ConfigMaps | Secrets","text":"<ul> <li>Configuration file for your pod.</li> <li>Certificate file for your pod.</li> </ul>"},{"location":"90DaysOfDevOps/day55/#storageclass","title":"StorageClass","text":"<ul> <li>Created via a YAML file</li> <li>Provisions Persistent Volumes Dynamically when a PVC claims it</li> <li>Each storage backend has its provisioner</li> <li>Storage backend is defined in YAML (via provisioner attribute)</li> <li>Abstracts underlying storage provider</li> <li>Define parameters for that storage</li> </ul>"},{"location":"90DaysOfDevOps/day55/#walkthrough-time","title":"Walkthrough time","text":"<p>In the session yesterday we walked through creating a stateless application, here we want to do the same but we want to use our minikube cluster to deploy a stateful workload.</p> <p>A recap on the minikube command we are using to have the capability and addons to use persistence is <code>minikube start --addons volumesnapshots,csi-hostpath-driver --apiserver-port=6443 --container-runtime=containerd -p mc-demo --kubernetes-version=1.21.2</code></p> <p>This command uses the CSI-hostpath-driver which is what gives us our storageclass, something I will show later.</p> <p>The build-out of the application looks like the below:</p> <p></p> <p>You can find the YAML configuration file for this application here pacman-stateful-demo.yaml</p>"},{"location":"90DaysOfDevOps/day55/#storageclass-configuration","title":"StorageClass Configuration","text":"<p>There is one more step though that we should run before we start deploying our application and that is to make sure that our storageclass (CSI-hostpath-sc) is our default one. We can firstly check this by running the <code>kubectl get storageclass</code> command but out of the box, the minikube cluster will be showing the standard storageclass as default so we have to change that with the following commands.</p> <p>This first command will make our CSI-hostpath-sc storageclass our default.</p> <p><code>kubectl patch storageclass csi-hostpath-sc -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'</code></p> <p>This command will remove the default annotation from the standard StorageClass.</p> <p><code>kubectl patch storageclass standard -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"false\"}}}'</code></p> <p></p> <p>We start with no Pacman namespace in our cluster. <code>kubectl get namespace</code></p> <p></p> <p>We will then deploy our YAML file. <code>kubectl create -f pacman-stateful-demo.yaml</code> you can see from this command we are creating several objects within our Kubernetes cluster.</p> <p></p> <p>We now have our newly created namespace.</p> <p></p> <p>You can then see from the next image and command <code>kubectl get all -n pacman</code> that we have several things happening inside of our namespace. We have our pods running our NodeJS web front end, we have mongo running our backend database. There are services for both Pacman and mongo to access those pods. We have a deployment for Pacman and a statefulset for mongo.</p> <p></p> <p>We also have our persistent volume and persistent volume claim by running <code>kubectl get pv</code> will give us our non-namespaced persistent volumes and running <code>kubectl get pvc -n pacman</code> will give us our namespaced persistent volume claims.</p> <p></p>"},{"location":"90DaysOfDevOps/day55/#playing-the-game-i-mean-accessing-our-mission-critical-application","title":"Playing the game | I mean accessing our mission-critical application","text":"<p>Because we are using Minikube as mentioned in the stateless application we have a few hurdles to get over when it comes to accessing our application, however, we had access to ingress or a load balancer within our cluster the service is set up to automatically get an IP from that to gain access externally. (you can see this above in the image of all components in the Pacman namespace).</p> <p>For this demo, we are going to use the port forward method to access our application. By opening a new terminal and running the following <code>kubectl port-forward svc/pacman 9090:80 -n pacman</code> command, opening a browser we will now have access to our application. If you are running this in AWS or specific locations then this will also report on the cloud and zone as well as the host which equals your pod within Kubernetes, again you can look back and see this pod name in our screenshots above.</p> <p></p> <p>Now we can go and create a high score which will then be stored in our database.</p> <p></p> <p>Ok, great we have a high score but what happens if we go and delete our <code>mongo-0</code> pod? by running <code>kubectl delete pod mongo-0 -n pacman</code> I can delete that and if you are still in the app you will see that high score not available at least for a few seconds.</p> <p></p> <p>Now if I go back to my game I can create a new game and see my high scores. The only way you can truly believe me on this though is if you give it a try and share on social media your high scores!</p> <p></p> <p>With the deployment, we can scale this up using the commands that we covered in the previous session but in particular here, especially if you want to host a huge Pacman party then you can scale this up using <code>kubectl scale deployment pacman --replicas=10 -n pacman</code></p> <p></p>"},{"location":"90DaysOfDevOps/day55/#ingress-explained","title":"Ingress explained","text":"<p>Before we wrap things up with Kubernetes I also wanted to touch on a huge aspect of Kubernetes and that is ingress.</p>"},{"location":"90DaysOfDevOps/day55/#what-is-ingress","title":"What is ingress?","text":"<p>So far with our examples, we have used port-forward or we have used specific commands within minikube to gain access to our applications but this in production is not going to work. We are going to want a better way of accessing our applications at scale with multiple users.</p> <p>We also spoke about NodePort being an option but again this should be only for test purposes.</p> <p>Ingress gives us a better way of exposing our applications, this allows us to define routing rules within our Kubernetes cluster.</p> <p>For ingress, we would create a forward request to the internal service of our application.</p>"},{"location":"90DaysOfDevOps/day55/#when-do-you-need-ingress","title":"When do you need ingress?","text":"<p>If you are using a cloud provider, a managed Kubernetes offering they most likely will have their ingress option for your cluster or they provide you with their load balancer option. You don't have to implement this yourself, one of the benefits of managed Kubernetes.</p> <p>If you are running your cluster then you will need to configure an entrypoint.</p>"},{"location":"90DaysOfDevOps/day55/#configure-ingress-on-minikube","title":"Configure Ingress on Minikube","text":"<p>On my particular running cluster called mc-demo, I can run the following command to get ingress enabled on my cluster.</p> <p><code>minikube --profile='mc-demo' addons enable ingress</code></p> <p></p> <p>If we check our namespaces now you will see that we have a new ingress-nginx namespace. <code>kubectl get ns</code></p> <p></p> <p>Now we must create our ingress YAML configuration to hit our Pacman service I have added this file to the repository pacman-ingress.yaml</p> <p>We can then create this in our ingress namespace with <code>kubectl create -f Pacman-ingress.yaml</code></p> <p></p> <p>Then if we run <code>kubectl get ingress -n Pacman</code></p> <p></p> <p>I am then told because we are using minikube running on WSL2 in Windows we have to create the minikube tunnel using <code>minikube tunnel --profile=mc-demo</code></p> <p>But I am still not able to gain access to 192.168.49.2 and play my Pacman game.</p> <p>If anyone has or can get this working on Windows and WSL I would appreciate the feedback. I will raise an issue on the repository for this and come back to it once I have time and a fix.</p> <p>UPDATE: I feel like this blog helps identify maybe the cause of this not working with WSL Configuring Ingress to run Minikube on WSL2 using Docker runtime</p>"},{"location":"90DaysOfDevOps/day55/#resources","title":"Resources","text":"<p>If you have FREE resources that you have used then please feel free to add them here via a PR to the repository and I will be happy to include them.</p> <ul> <li>Kubernetes StatefulSet simply explained</li> <li>Kubernetes Volumes explained</li> <li>Kubernetes Ingress Tutorial for Beginners</li> <li>Kubernetes Documentation</li> <li>TechWorld with Nana - Kubernetes Tutorial for Beginners [FULL COURSE in 4 Hours]</li> <li>TechWorld with Nana - Kubernetes Crash Course for Absolute Beginners</li> <li>Kunal Kushwaha - Kubernetes Tutorial for Beginners | What is Kubernetes? Architecture Simplified!</li> </ul> <p>This wraps up our Kubernetes section, there is so much additional content we could cover on Kubernetes and 7 days gives us foundational knowledge but people are running through 100DaysOfKubernetes where you can get really into the weeds.</p> <p>Next up we are going to be taking a look at Infrastructure as Code and the important role it plays from a DevOps perspective.</p> <p>See you on Day 56</p>"},{"location":"90DaysOfDevOps/day56/","title":"#90DaysOfDevOps - The Big Picture: IaC - Day 56","text":""},{"location":"90DaysOfDevOps/day56/#the-big-picture-iac","title":"The Big Picture: IaC","text":"<p>Humans make mistakes! Automation is the way to go!</p> <p>How do you build your systems today?</p> <p>What would be your plan if you were to lose everything today, Physical machines, Virtual Machines, Cloud VMs, Cloud PaaS etc etc.?</p> <p>How long would it take you to replace everything?</p> <p>Infrastructure as code provides a solution to be able to do this whilst also being able to test this, we should not confuse this with backup and recovery but in terms of your infrastructure and environments, your platforms we should be able to spin them up and treat them as cattle vs pets.</p> <p>The TLDR; is that we can use code to rebuild our entire environment.</p> <p>If we also remember from the start we said about DevOps in general is a way in which to break down barriers to deliver systems into production safely and rapidly.</p> <p>Infrastructure as code helps us deliver the systems, we have spoken a lot of processes and tools. IaC brings us more tools to be familiar with to enable this part of the process.</p> <p>We are going to concentrate on Infrastructure as code in this section. You might also hear this mentioned as Infrastructure from code or configuration as code. I think the most well-known term is likely Infrastructure as code.</p>"},{"location":"90DaysOfDevOps/day56/#pets-vs-cattle","title":"Pets vs Cattle","text":"<p>If we take a look at pre-DevOps, if we had the requirement to build a new Application, we would need to prepare our servers manually for the most part.</p> <ul> <li>Deploy VMs | Physical Servers and install the operating system</li> <li>Configure networking</li> <li>Create routing tables</li> <li>Install software and updates</li> <li>Configure software</li> <li>Install database</li> </ul> <p>This would be a manual process performed by Systems Administrators. The bigger the application the more resource and servers required the more manual effort it would take to bring up those systems. This would take a huge amount of human effort and time but also as a business you would have to pay for that resource to build out this environment. As I opened the section with \"Humans make mistakes! Automation is the way to go!\"</p> <p>Ongoing from the above initial setup phase you then have maintenance of these servers.</p> <ul> <li>Update versions</li> <li>Deploy new releases</li> <li>Data Management</li> <li>Recovery of Applications</li> <li>Add, Remove and Scale Servers</li> <li>Network Configuration</li> </ul> <p>Add the complexity of multiple test and dev environments.</p> <p>This is where Infrastructure as Code comes in, the above was very much a time where we would look after those servers as if they were pets, people even called them servers pet names or at least named them something because they were going to be around for a while, they were going to hopefully be part of the \"family\" for a while.</p> <p>With Infrastructure as Code, we can automate all these tasks end to end. Infrastructure as code is a concept and some tools carry out this automated provisioning of infrastructure, at this point if something bad happens to a server you throw it away and you spin up a new one. This process is automated and the server is exactly as defined in the code. At this point we don't care what they are called they are there in the field serving their purpose until they are no longer in the field and we have another to replace it either because of a failure or because we updated part or all of our application.</p> <p>This can be used in almost all platforms, virtualisation, cloud-based workloads and also cloud-native infrastructures such as Kubernetes and containers.</p>"},{"location":"90DaysOfDevOps/day56/#infrastructure-provisioning","title":"Infrastructure Provisioning","text":"<p>Not all IaC cover all of the below, You will find that the tool we are going to be using during this section only really covers the first 2 areas below; Terraform is that tool we will be covering and this allows us to start from nothing and define in code what our infrastructure should look like and then deploy that, it will also enable us to manage that infrastructure and also initially deploy an application but at that point it is going to lose track of the application which is where the next section comes in and something like Ansible as a configuration management tool might work better on that front.</p> <p>Without jumping ahead tools like chef, puppet and ansible are best suited to deal with the initial application setup and then manage those applications and their configuration.</p> <p>Initial installation &amp; configuration of software</p> <ul> <li>Spinning up new servers</li> <li>Network configuration</li> <li>Creating load balancers</li> <li>Configuration on the infrastructure level</li> </ul>"},{"location":"90DaysOfDevOps/day56/#configuration-of-provisioned-infrastructure","title":"Configuration of provisioned infrastructure","text":"<ul> <li>Installing applications on servers</li> <li>Prepare the servers to deploy your application.</li> </ul>"},{"location":"90DaysOfDevOps/day56/#deployment-of-application","title":"Deployment of Application","text":"<ul> <li>Deploy and Manage Application</li> <li>Maintain phase</li> <li>Software updates</li> <li>Reconfiguration</li> </ul>"},{"location":"90DaysOfDevOps/day56/#difference-between-iac-tools","title":"Difference between IaC tools","text":"<p>Declarative vs procedural</p> <p>Procedural</p> <ul> <li>Step-by-step instruction</li> <li>Create a server &gt; Add a server &gt; Make this change</li> </ul> <p>Declarative</p> <ul> <li>declare the result</li> <li>2 Servers</li> </ul> <p>Mutable (pets) vs Immutable (cattle)</p> <p>Mutable</p> <ul> <li>Change instead of replacing</li> <li>Generally long lived</li> </ul> <p>Immutable</p> <ul> <li>Replace instead of change</li> <li>Possibly short-lived</li> </ul> <p>This is really why we have lots of different options for Infrastructure as Code because there is no one tool to rule them all.</p> <p>We are going to be mostly using terraform and getting hands-on as this is the best way to start seeing the benefits of Infrastructure as Code when it is in action. Getting hands-on is also the best way to pick up the skills you are going to be writing code.</p> <p>Next up we will start looking into Terraform with a 101 before we get some hands-on getting used.</p>"},{"location":"90DaysOfDevOps/day56/#resources","title":"Resources","text":"<p>I have listed a lot of resources down below and I think this topic has been covered so many times out there, If you have additional resources be sure to raise a PR with your resources and I will be happy to review and add them to the list.</p> <ul> <li>What is Infrastructure as Code? Difference of Infrastructure as Code Tools</li> <li>Terraform Tutorial | Terraform Course Overview 2021</li> <li>Terraform explained in 15 mins | Terraform Tutorial for Beginners</li> <li>Terraform Course - From BEGINNER to PRO!</li> <li>HashiCorp Terraform Associate Certification Course</li> <li>Terraform Full Course for Beginners</li> <li>KodeKloud - Terraform for DevOps Beginners + Labs: Complete Step by Step Guide!</li> <li>Terraform Simple Projects</li> <li>Terraform Tutorial - The Best Project Ideas</li> <li>Awesome Terraform</li> </ul> <p>See you on Day 57</p>"},{"location":"90DaysOfDevOps/day57/","title":"#90DaysOfDevOps - An intro to Terraform - Day 57","text":""},{"location":"90DaysOfDevOps/day57/#an-intro-to-terraform","title":"An intro to Terraform","text":"<p>\"Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently\"</p> <p>The above quote is from HashiCorp, HashiCorp is the company behind Terraform.</p> <p>\"Terraform is an open-source infrastructure as a code software tool that provides a consistent CLI workflow to manage hundreds of cloud services. Terraform codifies cloud APIs into declarative configuration files.\"</p> <p>HashiCorp has a great resource in HashiCorp Learn which covers all of their products and gives some great walkthrough demos when you are trying to achieve something with Infrastructure as Code.</p> <p>All cloud providers and on-prem platforms generally give us access to management consoles which enables us to create our resources via a UI, generally, these platforms also provide a CLI or API access to create the same resources but with an API we can provision fast.</p> <p>Infrastructure as Code allows us to hook into those APIs to deploy our resources in the desired state.</p> <p>Other tools but not exclusive or exhaustive below. If you have other tools then please share via a PR.</p> Cloud Specific Cloud Agnostic AWS CloudFormation Terraform Azure Resource Manager Pulumi Google Cloud Deployment Manager <p>This is another reason why we are using Terraform, we want to be agnostic to the clouds and platforms that we wish to use for our demos but also in general.</p>"},{"location":"90DaysOfDevOps/day57/#terraform-overview","title":"Terraform Overview","text":"<p>Terraform is a provisioning-focused tool, Terraform is a CLI that gives the capabilities of being able to provision complex infrastructure environments. With Terraform we can define complex infrastructure requirements that exist locally or remote (cloud) Terraform not only enables us to build things initially but also to maintain and update those resources for their lifetime.</p> <p>We are going to cover the high level here but for more details and loads of resources, you can head to terraform. io</p>"},{"location":"90DaysOfDevOps/day57/#write","title":"Write","text":"<p>Terraform allows us to create declarative configuration files that will build our environments. The files are written using the HashiCorp Configuration Language (HCL) which allows for concise descriptions of resources using blocks, arguments, and expressions. We will of course be looking into these in detail in deploying VMs, Containers and within Kubernetes.</p>"},{"location":"90DaysOfDevOps/day57/#plan","title":"Plan","text":"<p>The ability to check that the above configuration files are going to deploy what we want to see using specific functions of the terraform cli to be able to test that plan before deploying anything or changing anything. Remember Terraform is a continued tool for your infrastructure if you would like to change aspects of your infrastructure you should do that via terraform so that it is captured all in code.</p>"},{"location":"90DaysOfDevOps/day57/#apply","title":"Apply","text":"<p>Once you are happy you can go ahead and apply this configuration to the many providers that are available within Terraform. You can see a large number of providers available here</p> <p>Another thing to mention is that there are also modules available, and this is similar to container images in that these modules have been created and shared in public so you do not have to create them again and again just reuse the best practice of deploying a specific infrastructure resource the same way everywhere. You can find the modules available here</p> <p>The Terraform workflow looks like this: (taken from the terraform site)</p> <p></p>"},{"location":"90DaysOfDevOps/day57/#terraform-vs-vagrant","title":"Terraform vs Vagrant","text":"<p>During this challenge, we have used Vagrant which happens to be another Hashicorp open source tool which concentrates on the development environments.</p> <ul> <li> <p>Vagrant is a tool focused on managing development environments</p> </li> <li> <p>Terraform is a tool for building infrastructure.</p> </li> </ul> <p>A great comparison of the two tools can be found here on the official Hashicorp site</p>"},{"location":"90DaysOfDevOps/day57/#terraform-installation","title":"Terraform Installation","text":"<p>There is not much to the installation of Terraform.</p> <p>Terraform is cross-platform and you can see below on my Linux machine we have several options to download and install the CLI</p> <p></p> <p>Using <code>arkade</code> to install Terraform, arkade is a handy little tool for getting your required tools, apps and clis onto your system. A simple <code>arkade get terraform</code> will allow for an update of terraform if available or this same command will also install the Terraform CLI</p> <p></p> <p>We are going to get into more around HCL and then also start using Terraform to create some infrastructure resources in various platforms.</p>"},{"location":"90DaysOfDevOps/day57/#resources","title":"Resources","text":"<p>I have listed a lot of resources down below and I think this topic has been covered so many times out there, If you have additional resources be sure to raise a PR with your resources and I will be happy to review and add them to the list.</p> <ul> <li>What is Infrastructure as Code? Difference of Infrastructure as Code Tools</li> <li>Terraform Tutorial | Terraform Course Overview 2021</li> <li>Terraform explained in 15 mins | Terraform Tutorial for Beginners</li> <li>Terraform Course - From BEGINNER to PRO!</li> <li>HashiCorp Terraform Associate Certification Course</li> <li>Terraform Full Course for Beginners</li> <li>KodeKloud - Terraform for DevOps Beginners + Labs: Complete Step by Step Guide!</li> <li>Terraform Simple Projects</li> <li>Terraform Tutorial - The Best Project Ideas</li> <li>Awesome Terraform</li> </ul> <p>See you on Day 58</p>"},{"location":"90DaysOfDevOps/day58/","title":"#90DaysOfDevOps - HashiCorp Configuration Language (HCL) - Day 58","text":""},{"location":"90DaysOfDevOps/day58/#hashicorp-configuration-language-hcl","title":"HashiCorp Configuration Language (HCL)","text":"<p>Before we start making stuff with Terraform we have to dive a little into HashiCorp Configuration Language (HCL). So far during our challenge, we have looked at a few different scripting and programming languages and here is another one. We touched on the Go programming language then bash scripts we even touched on a little python when it came to network automation</p> <p>Now we must cover HashiCorp Configuration Language (HCL) if this is the first time you are seeing the language it might look a little daunting but it's quite simple and very powerful.</p> <p>As we move through this section, we are going to be using examples that we can run locally on our system regardless of what OS you are using, we will be using VirtualBox, albeit not the infrastructure platform you would usually be using with Terraform. However running this locally, is free and will allow us to achieve what we are looking for in this post. We could also extend this post's concept to docker or Kubernetes as well.</p> <p>In general, though, you would or should be using Terraform to deploy your infrastructure in the public cloud (AWS, Google, Microsoft Azure) but then also in your virtualisation environments such as (VMware, Microsoft Hyper-V, Nutanix AHV). In the public cloud Terraform allows for us to do a lot more than just Virtual Machine automated deployment, we can create all the required infrastructure such as PaaS workloads and all of the networking required assets such as VPCs and Security Groups.</p> <p>There are two important aspects to Terraform, we have the code which we are going to get into in this post and then we also have the state. Both of these together could be called the Terraform core. We then have the environment we wish to speak to and deploy into, which is executed using Terraform providers, briefly mentioned in the last session, but we have an AWS provider, we have Azure providers etc. There are hundreds.</p>"},{"location":"90DaysOfDevOps/day58/#basic-terraform-usage","title":"Basic Terraform Usage","text":"<p>Let's take a look at a Terraform <code>.tf</code> file to see how they are made up. The first example we will walk through will be code to deploy resources to AWS, this would then also require the AWS CLI to be installed on your system and configured for your account.</p>"},{"location":"90DaysOfDevOps/day58/#providers","title":"Providers","text":"<p>At the top of our <code>.tf</code> file structure, generally called <code>main.tf</code> at least until we make things more complex. Here we will define the providers that we have mentioned before. Our source of the AWS provider as you can see is <code>hashicorp/aws</code> this means the provider is maintained or has been published by hashicorp themselves. By default you will reference providers that are available from the Terraform Registry, you also can write your providers, and use these locally, or self-publish to the Terraform Registry.</p> <pre><code>terraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 3.0\"\n    }\n  }\n}\n</code></pre> <p>We might also add in a region as well here to determine which AWS region we would like to provision to we can do this by adding the following:</p> <pre><code>provider \"aws\" {\n  region = \"ap-southeast-1\" //region where resources need to be deployed\n}\n</code></pre>"},{"location":"90DaysOfDevOps/day58/#terraform-resources","title":"Terraform Resources","text":"<ul> <li> <p>Another important component of a terraform config file which describes one or more infrastructure objects like EC2, Load Balancer, VPC, etc.</p> </li> <li> <p>A resource block declares a resource of a given type (\"aws_instance\") with a given local name (\"90daysofdevops\").</p> </li> <li> <p>The resource type and name together serve as an identifier for a given resource.</p> </li> </ul> <pre><code>resource \"aws_instance\" \"90daysofdevops\" {\n  ami               = data.aws_ami.instance_id.id\n  instance_type     = \"t2.micro\"\n  availability_zone = \"us-west-2a\"\n  security_groups   = [aws_security_group.allow_web.name]\n  user_data         = &lt;&lt;-EOF\n                #! /bin/bash\n                sudo yum update\n                sudo yum install -y httpd\n                sudo systemctl start httpd\n                sudo systemctl enable httpd\n                echo \"\n&lt;h1&gt;Deployed via Terraform&lt;/h1&gt;\n\n\" | sudo tee /var/www/html/index.html\n        EOF\n  tags = {\n    Name = \"Created by Terraform\"\n  }\n}\n</code></pre> <p>You can see from the above we are also running a <code>yum</code> update and installing <code>httpd</code> into our ec2 instance.</p> <p>If we now look at the complete main.tf file it might look something like this.</p> <pre><code>terraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 3.27\"\n    }\n  }\n\n  required_version = \"&gt;= 0.14.9\"\n}\n\nprovider \"aws\" {\n  profile = \"default\"\n  region  = \"us-west-2\"\n}\n\nresource \"aws_instance\" \"90daysofdevops\" {\n  ami           = \"ami-830c94e3\"\n  instance_type = \"t2.micro\"\n  availability_zone = \"us-west-2a\"\n    user_data         = &lt;&lt;-EOF\n                #! /bin/bash\n                sudo yum update\n                sudo yum install -y httpd\n                sudo systemctl start httpd\n                sudo systemctl enable httpd\n                echo \"\n&lt;h1&gt;Deployed via Terraform&lt;/h1&gt;\n\n\" | sudo tee /var/www/html/index.html\n        EOF\n  tags = {\n    Name = \"Created by Terraform\"\n\n  tags = {\n    Name = \"ExampleAppServerInstance\"\n  }\n}\n</code></pre> <p>The above code will go and deploy a very simple web server as an ec2 instance in AWS, the great thing about this and any other configuration like this is that we can repeat this and we will get the same output every single time. Other than the chance that I have messed up the code there is no human interaction with the above.</p> <p>We can take a look at a super simple example, one that you will likely never use but let's humour it anyway. Like with all good scripting and programming language we should start with a hello-world scenario.</p> <pre><code>terraform {\n  # This module is now only being tested with Terraform 0.13.x. However, to make upgrading easier, we are setting\n  # 0.12.26 as the minimum version, as that version added support for required_providers with source URLs, making it\n  # forwards compatible with 0.13.x code.\n  required_version = \"&gt;= 0.12.26\"\n}\n\n# website::tag::1:: The simplest possible Terraform module: it just outputs \"Hello, World!\"\noutput \"hello_world\" {\n  value = \"Hello, 90DaysOfDevOps from Terraform\"\n}\n</code></pre> <p>You will find this file in the IAC folder under hello-world, but out of the box, this is not going to simply work there are some commands we need to run to use our terraform code.</p> <p>In your terminal navigate to your folder where the main.tf has been created, this could be from this repository or you could create a new one using the code above.</p> <p>When in that folder we are going to run <code>terraform init</code></p> <p>We need to perform this on any directory where we have or before we run any terraform code. Initialising a configuration directory downloads and installs the providers defined in the configuration, in this case, we have no providers but in the example above this would download the AWS provider for this configuration.</p> <p></p> <p>The next command will be <code>terraform plan</code></p> <p>The <code>terraform plan</code> command creates an execution plan, which lets you preview the changes that Terraform plans to make to your infrastructure.</p> <p>You can simply see below that with our hello-world example we are going to see output if this was an AWS ec2 instance we would see all the steps that we will be creating.</p> <p></p> <p>At this point, we have initialised our repository and we have our providers downloaded where required, we have run a test walkthrough to make sure this is what we want to see so now we can run and deploy our code.</p> <p><code>terraform apply</code> allows us to do this there is a built-in safety measure to this command and this will again give you a plan view on what is going to happen which warrants a response from you to say yes to continue.</p> <p></p> <p>When we type in yes to enter a value, our code is deployed. Not that exciting but you can see we have the output that we defined in our code.</p> <p></p> <p>Now we have not deployed anything, we have not added, changed or destroyed anything but if we did then we would see that indicated also in the above. If however we had deployed something and we wanted to get rid of everything we deployed we can use the <code>terraform destroy</code> command. Again this has that safety where you have to type yes although you can use <code>--auto-approve</code> on the end of your <code>apply</code> and <code>destroy</code> commands to bypass that manual intervention. But I would advise only using this shortcut when learning and testing as everything will disappear sometimes faster than it was built.</p> <p>From this, there are 4 commands we have covered from the Terraform CLI.</p> <ul> <li><code>terraform init</code> = get your project folder ready with providers</li> <li><code>terraform plan</code> = show what is going to be created, and changed during the next command based on our code.</li> <li><code>terraform apply</code> = will go and deploy the resources defined in our code.</li> <li><code>terraform destroy</code> = will destroy the resources we have created in our project</li> </ul> <p>We also covered two important aspects of our code files.</p> <ul> <li>providers = how does terraform speak to the end platform via APIs</li> <li>resources = what it is we want to deploy with code</li> </ul> <p>Another thing to note when running <code>terraform init</code> take a look at the tree on the folder before and after to see what happens and where we store providers and modules.</p>"},{"location":"90DaysOfDevOps/day58/#terraform-state","title":"Terraform state","text":"<p>We also need to be aware of the state file that is created also inside our directory and for this hello world example our state file is simple. This is a JSON file which is the representation of the world according to Terraform. The state will happily show off your sensitive data so be careful and as a best practice put your <code>.tfstate</code> files in your <code>.gitignore</code> folder before uploading to GitHub.</p> <p>By default, the state file as you can see lives inside the same directory as your project code, but it can also be stored remotely as an option. In a production environment, this is likely going to be a shared location such as an S3 bucket.</p> <p>Another option could be Terraform Cloud, this is a paid-for-managed service. (Free up to 5 users)</p> <p>The pros for storing state in a remote location are that we get:</p> <ul> <li>Sensitive data encrypted</li> <li>Collaboration</li> <li>Automation</li> <li>However, it could bring increase the complexity</li> </ul> <pre><code>{\n  \"version\": 4,\n  \"terraform_version\": \"1.1.6\",\n  \"serial\": 1,\n  \"lineage\": \"a74296e7-670d-0cbb-a048-f332696ca850\",\n  \"outputs\": {\n    \"hello_world\": {\n      \"value\": \"Hello, 90DaysOfDevOps from Terraform\",\n      \"type\": \"string\"\n    }\n  },\n  \"resources\": []\n}\n</code></pre>"},{"location":"90DaysOfDevOps/day58/#resources","title":"Resources","text":"<p>I have listed a lot of resources down below and I think this topic has been covered so many times out there, If you have additional resources be sure to raise a PR with your resources and I will be happy to review and add them to the list.</p> <ul> <li>What is Infrastructure as Code? Difference of Infrastructure as Code Tools</li> <li>Terraform Tutorial | Terraform Course Overview 2021</li> <li>Terraform explained in 15 mins | Terraform Tutorial for Beginners</li> <li>Terraform Course - From BEGINNER to PRO!</li> <li>HashiCorp Terraform Associate Certification Course</li> <li>Terraform Full Course for Beginners</li> <li>KodeKloud - Terraform for DevOps Beginners + Labs: Complete Step by Step Guide!</li> <li>Terraform Simple Projects</li> <li>Terraform Tutorial - The Best Project Ideas</li> <li>Awesome Terraform</li> </ul> <p>See you on Day 59</p>"},{"location":"90DaysOfDevOps/day59/","title":"#90DaysOfDevOps - Create a VM with Terraform & Variables - Day 59","text":""},{"location":"90DaysOfDevOps/day59/#create-a-vm-with-terraform-variables","title":"Create a VM with Terraform &amp; Variables","text":"<p>In this session, we are going to be creating a VM or two VMs using terraform inside VirtualBox. This is not normal, VirtualBox is a workstation virtualisation option and this would not be a use case for Terraform but I am currently 36,000ft in the air and as much as I have deployed public cloud resources this high in the clouds it is much faster to do this locally on my laptop.</p> <p>Purely for demo purposes but the concept is the same we are going to have our desired state configuration code and then we are going to run that against the VirtualBox provider. In the past, we have used vagrant here and I covered the differences between vagrant and terraform at the beginning of the section.</p>"},{"location":"90DaysOfDevOps/day59/#create-a-virtual-machine-in-virtualbox","title":"Create a virtual machine in VirtualBox","text":"<p>The first thing we are going to do is create a new folder called VirtualBox, we can then create a VirtualBox.tf file and this is going to be where we define our resources. The code below which can be found in the VirtualBox folder as VirtualBox.tf is going to create 2 VMs in Virtualbox.</p> <p>You can find more about the community VirtualBox provider here</p> <pre><code>terraform {\n  required_providers {\n    virtualbox = {\n      source = \"terra-farm/virtualbox\"\n      version = \"0.2.2-alpha.1\"\n    }\n  }\n}\n\n# There are currently no configuration options for the provider itself.\n\nresource \"virtualbox_vm\" \"node\" {\n  count     = 2\n  name      = format(\"node-%02d\", count.index + 1)\n  image     = \"https://app.vagrantup.com/ubuntu/boxes/bionic64/versions/20180903.0.0/providers/virtualbox.box\"\n  cpus      = 2\n  memory    = \"512 mib\"\n\n  network_adapter {\n    type           = \"hostonly\"\n    host_interface = \"vboxnet1\"\n  }\n}\n\noutput \"IPAddr\" {\n  value = element(virtualbox_vm.node.*.network_adapter.0.ipv4_address, 1)\n}\n\noutput \"IPAddr_2\" {\n  value = element(virtualbox_vm.node.*.network_adapter.0.ipv4_address, 2)\n}\n</code></pre> <p>Now that we have our code defined we can now perform the <code>terraform init</code> on our folder to download the provider for Virtualbox.</p> <p></p> <p>You will also need to have VirtualBox installed on your system as well. We can then next run <code>terraform plan</code> to see what our code will create for us. Followed by <code>terraform apply</code> the below image shows your completed process.</p> <p></p> <p>In Virtualbox, you will now see your 2 virtual machines.</p> <p></p>"},{"location":"90DaysOfDevOps/day59/#change-configuration","title":"Change configuration","text":"<p>Let's add another node to our deployment. We can simply change the count line to show our new desired number of nodes. When we run our <code>terraform apply</code> it will look something like the below.</p> <p></p> <p>Once complete in VirtualBox you can see we now have 3 nodes up and running.</p> <p></p> <p>When we are finished we can clear this up using the <code>terraform destroy</code> and our machines will be removed.</p> <p></p>"},{"location":"90DaysOfDevOps/day59/#variables-outputs","title":"Variables &amp; Outputs","text":"<p>We did mention outputs when we ran our hello-world example in the last session. But we can get into more detail here.</p> <p>But there are many other variables that we can use here as well, there are also a few different ways in which we can define variables.</p> <ul> <li> <p>We can manually enter our variables with the <code>terraform plan</code> or <code>terraform apply</code> command</p> </li> <li> <p>We can define them in the .tf file within the block</p> </li> <li> <p>We can use environment variables within our system using <code>TF_VAR_NAME</code> as the format.</p> </li> <li> <p>My preference is to use a terraform.tfvars file in our project folder.</p> </li> <li> <p>There is an *auto.tfvars file option</p> </li> <li> <p>or we can define when we run the <code>terraform plan</code> or <code>terraform apply</code> with the <code>-var</code> or <code>-var-file</code>.</p> </li> </ul> <p>Starting from the bottom moving up would be the order in which the variables are defined.</p> <p>We have also mentioned that the state file will contain sensitive information. We can define our sensitive information as a variable and we can define this as being sensitive.</p> <pre><code>variable \"some resource\"  {\n    description = \"something important\"\n    type= string\n    sensitive = true\n\n}\n</code></pre>"},{"location":"90DaysOfDevOps/day59/#resources","title":"Resources","text":"<p>I have listed a lot of resources down below and I think this topic has been covered so many times out there, If you have additional resources be sure to raise a PR with your resources and I will be happy to review and add them to the list.</p> <ul> <li>What is Infrastructure as Code? Difference of Infrastructure as Code Tools</li> <li>Terraform Tutorial | Terraform Course Overview 2021</li> <li>Terraform explained in 15 mins | Terraform Tutorial for Beginners</li> <li>Terraform Course - From BEGINNER to PRO!</li> <li>HashiCorp Terraform Associate Certification Course</li> <li>Terraform Full Course for Beginners</li> <li>KodeKloud - Terraform for DevOps Beginners + Labs: Complete Step by Step Guide!</li> <li>Terraform Simple Projects</li> <li>Terraform Tutorial - The Best Project Ideas</li> <li>Awesome Terraform</li> </ul> <p>See you on Day 60</p>"},{"location":"90DaysOfDevOps/day60/","title":"#90DaysOfDevOps - Docker Containers, Provisioners & Modules - Day 60","text":""},{"location":"90DaysOfDevOps/day60/#docker-containers-provisioners-modules","title":"Docker Containers, Provisioners &amp; Modules","text":"<p>On Day 59 we provisioned a virtual machine using Terraform to our local FREE VirtualBox environment. In this section, we are going to deploy a Docker container with some configuration to our local Docker environment.</p>"},{"location":"90DaysOfDevOps/day60/#docker-demo","title":"Docker Demo","text":"<p>First up we are going to use the code block below, the outcome of the below is that we would like a simple web app to be deployed into docker and to publish this so that it is available to our network. We will be using nginx and we will make this available externally on our laptop over localhost and port 8000. We are using a docker provider from the community and you can see the docker image we are using also stated in our configuration.</p> <pre><code>terraform {\n  required_providers {\n    docker = {\n      source  = \"kreuzwerker/docker\"\n      version = \"2.16.0\"\n    }\n  }\n}\n\nprovider \"docker\" {}\n\nresource \"docker_image\" \"nginx\" {\n  name         = \"nginx:latest\"\n  keep_locally = false\n}\n\nresource \"docker_container\" \"nginx\" {\n  image = docker_image.nginx.latest\n  name  = \"tutorial\"\n  ports {\n    internal = 80\n    external = 8000\n  }\n}\n</code></pre> <p>The first task is to use <code>terraform init</code> command to download the provider to our local machine.</p> <p></p> <p>We then run our <code>terraform apply</code> followed by <code>docker ps</code> and you can see we have a running container.</p> <p></p> <p>If we then open a browser we can navigate to <code>http://localhost:8000/</code> and you will see we have access to our NGINX container.</p> <p></p> <p>You can find out more information on the Docker Provider</p> <p>The above is a very simple demo of what can be done with Terraform plus Docker and how we can now manage this under the Terraform state. We covered docker-compose in the containers section and there is a little crossover in a way between this, infrastructure as code as well as then Kubernetes.</p> <p>To show this and how Terraform can handle a little more complexity, we are going to take the docker-compose file for WordPress and MySQL that we created with docker-compose and we will put this to Terraform. You can find the docker-wordpress.tf</p> <pre><code>terraform {\n  required_providers {\n    docker = {\n      source  = \"kreuzwerker/docker\"\n      version = \"2.16.0\"\n    }\n  }\n}\n\nprovider \"docker\" {}\n\nvariable wordpress_port {\n  default = \"8080\"\n}\n\nresource \"docker_volume\" \"db_data\" {\n  name = \"db_data\"\n}\n\nresource \"docker_network\" \"wordpress_net\" {\n  name = \"wordpress_net\"\n}\n\nresource \"docker_container\" \"db\" {\n  name  = \"db\"\n  image = \"mysql:5.7\"\n  restart = \"always\"\n  network_mode = \"wordpress_net\"\n  env = [\n     \"MYSQL_ROOT_PASSWORD=wordpress\",\n     \"MYSQL_PASSWORD=wordpress\",\n     \"MYSQL_USER=wordpress\",\n     \"MYSQL_DATABASE=wordpress\"\n  ]\n  mounts {\n    type = \"volume\"\n    target = \"/var/lib/mysql\"\n    source = \"db_data\"\n    }\n}\n\nresource \"docker_container\" \"wordpress\" {\n  name  = \"wordpress\"\n  image = \"wordpress:latest\"\n  restart = \"always\"\n  network_mode = \"wordpress_net\"\n  env = [\n    \"WORDPRESS_DB_HOST=db:3306\",\n    \"WORDPRESS_DB_USER=wordpress\",\n    \"WORDPRESS_DB_NAME=wordpress\",\n    \"WORDPRESS_DB_PASSWORD=wordpress\"\n  ]\n  ports {\n    internal = \"80\"\n    external = \"${var.wordpress_port}\"\n  }\n}\n</code></pre> <p>We again put this in a new folder and then run our <code>terraform init</code> command to pull down our provisioners required.</p> <p></p> <p>We then run our <code>terraform apply</code> command and then take a look at our docker ps output we should see our newly created containers.</p> <p></p> <p>We can then also navigate to our WordPress front end. Much like when we went through this process with docker-compose in the containers section we can now run through the setup and our WordPress posts would be living in our MySQL database.</p> <p></p> <p>Now we have covered containers and Kubernetes in some detail, we probably know that this is ok for testing but if you were going to be running a website you would not do this with containers alone and you would look at using Kubernetes to achieve this, Next up we are going to take a look using Terraform with Kubernetes.</p>"},{"location":"90DaysOfDevOps/day60/#provisioners","title":"Provisioners","text":"<p>Provisioners are there so that if something cannot be declarative we have a way in which to parse this to our deployment.</p> <p>If you have no other alternative and adding this complexity to your code is the place to go then you can do this by running something similar to the following block of code.</p> <pre><code>resource \"docker_container\" \"db\" {\n  # ...\n\n  provisioner \"local-exec\" {\n    command = \"echo The server's IP address is ${self.private_ip}\"\n  }\n}\n</code></pre> <p>The remote-exec provisioner invokes a script on a remote resource after it is created. This could be used for something OS-specific or it could be used to wrap in a configuration management tool. Although notice that we have some of these covered in their provisioners.</p> <p>More details on provisioners</p> <ul> <li>file</li> <li>local-exec</li> <li>remote-exec</li> <li>vendor</li> <li>ansible</li> <li>chef</li> <li>puppet</li> </ul>"},{"location":"90DaysOfDevOps/day60/#modules","title":"Modules","text":"<p>Modules are containers for multiple resources that are used together. A module consists of a collection of .tf files in the same directory.</p> <p>Modules are a good way to separate your infrastructure resources as well as be able to pull in third-party modules that have already been created so you do not have to reinvent the wheel.</p> <p>For example, if we wanted to use the same project to build out some VMs, VPCs, Security Groups and then also a Kubernetes cluster we would likely want to split our resources out into modules to better define our resources and where they are grouped.</p> <p>Another benefit to modules is that you can take these modules and use them on other projects or share them publicly to help the community.</p> <p>We are breaking down our infrastructure into components, components are known here as modules.</p>"},{"location":"90DaysOfDevOps/day60/#resources","title":"Resources","text":"<p>I have listed a lot of resources down below and I think this topic has been covered so many times out there, If you have additional resources be sure to raise a PR with your resources and I will be happy to review and add them to the list.</p> <ul> <li>What is Infrastructure as Code? Difference of Infrastructure as Code Tools</li> <li>Terraform Tutorial | Terraform Course Overview 2021</li> <li>Terraform explained in 15 mins | Terraform Tutorial for Beginners</li> <li>Terraform Course - From BEGINNER to PRO!</li> <li>HashiCorp Terraform Associate Certification Course</li> <li>Terraform Full Course for Beginners</li> <li>KodeKloud - Terraform for DevOps Beginners + Labs: Complete Step by Step Guide!</li> <li>Terraform Simple Projects</li> <li>Terraform Tutorial - The Best Project Ideas</li> <li>Awesome Terraform</li> </ul> <p>See you on Day 61</p>"},{"location":"90DaysOfDevOps/day61/","title":"#90DaysOfDevOps - Kubernetes & Multiple Environments - Day 61","text":""},{"location":"90DaysOfDevOps/day61/#kubernetes-multiple-environments","title":"Kubernetes &amp; Multiple Environments","text":"<p>So far during this section on Infrastructure as code, we have looked at deploying virtual machines albeit to VirtualBox but the premise is the same really as we define in code what we want our virtual machine to look like and then we deploy. The same for Docker containers and in this session, we are going to take a look at how Terraform can be used to interact with resources supported by Kubernetes.</p> <p>I have been using Terraform to deploy my Kubernetes clusters for demo purposes across the 3 main cloud providers and you can find the repository tf_k8deploy</p> <p>However you can also use Terraform to interact with objects within the Kubernetes cluster, this could be using the Kubernetes provider or it could be using the Helm provider to manage your chart deployments.</p> <p>Now we could use <code>kubectl</code> as we have shown in previous sections. But there are some benefits to using Terraform in your Kubernetes environment.</p> <ul> <li> <p>Unified workflow - if you have used Terraform to deploy your clusters, you could use the same workflow and tool to deploy within your Kubernetes clusters</p> </li> <li> <p>Lifecycle management - Terraform is not just a provisioning tool, it's going to enable change, updates and deletions.</p> </li> </ul>"},{"location":"90DaysOfDevOps/day61/#simple-kubernetes-demo","title":"Simple Kubernetes Demo","text":"<p>Much like the demo we created in the last session, we can now deploy nginx into our Kubernetes cluster, I will be using minikube here again for demo purposes. We create our Kubernetes.tf file and you can find this in the folder</p> <p>In that file we are going to define our Kubernetes provider, we are going to point to our kubeconfig file, create a namespace called nginx, and then we will create a deployment which contains 2 replicas and finally service.</p> <pre><code>terraform {\n  required_providers {\n    kubernetes = {\n      source  = \"hashicorp/kubernetes\"\n      version = \"&gt;= 2.0.0\"\n    }\n  }\n}\nprovider \"kubernetes\" {\n  config_path = \"~/.kube/config\"\n}\nresource \"kubernetes_namespace\" \"test\" {\n  metadata {\n    name = \"nginx\"\n  }\n}\nresource \"kubernetes_deployment\" \"test\" {\n  metadata {\n    name      = \"nginx\"\n    namespace = kubernetes_namespace.test.metadata.0.name\n  }\n  spec {\n    replicas = 2\n    selector {\n      match_labels = {\n        app = \"MyTestApp\"\n      }\n    }\n    template {\n      metadata {\n        labels = {\n          app = \"MyTestApp\"\n        }\n      }\n      spec {\n        container {\n          image = \"nginx\"\n          name  = \"nginx-container\"\n          port {\n            container_port = 80\n          }\n        }\n      }\n    }\n  }\n}\nresource \"kubernetes_service\" \"test\" {\n  metadata {\n    name      = \"nginx\"\n    namespace = kubernetes_namespace.test.metadata.0.name\n  }\n  spec {\n    selector = {\n      app = kubernetes_deployment.test.spec.0.template.0.metadata.0.labels.app\n    }\n    type = \"NodePort\"\n    port {\n      node_port   = 30201\n      port        = 80\n      target_port = 80\n    }\n  }\n}\n</code></pre> <p>The first thing we have to do in our new project folder is run the <code>terraform init</code> command.</p> <p></p> <p>And then before we run the <code>terraform apply</code> command, let me show you that we have no namespaces.</p> <p></p> <p>When we run our apply command this is going to create those 3 new resources, namespace, deployment and service within our Kubernetes cluster.</p> <p></p> <p>We can now take a look at the deployed resources within our cluster.</p> <p></p> <p>Now because we are using minikube as you will have seen in the previous section this has its limitations when we try and play with the docker networking for ingress. But if we simply issue the <code>kubectl port-forward -n nginx svc/nginx 30201:80</code> command and open a browser to <code>http://localhost:30201/</code> we should see our NGINX page.</p> <p></p> <p>If you want to try out more detailed demos with Terraform and Kubernetes then the HashiCorp Learn site is fantastic to run through.</p>"},{"location":"90DaysOfDevOps/day61/#multiple-environments","title":"Multiple Environments","text":"<p>If we wanted to take any of the demos we have run through but wanted to now have specific production, staging and development environments looking the same and leveraging this code there are two approaches to achieve this with Terraform</p> <ul> <li> <p><code>terraform workspaces</code> - multiple named sections within a single backend</p> </li> <li> <p>file structure - Directory layout provides separation, modules provide reuse.</p> </li> </ul> <p>Each of the above does have its pros and cons though.</p>"},{"location":"90DaysOfDevOps/day61/#terraform-workspaces","title":"terraform workspaces","text":"<p>Pros</p> <ul> <li>Easy to get started</li> <li>Convenient terraform.workspace expression</li> <li>Minimises code duplication</li> </ul> <p>Cons</p> <ul> <li>Prone to human error (we were trying to eliminate this by using TF)</li> <li>State stored within the same backend</li> <li>Codebase doesn't unambiguously show deployment configurations.</li> </ul>"},{"location":"90DaysOfDevOps/day61/#file-structure","title":"File Structure","text":"<p>Pros</p> <ul> <li>Isolation of backends</li> <li>improved security</li> <li>decreased potential for human error</li> <li>Codebase fully represents deployed state</li> </ul> <p>Cons</p> <ul> <li>Multiple terraform apply required to provision environments</li> <li>More code duplication, but can be minimised with modules.</li> </ul>"},{"location":"90DaysOfDevOps/day61/#resources","title":"Resources","text":"<p>I have listed a lot of resources down below and I think this topic has been covered so many times out there, If you have additional resources be sure to raise a PR with your resources and I will be happy to review and add them to the list.</p> <ul> <li>What is Infrastructure as Code? Difference of Infrastructure as Code Tools</li> <li>Terraform Tutorial | Terraform Course Overview 2021</li> <li>Terraform explained in 15 mins | Terraform Tutorial for Beginners</li> <li>Terraform Course - From BEGINNER to PRO!</li> <li>HashiCorp Terraform Associate Certification Course</li> <li>Terraform Full Course for Beginners</li> <li>KodeKloud - Terraform for DevOps Beginners + Labs: Complete Step by Step Guide!</li> <li>Terraform Simple Projects</li> <li>Terraform Tutorial - The Best Project Ideas</li> <li>Awesome Terraform</li> </ul> <p>See you on Day 62</p>"},{"location":"90DaysOfDevOps/day62/","title":"#90DaysOfDevOps - Testing, Tools & Alternatives - Day 62","text":""},{"location":"90DaysOfDevOps/day62/#testing-tools-alternatives","title":"Testing, Tools &amp; Alternatives","text":"<p>As we close out this section on Infrastructure as Code we must mention testing our code, the various tools available and then some of the alternatives to Terraform to achieve this. As I said at the start of the section my focus was on Terraform because it is firstly free and open source, secondly, it is cross-platform and agnostic to environments. But there are also alternatives out there that should be considered but the overall goal is to make people aware that this is the way to deploy your infrastructure.</p>"},{"location":"90DaysOfDevOps/day62/#code-rot","title":"Code Rot","text":"<p>The first area I want to cover in this session is code rot, unlike application code, infrastructure as code might get used and then not for a very long time. Let's take the example that we are going to be using Terraform to deploy our VM environment in AWS, perfect and it works the first time and we have our environment, but this environment doesn't change too often so the code gets left the state possibly or hopefully stored in a central location but the code does not change.</p> <p>What if something changes in the infrastructure? But it is done out of band, or other things change in our environment.</p> <ul> <li>Out of band changes</li> <li>Unpinned versions</li> <li>Deprecated dependencies</li> <li>Unapplied changes</li> </ul>"},{"location":"90DaysOfDevOps/day62/#testing","title":"Testing","text":"<p>Another huge area that follows on from code rot and in general is the ability to test your IaC and make sure all areas are working the way they should.</p> <p>First up there are some built-in testing commands we can take a look at:</p> Command Description <code>terraform fmt</code> Rewrite Terraform configuration files to a canonical format and style. <code>terraform validate</code> Validates the configuration files in a directory, referring only to the configuration <code>terraform plan</code> Creates an execution plan, which lets you preview the changes that Terraform plans to make Custom validation Validation of your input variables to ensure they match what you would expect them to be <p>We also have some testing tools available external to Terraform:</p> <ul> <li> <p>tflint</p> </li> <li> <p>Find possible errors</p> </li> <li>Warn about deprecated syntax and unused declarations.</li> <li>Enforce best practices, and naming conventions.</li> </ul> <p>Scanning tools</p> <ul> <li>checkov - scans cloud infrastructure configurations to find misconfigurations before they're deployed.</li> <li>tfsec - static analysis security scanner for your Terraform code.</li> <li>terrascan - static code analyser for Infrastructure as Code.</li> <li>terraform-compliance - a lightweight, security and compliance-focused test framework against terraform to enable the negative testing capability for your infrastructure-as-code.</li> <li>snyk - scans your Terraform code for misconfigurations and security issues</li> </ul> <p>Managed Cloud offering</p> <ul> <li>Terraform Sentinel - embedded policy-as-code framework integrated with the HashiCorp Enterprise products. It enables fine-grained, logic-based policy decisions, and can be extended to use information from external sources.</li> </ul>"},{"location":"90DaysOfDevOps/day62/#automated-testing","title":"Automated testing","text":"<ul> <li>Terratest - Terratest is a Go library that provides patterns and helper functions for testing infrastructure</li> <li>Terratest makes it easier to write automated tests for our infrastructure code. It provides a variety of helper functions and patterns for common infrastructure testing.</li> <li>find code at 2022/Days/IaC/Terratest</li> <li>To Run this application</li> <li>git clone #repo_url# </li> <li>cd test  </li> <li>go mod init \"\"   MODULE_NAME would be github.com// <li>go mod init github.com/ <li>go run</li> <p>go mod init \"\" would create go.mod file into test folder.   * The go.mod file is the root of dependency management in GoLang.  * All the modules which are needed or to be used in the project are maintained here in go.mod file. * It creates entry for all the packages we are going to use/import in our project. * It reduces effort for getting each dependencies manually. <p>On running go test for the first time you would get go.sum file created.  * go.sum file is created when go test or go build is executed for the first time. * It installs all the packages with specific version(latest) * we do not need to edit or modify this file.</p> <p>Worth a mention</p> <ul> <li> <p>Terraform Cloud - Terraform Cloud is HashiCorp\u2019s managed service offering. It eliminates the need for unnecessary tooling and documentation for practitioners, teams, and organizations to use Terraform in production.</p> </li> <li> <p>Terragrunt - Terragrunt is a thin wrapper that provides extra tools for keeping your configurations DRY, working with multiple Terraform modules, and managing remote state.</p> </li> <li> <p>Atlantis - Terraform Pull Request Automation</p> </li> </ul>"},{"location":"90DaysOfDevOps/day62/#alternatives","title":"Alternatives","text":"<p>We mentioned on Day 57 when we started this section that there were some alternatives and I very much plan on exploring this following on from this challenge.</p> Cloud Specific Cloud Agnostic AWS CloudFormation Terraform Azure Resource Manager Pulumi Google Cloud Deployment Manager <p>I have used AWS CloudFormation probably the most out of the above list and native to AWS but I have not used the others other than Terraform. As you can imagine the cloud-specific versions are very good in that particular cloud but if you have multiple cloud environments then you are going to struggle to migrate those configurations or you are going to have multiple management planes for your IaC efforts.</p> <p>I think an interesting next step for me is to take some time and learn more about Pulumi</p> <p>From a Pulumi comparison on their site</p> <p>\"Both Terraform and Pulumi offer the desired state infrastructure as code model where the code represents the desired infrastructure state and the deployment engine compares this desired state with the stack\u2019s current state and determines what resources need to be created, updated or deleted.\"</p> <p>The biggest difference I can see is that unlike the HashiCorp Configuration Language (HCL) Pulumi allows for general-purpose languages like Python, TypeScript, JavaScript, Go and .NET.</p> <p>A quick overview Introduction to Pulumi: Modern Infrastructure as Code I like the ease and choices you are prompted with and want to get into this a little more.</p> <p>This wraps up the Infrastructure as code section and next we move on to that little bit of overlap with configuration management in particular as we get past the big picture of configuration management we are going to be using Ansible for some of those tasks and demos.</p>"},{"location":"90DaysOfDevOps/day62/#resources","title":"Resources","text":"<p>I have listed a lot of resources down below and I think this topic has been covered so many times out there, If you have additional resources be sure to raise a PR with your resources and I will be happy to review and add them to the list.</p> <ul> <li>What is Infrastructure as Code? Difference of Infrastructure as Code Tools</li> <li>Terraform Tutorial | Terraform Course Overview 2021</li> <li>Terraform explained in 15 mins | Terraform Tutorial for Beginners</li> <li>Terraform Course - From BEGINNER to PRO!</li> <li>HashiCorp Terraform Associate Certification Course</li> <li>Terraform Full Course for Beginners</li> <li>KodeKloud - Terraform for DevOps Beginners + Labs: Complete Step by Step Guide!</li> <li>Terraform Simple Projects</li> <li>Terraform Tutorial - The Best Project Ideas</li> <li>Awesome Terraform</li> <li>Pulumi - IaC in your favorite programming language!</li> </ul> <p>See you on Day 63</p>"},{"location":"90DaysOfDevOps/day63/","title":"#90DaysOfDevOps - The Big Picture: Configuration Management - Day 63","text":""},{"location":"90DaysOfDevOps/day63/#the-big-picture-configuration-management","title":"The Big Picture: Configuration Management","text":"<p>Coming straight off the back of the section covering Infrastructure as Code, there is likely going to be some crossover as we talk about Configuration Management or Application Configuration Management.</p> <p>Configuration Management is the process of maintaining applications, systems and servers in the desired state. The overlap with Infrastructure as code is that IaC is going to make sure your infrastructure is at the desired state but after that especially terraform is not going to look after the desired state of your OS settings or Application and that is where Configuration Management tools come in. Make sure that the system and applications perform the way it is expected as changes occur over Deane.</p> <p>Configuration management keeps you from making small or large changes that go undocumented.</p>"},{"location":"90DaysOfDevOps/day63/#scenario-why-would-you-want-to-use-configuration-management","title":"Scenario: Why would you want to use Configuration Management","text":"<p>The scenario or why you'd want to use Configuration Management, meet Dean he's our system administrator and Dean is a happy camper pretty and working on all of the systems in his environment.</p> <p>What happens if their system fails, if there's a fire, a server goes down well? Dean knows exactly what to do he can fix that fire easily the problems become difficult for Dean however if multiple servers start failing particularly when you have large and expanding environments, this is why Dean needs to have a configuration management tool. Configuration Management tools can help make Dean look like a rockstar, all he has to do is configure the right codes that allow him to push out the instructions on how to set up each of the servers quickly effectively and at scale.</p>"},{"location":"90DaysOfDevOps/day63/#configuration-management-tools","title":"Configuration Management tools","text":"<p>There are a variety of configuration management tools available, and each has specific features that make it better for some situations than others.</p> <p></p> <p>At this stage, we will take a quickfire look at the options in the above picture before making our choice on which one we will use and why.</p> <ul> <li> <p>Chef</p> </li> <li> <p>Chef ensures configuration is applied consistently in every environment, at any scale with infrastructure automation.</p> </li> <li>Chef is an open-source tool developed by OpsCode written in Ruby and Erlang.</li> <li>Chef is best suited for organisations that have a heterogeneous infrastructure and are looking for mature solutions.</li> <li>Recipes and Cookbooks determine the configuration code for your systems.</li> <li>Pro - A large collection of recipes is available</li> <li>Pro - Integrates well with Git which provides a strong version control</li> <li>Con - Steep learning curve, a considerable amount of time required.</li> <li>Con - The main server doesn't have much control.</li> <li>Architecture - Server / Clients</li> <li>Ease of setup - Moderate</li> <li> <p>Language - Procedural - Specify how to do a task</p> </li> <li> <p>Puppet</p> </li> <li>Puppet is a configuration management tool that supports automatic deployment.</li> <li>Puppet is built in Ruby and uses DSL for writing manifests.</li> <li>Puppet also works well with heterogeneous infrastructure where the focus is on scalability.</li> <li>Pro - Large community for support.</li> <li>Pro - Well-developed reporting mechanism.</li> <li>Con - Advance tasks require knowledge of the Ruby language.</li> <li>Con - The main server doesn't have much control.</li> <li>Architecture - Server / Clients</li> <li>Ease of setup - Moderate</li> <li>Language - Declarative - Specify only what to do</li> <li> <p>Ansible</p> </li> <li> <p>Ansible is an IT automation tool that automates configuration management, cloud provisioning, deployment and orchestration.</p> </li> <li>The core of Ansible playbooks is written in YAML. (Should do a section on YAML as we have seen this a few times)</li> <li>Ansible works well when there are environments that focus on getting things up and running fast.</li> <li>Works on playbooks which provide instructions to your servers.</li> <li>Pro - No agents are needed on remote nodes.</li> <li>Pro - YAML is easy to learn.</li> <li>Con - Performance speed is often less than other tools (Faster than Dean doing it himself manually)</li> <li>Con - YAML is not as powerful as Ruby but has less of a learning curve.</li> <li>Architecture - Client Only</li> <li>Ease of setup - Very Easy</li> <li> <p>Language - Procedural - Specify how to do a task</p> </li> <li> <p>SaltStack</p> </li> <li>SaltStack is a CLI-based tool that automates configuration management and remote execution.</li> <li>SaltStack is Python based whilst the instructions are written in YAML or its DSL.</li> <li>Perfect for environments with scalability and resilience as the priority.</li> <li>Pro - Easy to use when up and running</li> <li>Pro - Good reporting mechanism</li> <li>Con - The setup phase is tough</li> <li>Con - New web UI which is much less developed than the others.</li> <li>Architecture - Server / Clients</li> <li>Ease of setup - Moderate</li> <li>Language - Declarative - Specify only what to do</li> </ul>"},{"location":"90DaysOfDevOps/day63/#ansible-vs-terraform","title":"Ansible vs Terraform","text":"<p>The tool that we will be using for this section is going to be Ansible. (Easy to use and easier language basics required.)</p> <p>I think it is important to touch on some of the differences between Ansible and Terraform before we look into the tooling a little further.</p> Ansible Terraform Type Ansible is a configuration management tool Terraform is an orchestration tool Infrastructure Ansible provides support for mutable infrastructure Terraform provides support for immutable infrastructure Language Ansible follows procedural language Terraform follows a declarative language Provisioning Ansible provides partial provisioning (VM, Network, Storage) Terraform provides extensive provisioning (VM, Network, Storage) Packaging Ansible provides complete support for packaging &amp; templating Terraform provides partial support for packaging &amp; templating Lifecycle Mgmt Ansible does not have lifecycle management Terraform is heavily dependent on lifecycle and state management"},{"location":"90DaysOfDevOps/day63/#resources","title":"Resources","text":"<ul> <li>What is Ansible</li> <li>Ansible 101 - Episode 1 - Introduction to Ansible</li> <li>NetworkChuck - You need to learn Ansible right now!</li> </ul> <p>See you on Day 64</p>"},{"location":"90DaysOfDevOps/day64/","title":"#90DaysOfDevOps - Ansible: Getting Started - Day 64","text":""},{"location":"90DaysOfDevOps/day64/#ansible-getting-started","title":"Ansible: Getting Started","text":"<p>We covered a little about what Ansible is in the big picture session yesterday But we are going to get started with a little more information on top of that here. Firstly Ansible comes from RedHat. Secondly, it is agentless, connects via SSH and runs commands. Thirdly it is cross-platform (Linux &amp; macOS, WSL2) and open-source (there is also a paid-for enterprise option) Ansible pushes configuration vs other models.</p>"},{"location":"90DaysOfDevOps/day64/#ansible-installation","title":"Ansible Installation","text":"<p>As you might imagine, RedHat and the Ansible team have done a fantastic job of documenting Ansible. This generally starts with the installation steps which you can find here Remember we said that Ansible is an agentless automation tool, the tool is deployed to a system referred to as a \"Control Node\" from this control node is manages machines and other devices (possibly network) over SSH.</p> <p>It does state in the above-linked documentation that the Windows OS cannot be used as the control node.</p> <p>For my control node and at least this demo, I am going to use the Linux VM we created way back in the Linux section as my control node.</p> <p>This system was running Ubuntu and the installation steps simply need the following commands.</p> <pre><code>sudo apt update\nsudo apt install software-properties-common\nsudo add-apt-repository --yes --update ppa:ansible/ansible\nsudo apt install ansible\n</code></pre> <p>Now we should have ansible installed on our control node, you can check this by running <code>ansible --version</code> and you should see something similar to this below.</p> <p></p> <p>Before we then start to look at controlling other nodes in our environment, we can also check the functionality of ansible by running a command against our local machine <code>ansible localhost -m ping</code> will use an Ansible Module and this is a quick way to perform a single task across many different systems. I mean it is not much fun with just the local host but imagines you wanted to get something or make sure all your systems were up and you had 1000+ servers and devices.</p> <p></p> <p>Or an actual real-life use for a module might be something like <code>ansible webservers -m service -a \"name=httpd state=started\"</code> this will tell us if all of our webservers have the httpd service running. I have glossed over the webservers term used in that command.</p>"},{"location":"90DaysOfDevOps/day64/#hosts","title":"hosts","text":"<p>The way I used localhost above to run a simple ping module against the system, I cannot specify another machine on my network, for example in the environment I am using my Windows host where VirtualBox is running has a network adapter with the IP 10.0.0.1 but you can see below that I can reach by pinging but I cannot use ansible to perform that task.</p> <p></p> <p>For us to specify our hosts or the nodes that we want to automate with these tasks, we need to define them. We can define them by navigating to the /etc/ansible directory on your system.</p> <p></p> <p>The file we want to edit is the host's file, using a text editor we can jump in and define our hosts. The host file contains lots of great instructions on how to use and modify the file. We want to scroll down to the bottom and we are going to create a new group called [windows] and we are going to add our <code>10.0.0.1</code> IP address for that host. Save the file.</p> <p></p> <p>However, remember I said you will need to have SSH available to enable Ansible to connect to your system. As you can see below when I run <code>ansible windows -m ping</code> we get an unreachable because things failed to connect via SSH.</p> <p></p> <p>I have now also started adding some additional hosts to our inventory, another name for this file as this is where you are going to define all of your devices, could be network devices, switches and routers for example also would be added here and grouped. In our hosts file though I have also added my credentials for accessing the Linux group of systems.</p> <p></p> <p>Now if we run <code>ansible Linux -m ping</code> we get success as per below.</p> <p></p> <p>We then have the node requirements, these are the target systems you wish to automate the configuration on. We are not installing anything for Ansible on these (I mean we might be installing software but there is no client from Ansible we need) Ansible will make a connection over SSH and send anything over SFTP. (If you so desire though and you have SSH configured you could use SCP vs SFTP.)</p>"},{"location":"90DaysOfDevOps/day64/#ansible-commands","title":"Ansible Commands","text":"<p>You saw that we were able to run <code>ansible Linux -m ping</code> against our Linux machine and get a response, basically, with Ansible we can run many ad-hoc commands. But you can run this against a group of systems and get that information back. ad hoc commands</p> <p>If you find yourself repeating commands or even worse you are having to log into individual systems to run these commands then Ansible can help there. For example, the simple command below would give us the output of all the operating system details for all of the systems we add to our Linux group. <code>ansible linux -a \"cat /etc/os-release\"</code></p> <p>Other use cases could be to reboot systems, copy files, and manage packers and users. You can also couple ad hoc commands with Ansible modules.</p> <p>Ad hoc commands use a declarative model, calculating and executing the actions required to reach a specified final state. They achieve a form of idempotence by checking the current state before they begin and doing nothing unless the current state is different from the specified final state.</p>"},{"location":"90DaysOfDevOps/day64/#resources","title":"Resources","text":"<ul> <li>What is Ansible</li> <li>Ansible 101 - Episode 1 - Introduction to Ansible</li> <li>NetworkChuck - You need to learn Ansible right now!</li> </ul> <p>See you on Day 65</p>"},{"location":"90DaysOfDevOps/day65/","title":"#90DaysOfDevOps - Ansible Playbooks - Day 65","text":""},{"location":"90DaysOfDevOps/day65/#ansible-playbooks","title":"Ansible Playbooks","text":"<p>In this section we will take a look at the main reason that I can see at least for Ansible, I mean it is great to take a single command and hit many different servers to perform simple commands such as rebooting a long list of servers and saving the hassle of having to connect to each one individually.</p> <p>But what about actually taking a bare operating system and declaring the software and services we want running on that system and making sure they are all running in that desired state.</p> <p>This is where ansible playbooks come in. A playbook enables us to take our group of servers and perform configuration and installation tasks against that group.</p>"},{"location":"90DaysOfDevOps/day65/#playbook-format","title":"Playbook format","text":"<p>Playbook &gt; Plays &gt; Tasks</p> <p>For anyone that comes from a sports background you may have come across the term playbook, a playbook then tells the team how you will play made up of various plays and tasks if we think of the plays as the set pieces within the sport or game, and the tasks are associated to each play, you can have multiple tasks to make up a play and in the playbook, you may have multiple different plays.</p> <p>These playbooks are written in YAML (YAML ain\u2019t markup language) you will find a lot of the sections we have covered so far especially Containers and Kubernetes to feature YAML formatted configuration files.</p> <p>Let\u2019s take a look at a simple playbook called playbook.yml.</p> <pre><code>- name: Simple Play\n  hosts: localhost\n  connection: local\n  tasks:\n    - name: Ping me\n      ping:\n    - name: print os\n      debug:\n        msg: \"{{ ansible_os_family }}\"\n</code></pre> <p>You will find the above file simple_play. If we then use the <code>ansible-playbook simple_play.yml</code> command we will walk through the following steps.</p> <p></p> <p>You can see the first task of \"gathering steps\" happened, but we didn't trigger or ask for this? This module is automatically called by playbooks to gather useful variables about remote hosts. ansible.builtin.setup</p> <p>Our second task was to set a ping, this is not an ICMP ping but a python script to report back <code>pong</code> on successful connectivity to remote or localhost. ansible.builtin.ping</p> <p>Then our third or our second defined task as the first one will run unless you disable was the printing of a message telling us our OS. In this task we are using conditionals, we could run this playbook against all different types of operating systems and this would return the OS name. We are simply messaging this output for ease but we could add a task to say something like:</p> <pre><code>tasks:\n  - name: \"shut down Debian flavoured systems\"\n    command: /sbin/shutdown -t now\n    when: ansible_os_family == \"Debian\"\n</code></pre>"},{"location":"90DaysOfDevOps/day65/#vagrant-to-set-up-our-environment","title":"Vagrant to set up our environment","text":"<p>We are going to use Vagrant to set up our node environment, I am going to keep this at a reasonable 4 nodes but you can hopefully see that this could easily be 300 or 3000 and this is the power of Ansible and other configuration management tools to be able to configure your servers.</p> <p>You can find this file located here (Vagrantfile)</p> <pre><code>Vagrant.configure(\"2\") do |config|\n  servers=[\n    {\n      :hostname =&gt; \"db01\",\n      :box =&gt; \"bento/ubuntu-21.10\",\n      :ip =&gt; \"192.168.169.130\",\n      :ssh_port =&gt; '2210'\n    },\n    {\n      :hostname =&gt; \"web01\",\n      :box =&gt; \"bento/ubuntu-21.10\",\n      :ip =&gt; \"192.168.169.131\",\n      :ssh_port =&gt; '2211'\n    },\n    {\n      :hostname =&gt; \"web02\",\n      :box =&gt; \"bento/ubuntu-21.10\",\n      :ip =&gt; \"192.168.169.132\",\n      :ssh_port =&gt; '2212'\n    },\n    {\n      :hostname =&gt; \"loadbalancer\",\n      :box =&gt; \"bento/ubuntu-21.10\",\n      :ip =&gt; \"192.168.169.134\",\n      :ssh_port =&gt; '2213'\n    }\n\n  ]\n\nconfig.vm.base_address = 600\n\n  servers.each do |machine|\n\n    config.vm.define machine[:hostname] do |node|\n      node.vm.box = machine[:box]\n      node.vm.hostname = machine[:hostname]\n\n      node.vm.network :public_network, bridge: \"Intel(R) Ethernet Connection (7) I219-V\", ip: machine[:ip]\n      node.vm.network \"forwarded_port\", guest: 22, host: machine[:ssh_port], id: \"ssh\"\n\n      node.vm.provider :virtualbox do |v|\n        v.customize [\"modifyvm\", :id, \"--memory\", 2048]\n        v.customize [\"modifyvm\", :id, \"--name\", machine[:hostname]]\n      end\n    end\n  end\n\nend\n</code></pre> <p>Use the <code>vagrant up</code> command to spin these machines up in VirtualBox, You might be able to add more memory and you might also want to define a different private_network address for each machine but this works in my environment. Remember our control box is the Ubuntu desktop we deployed during the Linux section.</p> <p>If you are resource contrained then you can also run <code>vagrant up web01 web02</code> to only bring up the webservers that we are using here.</p>"},{"location":"90DaysOfDevOps/day65/#ansible-host-configuration","title":"Ansible host configuration","text":"<p>Now that we have our environment ready, we can check ansible and for this, we will use our Ubuntu desktop (You could use this but you can equally use any Linux-based machine on your network access to the network below) as our control, let\u2019s also add the new nodes to our group in the ansible hosts file, you can think of this file as an inventory, an alternative to this could be another inventory file that is called on as part of your ansible command with <code>-i filename</code> this could be useful vs using the host file as you can have different files for different environments, maybe production, test and staging. Because we are using the default hosts file we do not need to specify as this would be the default used.</p> <p>I have added the following to the default hosts file.</p> <pre><code>[control]\nansible-control\n\n[proxy]\nloadbalancer\n\n[webservers]\nweb01\nweb02\n\n[database]\ndb01\n</code></pre> <p></p> <p>Before moving on we want to make sure we can run a command against our nodes, let\u2019s run <code>ansible nodes -m command -a hostname</code> this simple command will test that we have connectivity and report back our host names.</p> <p>Also, note that I have added these nodes and IPs to my Ubuntu control node within the /etc/hosts file to ensure connectivity. We might also need to do an SSH configuration for each node from the Ubuntu box.</p> <pre><code>192.168.169.140 ansible-control\n192.168.169.130 db01\n192.168.169.131 web01\n192.168.169.132 web02\n192.168.169.133 loadbalancer\n</code></pre> <p></p> <p>At this stage, we want to run through setting up SSH keys between your control and your server nodes. This is what we are going to do next, another way here could be to add variables into your host's file to give username and password. I would advise against this as this is never going to be a best practice.</p> <p>To set up SSH and share amongst your nodes, follow the steps below, you will be prompted for passwords (<code>vagrant</code>) and you will likely need to hit <code>y</code> a few times to accept.</p> <p><code>ssh-keygen</code></p> <p></p> <p><code>ssh-copy-id localhost</code></p> <p></p> <p>Now if you have all of your VMs switched on then you can run the <code>ssh-copy-id web01 &amp;&amp; ssh-copy-id web02 &amp;&amp; ssh-copy-id loadbalancer &amp;&amp; ssh-copy-id db01</code> this will prompt you for your password in our case our password is <code>vagrant</code></p> <p>I am not running all my VMs and only running the webservers so I issued <code>ssh-copy-id web01 &amp;&amp; ssh-copy-id web02</code></p> <p></p> <p>Before running any playbooks I like to make sure that I have simple connectivity with my groups so I have run <code>ansible webservers -m ping</code> to test connectivity.</p> <p></p>"},{"location":"90DaysOfDevOps/day65/#our-first-real-ansible-playbook","title":"Our First \"real\" Ansible Playbook","text":"<p>Our first Ansible playbook is going to configure our web servers, we have grouped these in our host's file under the grouping [webservers].</p> <p>Before we run our playbook we can confirm that our web01 and web02 do not have apache installed. The top of the screenshot below is showing you the folder and file layout I have created within my ansible control to run this playbook, we have the <code>playbook1.yml</code>, then in the templates folder we have the <code>index.html.j2</code> and <code>ports.conf.j2</code> files. You can find these files in the folder listed above in the repository.</p> <p>Then we SSH into web01 to check if we have apache installed?</p> <p></p> <p>You can see from the above that we have not got apache installed on our web01 so we can fix this by running the below playbook.</p> <pre><code>- hosts: webservers\n  become: yes\n  vars:\n    http_port: 8000\n    https_port: 4443\n    html_welcome_msg: \"Hello 90DaysOfDevOps\"\n  tasks:\n  - name: ensure apache is at the latest version\n    apt:\n      name: apache2\n      state: latest\n\n  - name: write the apache2 ports.conf config file\n    template:\n      src: templates/ports.conf.j2\n      dest: /etc/apache2/ports.conf\n    notify:\n    - restart apache\n\n  - name: write a basic index.html file\n    template:\n      src: templates/index.html.j2\n      dest: /var/www/html/index.html\n    notify:\n    - restart apache\n\n  - name: ensure apache is running\n    service:\n      name: apache2\n      state: started\n\n  handlers:\n    - name: restart apache\n      service:\n        name: apache2\n        state: restarted\n</code></pre> <p>Breaking down the above playbook:</p> <ul> <li><code>- hosts: webservers</code> this is saying that our group to run this playbook on is a group called webservers</li> <li><code>become: yes</code> means that our user running the playbook will become root on our remote systems. You will be prompted for the root password.</li> <li>We then have <code>vars</code> and this defines some environment variables we want throughout our webservers.</li> </ul> <p>Following this, we start our tasks,</p> <ul> <li>Task 1 is to ensure that apache is running the latest version</li> <li>Task 2 is writing the ports.conf file from our source found in the templates folder.</li> <li>Task 3 is creating a basic index.html file</li> <li>Task 4 is making sure apache is running</li> </ul> <p>Finally, we have a handlers section, Handlers: Running operations on change</p> <p>\"Sometimes you want a task to run only when a change is made on a machine. For example, you may want to restart a service if a task updates the configuration of that service, but not if the configuration is unchanged. Ansible uses handlers to address this use case. Handlers are tasks that only run when notified. Each handler should have a globally unique name.\"</p> <p>At this stage, you might be thinking that we have deployed 5 VMs (including our Ubuntu Desktop machine which is acting as our Ansible Control) The other systems will come into play during the rest of the section.</p>"},{"location":"90DaysOfDevOps/day65/#run-our-playbook","title":"Run our Playbook","text":"<p>We are now ready to run our playbook against our nodes. To run our playbook we can use the <code>ansible-playbook playbook1.yml</code> We have defined the hosts that our playbook will run against within the playbook and this will walk through the tasks that we have defined.</p> <p>When the command is complete we get an output showing our plays and tasks, this may take some time you can see from the below image that this took a while to go and install our desired state.</p> <p></p> <p>We can then double-check this by jumping into a node and checking we have the installed software on our node.</p> <p></p> <p>Just to round this out as we have deployed two standalone webservers with the above we can now navigate to the respective IPs that we defined and get our new website.</p> <p></p> <p>We are going to build on this playbook as we move through the rest of this section. I am interested as well in taking our Ubuntu desktop and seeing if we could bootstrap our applications and configuration using Ansible so we might also touch this. You saw that we can use the local host in our commands we can also run playbooks against our local host for example.</p> <p>Another thing to add here is that we are only really working with Ubuntu VMs but Ansible is agnostic to the target systems. The alternatives that we have previously mentioned to manage your systems could be server by server (not scalable when you get over a large number of servers, plus a pain even with 3 nodes) we can also use shell scripting which again we covered in the Linux section but these nodes are potentially different so yes it can be done but then someone needs to maintain and manage those scripts. Ansible is free and hits the easy button vs having to have a specialised script.</p>"},{"location":"90DaysOfDevOps/day65/#resources","title":"Resources","text":"<ul> <li>What is Ansible</li> <li>Ansible 101 - Episode 1 - Introduction to Ansible</li> <li>NetworkChuck - You need to learn Ansible right now!</li> <li>Your complete guide to Ansible</li> </ul> <p>This final playlist listed above is where a lot of the code and ideas came from for this section, a great resource and walkthrough in video format.</p> <p>See you on Day 66</p>"},{"location":"90DaysOfDevOps/day66/","title":"#90DaysOfDevOps - Ansible Playbooks Continued... - Day 66","text":""},{"location":"90DaysOfDevOps/day66/#ansible-playbooks-continued","title":"Ansible Playbooks (Continued)","text":"<p>In our last section, we started with creating our small lab using a Vagrantfile to deploy 4 machines and we used the Linux machine we created in that section as our ansible control system.</p> <p>We also ran through a few scenarios of playbooks and at the end we had a playbook that made our web01 and web02 individual web servers.</p> <p></p>"},{"location":"90DaysOfDevOps/day66/#keeping-things-tidy","title":"Keeping things tidy","text":"<p>Before we get into further automation and deployment we should cover the ability to keep our playbook lean and tidy and how we can separate our tasks and handlers into subfolders.</p> <p>we are going to copy our tasks into their file within a folder.</p> <pre><code>- name: ensure apache is at the latest version\n  apt: name=apache2 state=latest\n\n- name: write the apache2 ports.conf config file\n  template:\n    src=templates/ports.conf.j2\n    dest=/etc/apache2/ports.conf\n  notify: restart apache\n\n- name: write a basic index.html file\n  template:\n    src: templates/index.html.j2\n    dest: /var/www/html/index.html\n  notify:\n  - restart apache\n\n- name: ensure apache is running\n  service:\n    name: apache2\n    state: started\n</code></pre> <p>and the same for the handlers.</p> <pre><code>- name: restart apache\n  service:\n    name: apache2\n    state: restarted\n</code></pre> <p>then within our playbook now named <code>playbook2.yml</code> we point to these files. All of which can be found at ansible-scenario2</p> <p>You can test this on your control machine. If you have copied the files from the repository you should have noticed something changed in the \"write a basic index.html file\"</p> <p></p> <p>Let's find out what simple change I made. Using <code>curl web01:8000</code></p> <p></p> <p>We have just tidied up our playbook and started to separate areas that could make a playbook very overwhelming at scale.</p>"},{"location":"90DaysOfDevOps/day66/#roles-and-ansible-galaxy","title":"Roles and Ansible Galaxy","text":"<p>At the moment we have deployed 4 VMs and we have configured 2 of these VMs as our webservers but we have some more specific functions namely, a database server and a loadbalancer or proxy. For us to do this and tidy up our repository, we can use roles within Ansible.</p> <p>To do this we will use the <code>ansible-galaxy</code> command which is there to manage ansible roles in shared repositories.</p> <p></p> <p>We are going to use <code>ansible-galaxy</code> to create a role for apache2 which is where we are going to put our specifics for our webservers.</p> <p></p> <p>The above command <code>ansible-galaxy init roles/apache2</code> will create the folder structure that we have shown above. Our next step is we need to move our existing tasks and templates to the relevant folders in the new structure.</p> <p></p> <p>Copy and paste are easy to move those files but we also need to make a change to the tasks/main.yml so that we point this to the apache2_install.yml.</p> <p>We also need to change our playbook now to refer to our new role. In the playbook1.yml and playbook2.yml we determine our tasks and handlers in different ways as we changed these between the two versions. We need to change our playbook to use this role as per below:</p> <pre><code>- hosts: webservers\n  become: yes\n  vars:\n    http_port: 8000\n    https_port: 4443\n    html_welcome_msg: \"Hello 90DaysOfDevOps - Welcome to Day 66!\"\n  roles:\n    - apache2\n</code></pre> <p></p> <p>We can now run our playbook again this time with the new playbook name <code>ansible-playbook playbook3.yml</code> you will notice the depreciation, we can fix that next.</p> <p></p> <p>Ok, the depreciation although our playbook ran we should fix our ways now, to do that I have changed the include option in the tasks/main.yml to now be import_tasks as per below.</p> <p></p> <p>You can find these files in the ansible-scenario3</p> <p>We are also going to create a few more roles whilst using <code>ansible-galaxy</code> we are going to create:</p> <ul> <li>common = for all of our servers (<code>ansible-galaxy init roles/common</code>)</li> <li>nginx = for our loadbalancer (<code>ansible-galaxy init roles/nginx</code>)</li> </ul> <p></p> <p>I am going to leave this one here and in the next session, we will start working on those other nodes we have deployed but have not done anything with yet.</p>"},{"location":"90DaysOfDevOps/day66/#resources","title":"Resources","text":"<ul> <li>What is Ansible</li> <li>Ansible 101 - Episode 1 - Introduction to Ansible</li> <li>NetworkChuck - You need to learn Ansible right now!</li> <li>Your complete guide to Ansible</li> </ul> <p>This final playlist listed above is where a lot of the code and ideas came from for this section, a great resource and walkthrough in video format.</p> <p>See you on Day 67</p>"},{"location":"90DaysOfDevOps/day67/","title":"#90DaysOfDevOps - Using Roles & Deploying a Loadbalancer - Day 67","text":""},{"location":"90DaysOfDevOps/day67/#using-roles-deploying-a-loadbalancer","title":"Using Roles &amp; Deploying a Loadbalancer","text":"<p>In the last session, we covered roles and used the <code>ansible-galaxy</code> command to help create our folder structures for some roles that we are going to use. We finished up with a much tidier working repository for our configuration code as everything is hidden away in our role folders.</p> <p>However, we have only used the apache2 role and have a working playbook3.yaml to handle our webservers.</p> <p>At this point if you have only used <code>vagrant up web01 web02</code> now is the time to run <code>vagrant up loadbalancer</code> this will bring up another Ubuntu system that we will use as our Load Balancer/Proxy.</p> <p>We have already defined this new machine in our host's file, but we do not have the ssh key configured until it is available, so we need to also run <code>ssh-copy-id loadbalancer</code> when the system is up and ready.</p>"},{"location":"90DaysOfDevOps/day67/#common-role","title":"Common role","text":"<p>I created at the end of yesterday's session the role of <code>common</code>, common will be used across all of our servers whereas the other roles are specific to use cases, now the applications I am going to install are as common as spurious and I cannot see many reasons for this to be the case but it shows the objective. In our common role folder structure, navigate to the tasks folder and you will have a main.yml. In this YAML, we need to point this to our install_tools.yml file and we do this by adding a line <code>- import_tasks: install_tools.yml</code> this used to be <code>include</code> but this is going to be depreciated soon enough so we are using import_tasks.</p> <pre><code>- name: \"Install Common packages\"\n  apt: name={{ item }} state=latest\n  with_items:\n   - neofetch\n   - tree\n   - figlet\n</code></pre> <p>In our playbook, we then add in the common role for each host block.</p> <pre><code>- hosts: webservers\n  become: yes\n  vars:\n    http_port: 8000\n    https_port: 4443\n    html_welcome_msg: \"Hello 90DaysOfDevOps - Welcome to Day 66!\"\n  roles:\n    - common\n    - apache2\n</code></pre>"},{"location":"90DaysOfDevOps/day67/#nginx","title":"nginx","text":"<p>The next phase is for us to install and configure nginx on our loadbalancer VM. Like the common folder structure, we have the nginx based on the last session.</p> <p>First of all, we are going to add a host block to our playbook. This block will include our common role and then our new nginx role.</p> <p>The playbook can be found here. playbook4.yml</p> <pre><code>- hosts: webservers\n  become: yes\n  vars:\n    http_port: 8000\n    https_port: 4443\n    html_welcome_msg: \"Hello 90DaysOfDevOps - Welcome to Day 66!\"\n  roles:\n    - common\n    - apache2\n\n- hosts: proxy\n  become: yes\n  roles:\n    - common\n    - nginx\n</code></pre> <p>For this to mean anything, we have to define the tasks that we wish to run, in the same way, we will modify the main.yml in tasks to point to two files this time, one for installation and one for configuration.</p> <p>There are some other files that I have modified based on the outcome we desire, take a look in the folder ansible-scenario4 for all the files changed. You should check the folders tasks, handlers and templates in the nginx folder and you will find those additional changes and files.</p>"},{"location":"90DaysOfDevOps/day67/#run-the-updated-playbook","title":"Run the updated playbook","text":"<p>Since yesterday we have added the common role which will now install some packages on our system and then we have also added our nginx role which includes installation and configuration.</p> <p>Let's run our playbook4.yml using the <code>ansible-playbook playbook4.yml</code></p> <p></p> <p>Now that we have our webservers and loadbalancer configured we should now be able to go to http://192.168.169.134/ which is the IP address of our loadbalancer.</p> <p></p> <p>If you are following along and you do not have this state then it could be down to the server IP addresses you have in your environment. The file can be found in <code>templates\\mysite.j2</code> and looks similar to the below: You would need to update with your web server IP addresses.</p> <pre><code>    upstream webservers {\n        server 192.168.169.131:8000;\n        server 192.168.169.132:8000;\n    }\n\n    server {\n        listen 80;\n\n        location / {\n                proxy_pass http://webservers;\n        }\n    }\n</code></pre> <p>I am pretty confident that what we have installed is all good but let's use an ad-hoc command using ansible to check these common tools installation.</p> <p><code>ansible loadbalancer -m command -a neofetch</code></p> <p></p>"},{"location":"90DaysOfDevOps/day67/#resources","title":"Resources","text":"<ul> <li>What is Ansible</li> <li>Ansible 101 - Episode 1 - Introduction to Ansible</li> <li>NetworkChuck - You need to learn Ansible right now!</li> <li>Your complete guide to Ansible</li> </ul> <p>This final playlist listed above is where a lot of the code and ideas came from for this section, a great resource and walkthrough in video format.</p> <p>See you on Day 68</p>"},{"location":"90DaysOfDevOps/day68/","title":"#90DaysOfDevOps - Tags, Variables, Inventory & Database Server config - Day 68","text":""},{"location":"90DaysOfDevOps/day68/#tags-variables-inventory-database-server-config","title":"Tags, Variables, Inventory &amp; Database Server config","text":""},{"location":"90DaysOfDevOps/day68/#tags","title":"Tags","text":"<p>As we left our playbook in the session yesterday we would need to run every task and play within that playbook. This means we would have to run the webservers and loadbalancer plays and tasks to completion.</p> <p>However, tags can enable us to separate these if we want. This could be an efficient move if we have extra large and long playbooks in our environments.</p> <p>In our playbook file, in this case, we are using ansible-scenario5</p> <pre><code>- hosts: webservers\n  become: yes\n  vars:\n    http_port: 8000\n    https_port: 4443\n    html_welcome_msg: \"Hello 90DaysOfDevOps - Welcome to Day 66!\"\n  roles:\n    - common\n    - apache2\n  tags: web\n\n- hosts: proxy\n  become: yes\n  roles:\n    - common\n    - nginx\n  tags: proxy\n</code></pre> <p>We can then confirm this by using the <code>ansible-playbook playbook5.yml --list-tags</code> and the list tags are going to outline the tags we have defined in our playbook.</p> <p></p> <p>Now if we wanted to target just the proxy we could do this by running <code>ansible-playbook playbook5.yml --tags proxy</code> and this will as you can see below only run the playbook against the proxy.</p> <p></p> <p>tags can be added at the task level as well so we can get granular on where and what you want to happen. It could be application-focused tags, we could go through tasks for example and tag our tasks based on installation, configuration or removal. Another very useful tag you can use is</p> <p><code>tag: always</code> this will ensure no matter what --tags you are using in your command if something is tagged with the always value then it will always be running when you run the ansible-playbook command.</p> <p>With tags, we can also bundle multiple tags together and if we choose to run <code>ansible-playbook playbook5.yml --tags proxy,web</code> this will run all of the items with those tags. Obviously, in our instance, that would mean the same as running the playbook but if we had multiple other plays then this would make sense.</p> <p>You can also define more than one tag.</p>"},{"location":"90DaysOfDevOps/day68/#variables","title":"Variables","text":"<p>There are two main types of variables within Ansible.</p> <ul> <li>User created</li> <li>Ansible Facts</li> </ul>"},{"location":"90DaysOfDevOps/day68/#ansible-facts","title":"Ansible Facts","text":"<p>Each time we have run our playbooks, we have had a task that we have not defined called \"Gathering facts\" we can use these variables or facts to make things happen with our automation tasks.</p> <p></p> <p>If we were to run the following <code>ansible proxy -m setup</code> command we should see a lot of output in JSON format. There is going to be a lot of information on your terminal though to use this so we would like to output this to a file using <code>ansible proxy -m setup &gt;&gt; facts.json</code> you can see this file in this repository, ansible-scenario5</p> <p></p> <p>If you open this file you can see all sorts of information for our command. We can get our IP addresses, architecture, and bios version. A lot of useful information if we want to leverage this and use this in our playbooks.</p> <p>An idea would be to potentially use one of these variables within our nginx template mysite.j2 where we hard-coded the IP addresses of our webservers. You can do this by creating a for loop in your mysite.j2 and this is going to cycle through the group [webservers] this enables us to have more than our 2 webservers automatically and dynamically created or added to this load balancer configuration.</p> <pre><code>#Dynamic Config for server {{ ansible_facts['nodename'] }}\n    upstream webservers {\n  {% for host in groups['webservers'] %}\n        server {{ hostvars[host]['ansible_facts']['nodename'] }}:8000;\n    {% endfor %}\n    }\n\n    server {\n        listen 80;\n\n        location / {\n                proxy_pass http://webservers;\n        }\n    }\n</code></pre> <p>The outcome of the above will look the same as it does right now but if we added more web servers or removed one this would dynamically change the proxy configuration. For this to work you will need to have name resolution configured.</p>"},{"location":"90DaysOfDevOps/day68/#user-created","title":"User created","text":"<p>User-created variables are what we have created ourselves. If you take a look in our playbook you will see we have <code>vars:</code> and then a list of 3 variables we are using there.</p> <pre><code>- hosts: webservers\n  become: yes\n  vars:\n    http_port: 8000\n    https_port: 4443\n    html_welcome_msg: \"Hello 90DaysOfDevOps - Welcome to Day 68!\"\n  roles:\n    - common\n    - apache2\n  tags: web\n\n- hosts: proxy\n  become: yes\n  roles:\n    - common\n    - nginx\n  tags: proxy\n</code></pre> <p>We can however keep our playbook clear of variables by moving them to their file. We are going to do this but we will move into the ansible-scenario6 folder. In the root of that folder, we are going to create a group_vars folder. We are then going to create another folder called all (all groups are going to get these variables). In there we will create a file called <code>common_variables.yml</code> and we will copy our variables from our playbook into this file. Removing them from the playbook along with vars: as well.</p> <pre><code>http_port: 8000\nhttps_port: 4443\nhtml_welcome_msg: \"Hello 90DaysOfDevOps - Welcome to Day 68!\"\n</code></pre> <p>Because we are associating this as a global variable we could also add in our NTP and DNS servers here as well. The variables are set from the folder structure that we have created. You can see below how clean our Playbook now looks.</p> <pre><code>- hosts: webservers\n  become: yes\n  roles:\n    - common\n    - apache2\n  tags: web\n\n- hosts: proxy\n  become: yes\n  roles:\n    - common\n    - nginx\n  tags: proxy\n</code></pre> <p>One of those variables was the http_port, we can use this again in our for loop within the mysite.j2 as per below:</p> <pre><code>#Dynamic Config for server {{ ansible_facts['nodename'] }}\n    upstream webservers {\n  {% for host in groups['webservers'] %}\n        server {{ hostvars[host]['ansible_facts']['nodename'] }}:{{ http_port }};\n    {% endfor %}\n    }\n\n    server {\n        listen 80;\n\n        location / {\n                proxy_pass http://webservers;\n        }\n    }\n</code></pre> <p>We can also define an ansible fact in our roles/apache2/templates/index.HTML.j2 file so that we can understand which webserver we are on.</p> <pre><code>&lt;html&gt;\n\n&lt;h1&gt;{{ html_welcome_msg }}! I'm webserver {{ ansible_facts['nodename'] }} &lt;/h1&gt;\n\n&lt;/html&gt;\n</code></pre> <p>The results of running the <code>ansible-playbook playbook6.yml</code> command with our variable changes mean that when we hit our loadbalancer you can see that we hit either of the webservers we have in our group.</p> <p></p> <p>We could also add a folder called host_vars and create a web01.yml and have a specific message or change what that looks like on a per host basis if we wish.</p>"},{"location":"90DaysOfDevOps/day68/#inventory-files","title":"Inventory Files","text":"<p>So far we have used the default hosts file in the /etc/ansible folder to determine our hosts. We could however have different files for different environments, for example, production and staging. I am not going to create more environments. But we can create our host files.</p> <p>We can create multiple files for our different inventory of servers and nodes. We would call these using <code>ansible-playbook -i dev playbook.yml</code> you can also define variables within your host's file and then print that out or leverage that variable somewhere else in your playbooks for example in the example and training course I am following along to below they have added the environment variable created in the host file to the loadbalancer web page template to show the environment as part of the web page message.</p>"},{"location":"90DaysOfDevOps/day68/#deploying-our-database-server","title":"Deploying our Database server","text":"<p>We still have one more machine we have not powered up yet and configured. We can do this using <code>vagrant up db01</code> from where our Vagrantfile is located. When this is up and accessible we then need to make sure the SSH key is copied over using <code>ssh-copy-id db01</code> so that we can access it.</p> <p>We are going to be working from the ansible-scenario7 folder</p> <p>Let's then use <code>ansible-galaxy init roles/mysql</code> to create a new folder structure for a new role called \"MySQL\"</p> <p>In our playbook, we are going to add a new play block for the database configuration. We have our group database defined in our /etc/ansible/hosts file. We then instruct our database group to have the role common and a new role called MySQL which we created in the previous step. We are also tagging our database group with the database, this means as we discussed earlier we can choose to only run against these tags if we wish.</p> <pre><code>- hosts: webservers\n  become: yes\n  roles:\n    - common\n    - apache2\n  tags:\n    web\n\n- hosts: proxy\n  become: yes\n  roles:\n    - common\n    - nginx\n  tags:\n    proxy\n\n- hosts: database\n  become: yes\n  roles:\n    - common\n    - mysql\n  tags: database\n</code></pre> <p>Within our roles folder structure, you will now have the tree automatically created, we need to populate the following:</p> <p>Handlers - main.yml</p> <pre><code># handlers file for roles/mysql\n- name: restart mysql\n  service:\n    name: mysql\n    state: restarted\n</code></pre> <p>Tasks - install_mysql.yml, main.yml &amp; setup_mysql.yml</p> <p>install_mysql.yml - this task is going to be there to install MySQL and ensure that the service is running.</p> <pre><code>- name: \"Install Common packages\"\n  apt: name={{ item }} state=latest\n  with_items:\n   - python3-pip\n   - mysql-client\n   - python3-mysqldb\n   - libmysqlclient-dev\n\n- name: Ensure mysql-server is installed latest version\n  apt: name=mysql-server state=latest\n\n- name: Installing python module MySQL-python\n  pip:\n    name: PyMySQL\n\n- name: Ensure mysql-server is running\n  service:\n    name: mysql\n    state: started\n</code></pre> <p>main.yml is a pointer file that will suggest that we import_tasks from these files.</p> <pre><code># tasks file for roles/mysql\n- import_tasks: install_mysql.yml\n- import_tasks: setup_mysql.yml\n</code></pre> <p>setup_mysql.yml - This task will create our database and database user.</p> <pre><code>- name: Create my.cnf configuration file\n  template: src=templates/my.cnf.j2 dest=/etc/mysql/conf.d/mysql.cnf\n  notify: restart mysql\n\n- name: Create database user with name 'devops' and password 'DevOps90' with all database privileges\n  community.mysql.mysql_user:\n    login_unix_socket: /var/run/mysqld/mysqld.sock\n    login_user: \"{{ mysql_user_name }}\"\n    login_password: \"{{ mysql_user_password }}\"\n    name: \"{{db_user}}\"\n    password: \"{{db_pass}}\"\n    priv: '*.*:ALL'\n    host: '%'\n    state: present\n\n- name: Create a new database with name '90daysofdevops'\n  mysql_db:\n    login_user: \"{{ mysql_user_name }}\"\n    login_password: \"{{ mysql_user_password }}\"\n    name: \"{{ db_name }}\"\n    state: present\n</code></pre> <p>You can see from the above we are using some variables to determine some of our configuration such as passwords, usernames and databases, this is all stored in our group_vars/all/common_variables.yml file.</p> <pre><code>http_port: 8000\nhttps_port: 4443\nhtml_welcome_msg: \"Hello 90DaysOfDevOps - Welcome to Day 68!\"\n\nmysql_user_name: root\nmysql_user_password: \"vagrant\"\ndb_user: devops\ndb_pass: DevOps90\ndb_name: 90DaysOfDevOps\n</code></pre> <p>We also have my.cnf.j2 file in the templates folder, which looks like below:</p> <pre><code>[mysql]\nbind-address = 0.0.0.0\n</code></pre>"},{"location":"90DaysOfDevOps/day68/#running-the-playbook","title":"Running the playbook","text":"<p>Now we have our VM up and running and we have our configuration files in place, we are now ready to run our playbook which will include everything we have done before if we run the following <code>ansible-playbook playbook7.yml</code> or we could choose to just deploy to our database group with the <code>ansible-playbook playbook7.yml --tags database</code> command, which will just run our new configuration files.</p> <p>I ran only against the database tag but I stumbled across an error. This error tells me that we do not have pip3 (Python) installed. We can fix this by adding this to our common tasks and install</p> <p></p> <p>We fixed the above and ran the playbook again and we have a successful change.</p> <p></p> <p>We should probably make sure that everything is how we want it to be on our newly configured db01 server. We can do this from our control node using the <code>ssh db01</code> command.</p> <p>To connect to MySQL I used <code>sudo /usr/bin/mysql -u root -p</code> and gave the vagrant password for root at the prompt.</p> <p>When we have connected let's first make sure we have our user created called DevOps. <code>select user, host from mysql.user;</code></p> <p></p> <p>Now we can issue the <code>SHOW DATABASES;</code> command to see our new database that has also been created.</p> <p></p> <p>I used the root to connect but we could also now log in with our DevOps account, in the same way, using <code>sudo /usr/bin/MySQL -u devops -p</code> but the password here is DevOps90.</p> <p>One thing I have found is that in our <code>setup_mysql.yml</code> I had to add the line <code>login_unix_socket: /var/run/mysqld/mysqld.sock</code> to successfully connect to my db01 MySQL instance and now every time I run this it reports a change when creating the user, any suggestions would be greatly appreciated.</p>"},{"location":"90DaysOfDevOps/day68/#resources","title":"Resources","text":"<ul> <li>What is Ansible</li> <li>Ansible 101 - Episode 1 - Introduction to Ansible</li> <li>NetworkChuck - You need to learn Ansible right now!</li> <li>Your complete guide to Ansible</li> </ul> <p>This final playlist listed above is where a lot of the code and ideas came from for this section, a great resource and walkthrough in video format.</p> <p>See you on Day 69</p>"},{"location":"90DaysOfDevOps/day69/","title":"#90DaysOfDevOps - All other things Ansible - Automation Controller (Tower), AWX, Vault - Day 69","text":""},{"location":"90DaysOfDevOps/day69/#all-other-things-ansible-automation-controller-tower-awx-vault","title":"All other things Ansible - Automation Controller (Tower), AWX, Vault","text":"<p>Rounding out the section on Configuration Management I wanted to have a look into the other areas that you might come across when dealing with Ansible.</p> <p>There are a lot of products that make up the Ansible Automation platform.</p> <p>Red Hat Ansible Automation Platform is a foundation for building and operating automation across an organization. The platform includes all the tools needed to implement enterprise-wide automation.</p> <p></p> <p>I will try and cover some of these in this post. But for more information then the official Red Hat Ansible site is going to have lots more information. Ansible.com</p>"},{"location":"90DaysOfDevOps/day69/#ansible-automation-controller-awx","title":"Ansible Automation Controller | AWX","text":"<p>I have bundled these two together because the Automation Controller and AWX are very similar in what they offer.</p> <p>The AWX project or AWX for short is an open-source community project, sponsored by Red Hat that enables you to better control your Ansible projects within your environments. AWX is the upstream project from which the automation controller component is derived.</p> <p>If you are looking for an enterprise solution then you will be looking for the Automation Controller or you might have previously heard this as Ansible Tower. The Ansible Automation Controller is the control plane for the Ansible Automation Platform.</p> <p>Both AWX and the Automation Controller bring the following features above everything else we have covered in this section thus far.</p> <ul> <li>User Interface</li> <li>Role-Based Access Control</li> <li>Workflows</li> <li>CI/CD integration</li> </ul> <p>The Automation Controller is the enterprise offering where you pay for your support.</p> <p>We are going to take a look at deploying AWX within our minikube Kubernetes environment.</p>"},{"location":"90DaysOfDevOps/day69/#deploying-ansible-awx","title":"Deploying Ansible AWX","text":"<p>AWX does not need to be deployed to a Kubernetes cluster, the github for AWX from ansible will give you that detail. However starting in version 18.0, the AWX Operator is the preferred way to install AWX.</p> <p>First of all, we need a minikube cluster. We can do this if you followed along during the Kubernetes section by creating a new minikube cluster with the <code>minikube start --cpus=4 --memory=6g --addons=ingress</code> command.</p> <p></p> <p>The official Ansible AWX Operator can be found here. As stated in the install instructions you should clone this repository and then run through the deployment.</p> <p>I forked the repo above and then ran <code>git clone https://github.com/MichaelCade/awx-operator.git</code> my advice is you do the same and do not use my repository as I might change things or it might not be there.</p> <p>In the cloned repository you will find an awx-demo.yml file we need to change <code>NodePort</code> for <code>ClusterIP</code> as per below:</p> <pre><code>---\napiVersion: awx.ansible.com/v1beta1\nkind: AWX\nmetadata:\n  name: awx-demo\nspec:\n  service_type: ClusterIP\n</code></pre> <p>The next step is to define our namespace where we will be deploying the awx operator, using the <code>export NAMESPACE=awx</code> command then followed by <code>make deploy</code> we will start the deployment.</p> <p></p> <p>In checking we have our new namespace and we have our awx-operator-controller pod running in our namespace. <code>kubectl get pods -n awx</code></p> <p></p> <p>Within the cloned repository you will find a file called awx-demo.yml we now want to deploy this into our Kubernetes cluster and our awx namespace. <code>kubectl create -f awx-demo.yml -n awx</code></p> <p></p> <p>You can keep an eye on the progress with <code>kubectl get pods -n awx -w</code> which will keep a visual watch on what is happening.</p> <p>You should have something that resembles the image you see below when everything is running.</p> <p></p> <p>Now we should be able to access our awx deployment after running in a new terminal <code>minikube service awx-demo-service --url -n $NAMESPACE</code> to expose this through the minikube ingress.</p> <p></p> <p>If we then open a browser to that address [] you can see we are prompted for username and password.</p> <p></p> <p>The username by default is admin, to get the password we can run the following command to get this <code>kubectl get secret awx-demo-admin-password -o jsonpath=\"{.data.password}\" -n awx| base64 --decode</code></p> <p></p> <p>This then gives you a UI to manage your playbook and configuration management tasks in a centralised location, it also allows you as a team to work together vs what we have been doing so far here where we have been running from one ansible control station.</p> <p>This is another one of those areas where you could probably go and spend another length of time walking through the capabilities within this tool.</p> <p>I will call out a great resource from Jeff Geerling, which goes into more detail on using Ansible AWX. Ansible 101 - Episode 10 - Ansible Tower and AWX</p> <p>In this video, he also goes into great detail on the differences between Automation Controller (Previously Ansible Tower) and Ansible AWX (Free and Open Source).</p>"},{"location":"90DaysOfDevOps/day69/#ansible-vault","title":"Ansible Vault","text":"<p><code>ansible-vault</code> allows us to encrypt and decrypt Ansible data files. Throughout this section, we have skipped over and put some of our sensitive information in plain text.</p> <p>Built into the Ansible binary is <code>ansible-vault</code> which allows us to mask away this sensitive information.</p> <p></p> <p>Secrets Management has progressively become another area in which more time should have been spent alongside tools such as HashiCorp Vault or the AWS Key Management Service. I will mark this as an area to dive deeper into.</p> <p>I am going to link a great resource and demo to run through from Jeff Geerling again Ansible 101 - Episode 6 - Ansible Vault and Roles</p>"},{"location":"90DaysOfDevOps/day69/#ansible-galaxy-docs","title":"Ansible Galaxy (Docs)","text":"<p>Now, we have already used <code>ansible-galaxy</code> to create some of our roles and file structure for our demo project. But we also have Ansible Galaxy documentation</p> <p>\"Galaxy is a hub for finding and sharing Ansible content.\"</p>"},{"location":"90DaysOfDevOps/day69/#ansible-testing","title":"Ansible Testing","text":"<ul> <li> <p>Ansible Molecule - The molecule project is designed to aid in the development and testing of Ansible roles</p> </li> <li> <p>Ansible Lint - CLI tool for linting playbooks, roles and collections</p> </li> </ul>"},{"location":"90DaysOfDevOps/day69/#other-resource","title":"Other Resource","text":"<ul> <li>Ansible Documentation</li> </ul>"},{"location":"90DaysOfDevOps/day69/#resources","title":"Resources","text":"<ul> <li>What is Ansible</li> <li>Ansible 101 - Episode 1 - Introduction to Ansible</li> <li>NetworkChuck - You need to learn Ansible right now!</li> <li>Your complete guide to Ansible</li> </ul> <p>This final playlist listed above is where a lot of the code and ideas came from for this section, a great resource and walkthrough in video format.</p> <p>This post wraps up our look into configuration management, we next move into CI/CD Pipelines and some of the tools and processes that we might see and use out there to achieve this workflow for our application development and release.</p> <p>See you on Day 70</p>"},{"location":"90DaysOfDevOps/day70/","title":"#90DaysOfDevOps - The Big Picture: CI/CD Pipelines - Day 70","text":""},{"location":"90DaysOfDevOps/day70/#the-big-picture-cicd-pipelines","title":"The Big Picture: CI/CD Pipelines","text":"<p>A CI/CD (Continuous Integration/Continuous Deployment) Pipeline implementation is the backbone of the modern DevOps environment.</p> <p>It bridges the gap between development and operations by automating the build, test and deployment of applications.</p> <p>We covered a lot of this continuous mantra in the opening section of the challenge. But to reiterate:</p> <p>Continuous Integration (CI) is a more modern software development practice in which incremental code changes are made more frequently and reliably. Automated build and test workflow steps triggered by Continuous Integration ensure that code changes being merged into the repository are reliable.</p> <p>That code / Application is then delivered quickly and seamlessly as part of the Continuous Deployment process.</p>"},{"location":"90DaysOfDevOps/day70/#the-importance-of-cicd","title":"The importance of CI/CD?","text":"<ul> <li>Ship software quickly and efficiently</li> <li>Facilitates an effective process for getting applications to market as fast as possible</li> <li>A continuous flow of bug fixes and new features without waiting months or years for version releases.</li> </ul> <p>The ability for developers to make small impactful changes regular means we get faster fixes and more features quicker.</p>"},{"location":"90DaysOfDevOps/day70/#ok-so-what-does-this-mean","title":"Ok, so what does this mean?","text":"<p>On Day 5 we covered a lot of the theory behind DevOps and as already mentioned here that the CI/CD Pipeline is the backbone of the modern DevOps environment.</p> <p></p> <p>I want to reiterate some of the key points on this image above, now that we are a little further into our journey of learning the fundamentals of DevOps.</p> <p>We are referring to the software development life cycle (SDLC).</p> <p>The steps are usually written out within an infinity loop since it's a cycle that repeats forever.</p> <p>The steps in the cycle are, developers write the code then it gets built or all compiled together then it's tested for bugs then it's deployed into production where it's used (Operated) by end users or customers then we monitor and collect feedback and finally we plan improvements around that feedback rinse and repeat.</p>"},{"location":"90DaysOfDevOps/day70/#lets-go-a-little-deeper-into-cicd","title":"Let's go a little deeper into CI/CD","text":""},{"location":"90DaysOfDevOps/day70/#ci","title":"CI","text":"<p>CI is a development practice that requires developers to integrate code into a shared repository several times a day.</p> <p>When the code is written and pushed to a repository like Github or GitLab that's where the magic begins.</p> <p></p> <p>The code is verified by an automated build which allows teams or the project owner to detect any problems early.</p> <p></p> <p>From there the code is analysed and given a series of automated tests three examples are</p> <ul> <li>Unit testing tests the individual units of the source code</li> <li>Validation testing makes sure that the software satisfies or fits the intended use</li> <li>Format testing checks for syntax and other formatting errors</li> </ul> <p>These tests are created as a workflow and then are run every time you push to the master branch so pretty much every major development team has some sort of CI/CD workflow and remember on a development team the new code could be coming in from teams all over the world at different times of the day from developers working on all sorts of different projects it's more efficient to build an automated workflow of tests that make sure that everyone is on the same page before the code is accepted. It would take much longer for a human to do this each time.</p> <p></p> <p>Once we have our tests complete and they are successful then we can compile and send them to our repository. For example, I am using Docker Hub but this could be anywhere that then gets leveraged for the CD aspect of the pipeline.</p> <p></p> <p>So this process is very much down to the software development process, we are creating our application, adding, fixing bugs etc and then updating our source control and versioning that whilst also testing.</p> <p>Moving onto the next phase is the CD element which more and more is what we generally see from any off-the-shelf software, I would argue that we will see a trend if we get our software from a vendor such as Oracle or Microsoft we will consume that from a Docker Hub type repository and then we would use our CD pipelines to deploy that into our environments.</p>"},{"location":"90DaysOfDevOps/day70/#cd","title":"CD","text":"<p>Now we have our tested version of our code and we are ready to deploy out into the wild as I say, the Software vendor will run through this stage but I strongly believe this is how we will all deploy the off-the-shelf software we require in the future.</p> <p>It is now time to release our code into an environment. This is going to include Production but also likely other environments as well such as staging.</p> <p></p> <p>Our next step at least on Day 1 of v1 of the software deployment is we need to make sure we are pulling the correct code base to the correct environment. This could be pulling elements from the software repository (DockerHub) but it is more than likely that we are also pulling additional configuration from maybe another code repository, the configuration for the application for example. In the diagram below we are pulling the latest release of the software from DockerHub and then we are releasing this to our environments whilst possibly picking up configuration from a Git repository. Our CD tool is performing this and pushing everything to our environment.</p> <p>It is most likely that this is not done at the same time. i.e we would go to a staging environment run against this with our configuration to make sure things are correct and this could be a manual step for testing or it could again be automated (let's go with automated) before then allowing this code to be deployed into production.</p> <p></p> <p>Then after this when v2 of the application comes out we rinse and repeat the steps this time we ensure our application + configuration is deployed to staging ensure everything is good and then we deploy to production.</p>"},{"location":"90DaysOfDevOps/day70/#why-use-cicd","title":"Why use CI/CD?","text":"<p>I think we have probably covered the benefits several times but it is because it automates things that otherwise would have to be done manually it finds small problems before it sneaks into the main codebase, you can probably imagine that if you push bad code out to your customers then you're going to have a bad time!</p> <p>It also helps to prevent something that we call technical debt which is the idea that since the main code repos are constantly being built upon over time then a shortcut fix taken on day one is now an exponentially more expensive fix years later because now that band-aid of a fix would be so deeply intertwined and baked into all the code bases and logic.</p>"},{"location":"90DaysOfDevOps/day70/#tooling","title":"Tooling","text":"<p>Like with other sections we are going to get hands-on with some of the tools that achieve the CI/CD pipeline process.</p> <p>I think it is also important to note that not all tools have to do both CI and CD, We will take a look at ArgoCD which you guessed is great at the CD element of deploying our software to a Kubernetes cluster. But something like Jenkins can work across many different platforms.</p> <p>I plan to look at the following:</p> <ul> <li>Jenkins</li> <li>ArgoCD</li> <li>GitHub Actions</li> </ul>"},{"location":"90DaysOfDevOps/day70/#resources","title":"Resources","text":"<ul> <li>Jenkins is the way to build, test, deploy</li> <li>Introduction to Jenkins</li> <li>Jenkins.io</li> <li>ArgoCD</li> <li>ArgoCD Tutorial for Beginners</li> <li>What is Jenkins?</li> <li>Complete Jenkins Tutorial</li> <li>GitHub Actions</li> <li>GitHub Actions CI/CD</li> </ul> <p>See you on Day 71</p>"},{"location":"90DaysOfDevOps/day71/","title":"#90DaysOfDevOps - What is Jenkins? - Day 71","text":""},{"location":"90DaysOfDevOps/day71/#what-is-jenkins","title":"What is Jenkins?","text":"<p>Jenkins is a continuous integration tool that allows continuous development, testing and deployment of newly created code.</p> <p>There are two ways we can achieve this with either nightly builds or continuous development. The first option is that our developers are developing throughout the day on their tasks and come to the end of the set day they push their changes to the source code repository. Then during the night we run our unit tests and build the software. This could be deemed as the old way to integrate all code.</p> <p></p> <p>The other option and the preferred way is that our developers are still committing their changes to source code, then when that code commit has been made there is a build process kicked off continuously.</p> <p></p> <p>The above methods mean that with distributed developers across the world we don't have a set time each day where we have to stop committing our code changes. This is where Jenkins comes in to act as that CI server to control those tests and build processes.</p> <p></p> <p>I know we are talking about Jenkins here but I also want to add a few more to maybe look into later on down the line to get an understanding of why I am seeing Jenkins as the overall most popular, why is that and what can the others do over Jenkins.</p> <ul> <li>TravisCI - A hosted, distributed continuous integration service used to build and test software projects hosted on GitHub.</li> <li>Bamboo - Can run multiple builds in parallel for faster compilation, built-in functionality to connect with repositories and has build tasks for Ant, and Maven.</li> <li>Buildbot - is an open-source framework for automating software build, test and release processes. It is written in Python and supports distributed, parallel execution of jobs across multiple platforms.</li> <li>Apache Gump - Specific to Java projects, designed to build and test those Java projects every night. ensures that all projects are compatible at both API and functionality levels.</li> </ul> <p>Because we are now going to focus on Jenkins - Jenkins is again open source like all of the above tools and is an automation server written in Java. It is used to automate the software development process via continuous integration and facilitates continuous delivery.</p>"},{"location":"90DaysOfDevOps/day71/#features-of-jenkins","title":"Features of Jenkins","text":"<p>As you can probably expect Jenkins has a lot of features spanning a lot of areas.</p> <p>Easy Installation - Jenkins is a self-contained java based program ready to run with packages for Windows, macOS and Linux operating systems.</p> <p>Easy Configuration - Easy setup and configuration via a web interface which includes error checks and built-in help.</p> <p>Plug-ins - Lots of plugins are available in the Update Centre and integrate with many tools in the CI / CD toolchain.</p> <p>Extensible - In addition to the Plug-Ins available, Jenkins can be extended by that plugin architecture which provides nearly infinite options for what it can be used for.</p> <p>Distributed - Jenkins easily distributes work across multiple machines, helping to speed up builds, tests and deployments across multiple platforms.</p>"},{"location":"90DaysOfDevOps/day71/#jenkins-pipeline","title":"Jenkins Pipeline","text":"<p>You will have seen this pipeline but used in a much broader and we have not spoken about specific tools.</p> <p>You are going to be committing code to Jenkins, which then will build out your application, with all automated tests, it will then release and deploy that code when each step is completed. Jenkins is what allows for the automation of this process.</p> <p></p>"},{"location":"90DaysOfDevOps/day71/#jenkins-architecture","title":"Jenkins Architecture","text":"<p>First up and not wanting to reinvent the wheel, the Jenkins Documentation is always the place to start but I am going to put down my notes and learnings here as well.</p> <p>Jenkins can be installed on many different operating systems, Windows, Linux and macOS but then also the ability to deploy as a Docker container and within Kubernetes. Installing Jenkins</p> <p>As we get into this we will likely take a look at installing Jenkins within a minikube cluster simulating the deployment to Kubernetes. But this will depend on the scenarios we put together throughout the rest of the section.</p> <p>Let's now break down the image below.</p> <p>Step 1 - Developers commit changes to the source code repository.</p> <p>Step 2 - Jenkins checks the repository at regular intervals and pulls any new code.</p> <p>Step 3 - A build server then builds the code into an executable, in this example, we are using maven as a well-known build server. Another area to cover.</p> <p>Step 4 - If the build fails then feedback is sent back to the developers.</p> <p>Step 5 - Jenkins then deploys the build app to the test server, in this example, we are using selenium as a well-known test server. Another area to cover.</p> <p>Step 6 - If the test fails then feedback is passed to the developers.</p> <p>Step 7 - If the tests are successful then we can release them to production.</p> <p>This cycle is continuous, this is what allows applications to be updated in minutes vs hours, days, months, and years!</p> <p></p> <p>There is a lot more to the architecture of Jenkins if you require it, they have a master-slave capability, which enables a master to distribute the tasks to the slave Jenkins environment.</p> <p>For reference with Jenkins being open source, there are going to be lots of enterprises that require support, CloudBees is that enterprise version of Jenkins that brings support and possibly other functionality for the paying enterprise customer.</p> <p>An example of this in a customer is Bosch, you can find the Bosch case study here</p> <p>I am going to be looking for a step-by-step example of an application that we can use to walk through using Jenkins and then also use this with some other tools.</p>"},{"location":"90DaysOfDevOps/day71/#resources","title":"Resources","text":"<ul> <li>Jenkins is the way to build, test, deploy</li> <li>Jenkins.io</li> <li>ArgoCD</li> <li>ArgoCD Tutorial for Beginners</li> <li>What is Jenkins?</li> <li>Complete Jenkins Tutorial</li> <li>GitHub Actions</li> <li>GitHub Actions CI/CD</li> </ul> <p>See you on Day 72</p>"},{"location":"90DaysOfDevOps/day72/","title":"#90DaysOfDevOps - Getting hands-on with Jenkins - Day 72","text":""},{"location":"90DaysOfDevOps/day72/#getting-hands-on-with-jenkins","title":"Getting hands-on with Jenkins","text":"<p>The plan today is to get some hands-on with Jenkins and make something happen as part of our CI pipeline, looking at some example code bases that we can use.</p>"},{"location":"90DaysOfDevOps/day72/#what-is-a-pipeline","title":"What is a pipeline?","text":"<p>Before we start we need to know what is a pipeline when it comes to CI, and we already covered this in the session yesterday with the following image.</p> <p></p> <p>We want to take the processes or steps above and we want to automate them to get an outcome eventually meaning that we have a deployed application that we can then ship to our customers, end users etc.</p> <p>This automated process enables us to have version control through to our users and customers. Every change, feature enhancement, bug fix etc goes through this automated process confirming that everything is fine without too much manual intervention to ensure our code is good.</p> <p>This process involves building the software in a reliable and repeatable manner, as well as progressing the built software (called a \"build\") through multiple stages of testing and deployment.</p> <p>A Jenkins pipeline is written into a text file called a Jenkinsfile. Which itself should be committed to a source control repository. This is also known as Pipeline as code, we could also very much liken this to Infrastructure as code which we covered a few weeks back.</p> <p>Jenkins Pipeline Definition</p>"},{"location":"90DaysOfDevOps/day72/#deploying-jenkins","title":"Deploying Jenkins","text":"<p>I had some fun deploying Jenkins, You will notice from the documentation that there are many options on where you can install Jenkins.</p> <p>Given that I have minikube on hand and we have used this several times I wanted to use this for this task also. (also it is free!) Although the steps are given in the Kubernetes Installation had me hitting a wall and not getting things up and running, you can compare the two when I document my steps here.</p> <p>The first step is to get our minikube cluster up and running, we can simply do this with the <code>minikube start</code> command.</p> <p></p> <p>I have added a folder with all the YAML configuration and values that can be found here Now that we have our cluster we can run the following to create our jenkins namespace. <code>kubectl create -f jenkins-namespace.yml</code></p> <p></p> <p>We will be using Helm to deploy Jenkins into our cluster, we covered helm in the Kubernetes section. We first need to add the jenkinsci helm repository <code>helm repo add jenkinsci https://charts.jenkins.io</code> then update our charts <code>helm repo update</code>.</p> <p></p> <p>The idea behind Jenkins is that it is going to save state for its pipelines, you can run the above helm installation without persistence but if those pods are rebooted, changed or modified then any pipeline or configuration you have made will be lost. We will create a volume for persistence using the jenkins-volume.yml file with the <code>kubectl apply -f jenkins-volume.yml</code> command.</p> <p></p> <p>We also need a service account which we can create using this YAML file and command. <code>kubectl apply -f jenkins-sa.yml</code></p> <p></p> <p>At this stage we are good to deploy using the helm chart, we will first define our chart using <code>chart=jenkinsci/jenkins</code> and then we will deploy using this command where the jenkins-values.yml contain the persistence and service accounts that we previously deployed to our cluster. <code>helm install jenkins -n jenkins -f jenkins-values.yml $chart</code></p> <p></p> <p>At this stage, our pods will be pulling the image but the pod will not have access to the storage so no configuration can be started in terms of getting Jenkins up and running.</p> <p>This is where the documentation did not help me massively understand what needed to happen. But we can see that we have no permission to start our Jenkins install.</p> <p></p> <p>To fix the above or resolve it, we need to make sure we provide access or the right permission for our Jenkins pods to be able to write to this location that we have suggested. We can do this by using the <code>minikube ssh</code> which will put us into the minikube docker container we are running on, and then using <code>sudo chown -R 1000:1000 /data/jenkins-volume</code> we can ensure we have permissions set on our data volume.</p> <p></p> <p>The above process should fix the pods, however, if not you can force the pods to be refreshed with the <code>kubectl delete pod jenkins-0 -n jenkins</code> command. At this point, you should have 2/2 running pods called jenkins-0.</p> <p></p> <p>We now need our admin password and we can this using the following command. <code>kubectl exec --namespace jenkins -it svc/jenkins -c jenkins -- /bin/cat /run/secrets/chart-admin-password &amp;&amp; echo</code></p> <p></p> <p>Now open a new terminal as we are going to use the <code>port-forward</code> command to allow us to gain access from our workstation. <code>kubectl --namespace jenkins port-forward svc/jenkins 8080:8080</code></p> <p></p> <p>We should now be able to open a browser and log in to <code>http://localhost:8080</code> and authenticate with the username: admin and password we gathered in a previous step.</p> <p></p> <p>When we have authenticated, our Jenkins welcome page should look something like this:</p> <p></p> <p>From here, I would suggest heading to \"Manage Jenkins\" and you will see \"Manage Plugins\" which will have some updates available. Select all of those plugins and choose \"Download now and install after restart\"</p> <p></p> <p>If you want to go even further and automate the deployment of Jenkins using a shell script this great repository was shared with me on Twitter mehyedes/nodejs-k8s</p>"},{"location":"90DaysOfDevOps/day72/#jenkinsfile","title":"Jenkinsfile","text":"<p>Now we have Jenkins deployed in our Kubernetes cluster, we can now go back and think about this Jenkinsfile.</p> <p>Every Jenkinsfile will likely start like this, Which is firstly where you would define the steps of your pipeline, in this instance you have Build &gt; Test &gt; Deploy. But we are not doing anything other than using the <code>echo</code> command to call out the specific stages.</p> <pre><code>Jenkinsfile (Declarative Pipeline)\n\npipeline {\n    agent any\n\n    stages {\n        stage('Build') {\n            steps {\n                echo 'Building..'\n            }\n        }\n        stage('Test') {\n            steps {\n                echo 'Testing..'\n            }\n        }\n        stage('Deploy') {\n            steps {\n                echo 'Deploying....'\n            }\n        }\n    }\n}\n</code></pre> <p>In our Jenkins dashboard, select \"New Item\" give the item a name, I am going to \"echo1\" I am going to suggest that this is a Pipeline.</p> <p></p> <p>Hit Ok and you will then have the tabs (General, Build Triggers, Advanced Project Options and Pipeline) for a simple test we are only interested in Pipeline. Under Pipeline you can add a script, we can copy and paste the above script into the box.</p> <p>As we said above this is not going to do much but it will show us the stages of our Build &gt; Test &gt; Deploy</p> <p></p> <p>Click Save, We can now run our build using the build now highlighted below.</p> <p></p> <p>We should also open a terminal and run the <code>kubectl get pods -n jenkins</code> to see what happens there.</p> <p></p> <p>Ok, very simple stuff but we can now see that our Jenkins deployment and installation are working correctly and we can start to see the building blocks of the CI pipeline here.</p> <p>In the next section, we will be building a Jenkins Pipeline.</p>"},{"location":"90DaysOfDevOps/day72/#resources","title":"Resources","text":"<ul> <li>Jenkins is the way to build, test, deploy</li> <li>Jenkins.io</li> <li>ArgoCD</li> <li>ArgoCD Tutorial for Beginners</li> <li>What is Jenkins?</li> <li>Complete Jenkins Tutorial</li> <li>GitHub Actions</li> <li>GitHub Actions CI/CD</li> </ul> <p>See you on Day 73</p>"},{"location":"90DaysOfDevOps/day73/","title":"#90DaysOfDevOps - Building a Jenkins Pipeline - Day 73","text":""},{"location":"90DaysOfDevOps/day73/#building-a-jenkins-pipeline","title":"Building a Jenkins Pipeline","text":"<p>In the last section, we got Jenkins deployed to our Minikube cluster and we set up a very basic Jenkins Pipeline, that didn't do much at all other than echo out the stages of a Pipeline.</p> <p>You might have also seen that there are some example scripts available for us to run in the Jenkins Pipeline creation.</p> <p></p> <p>The first demo script is \"Declarative (Kubernetes)\" and you can see the stages below.</p> <pre><code>// Uses Declarative syntax to run commands inside a container.\npipeline {\n    agent {\n        kubernetes {\n            // Rather than inline YAML, in a multibranch Pipeline you could use: yamlFile 'jenkins-pod.yaml'\n            // Or, to avoid YAML:\n            // containerTemplate {\n            //     name 'shell'\n            //     image 'ubuntu'\n            //     command 'sleep'\n            //     args 'infinity'\n            // }\n            yaml '''\napiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: shell\n    image: ubuntu\n    command:\n    - sleep\n    args:\n    - infinity\n'''\n            // Can also wrap individual steps:\n            // container('shell') {\n            //     sh 'hostname'\n            // }\n            defaultContainer 'shell'\n        }\n    }\n    stages {\n        stage('Main') {\n            steps {\n                sh 'hostname'\n            }\n        }\n    }\n}\n</code></pre> <p>You can see below the outcome of what happens when this Pipeline is run.</p> <p></p>"},{"location":"90DaysOfDevOps/day73/#job-creation","title":"Job creation","text":""},{"location":"90DaysOfDevOps/day73/#goals","title":"Goals","text":"<ul> <li> <p>Create a simple app and store it in GitHub public repository https://github.com/scriptcamp/kubernetes-kaniko.git</p> </li> <li> <p>Use Jenkins to build our docker Container image and push it to the docker hub. (for this we will use a private repository)</p> </li> </ul> <p>To achieve this in our Kubernetes cluster running in or using Minikube we need to use something called Kaniko It is general though if you are using Jenkins in a real Kubernetes cluster or you are running it on a server then you can specify an agent which will give you the ability to perform the docker build commands and upload that to DockerHub.</p> <p>With the above in mind, we are also going to deploy a secret into Kubernetes with our GitHub credentials.</p> <pre><code>kubectl create secret docker-registry dockercred \\\n    --docker-server=https://index.docker.io/v1/ \\\n    --docker-username=&lt;dockerhub-username&gt; \\\n    --docker-password=&lt;dockerhub-password&gt;\\\n    --docker-email=&lt;dockerhub-email&gt;\n</code></pre> <p>I want to share another great resource from DevOpsCube.com running through much of what we will cover here.</p>"},{"location":"90DaysOfDevOps/day73/#adding-credentials-to-jenkins","title":"Adding credentials to Jenkins","text":"<p>However, if you were on a Jenkins system unlike ours then you will likely want to define your credentials within Jenkins and then use them multiple times within your Pipelines and configurations. We can refer to these credentials in the Pipelines using the ID we determine on creation. I went ahead and stepped through and created a user entry for DockerHub and GitHub.</p> <p>First of all select \"Manage Jenkins\" and then \"Manage Credentials\"</p> <p></p> <p>You will see in the centre of the page, Stores scoped to Jenkins click on Jenkins here.</p> <p></p> <p>Now select Global Credentials (Unrestricted)</p> <p></p> <p>Then in the top left, you have Add Credentials</p> <p></p> <p>Fill in your details for your account and then select OK, remember the ID is what you will refer to when you want to call this credential. My advice here also is that you use specific token access vs passwords.</p> <p></p> <p>For GitHub, you should use a Personal Access Token</p> <p>I did not find this process very intuitive to create these accounts, so even though we are not using I wanted to share the process as it is not clear from the UI.</p>"},{"location":"90DaysOfDevOps/day73/#building-the-pipeline","title":"Building the pipeline","text":"<p>We have our DockerHub credentials deployed as a secret into our Kubernetes cluster which we will call upon for our docker deploy to the DockerHub stage in our pipeline.</p> <p>The pipeline script is what you can see below, this could in turn become our Jenkinsfile located in our GitHub repository which you can also see is listed in the Get the project stage of the pipeline.</p> <pre><code>podTemplate(yaml: '''\n    apiVersion: v1\n    kind: Pod\n    spec:\n      containers:\n      - name: maven\n        image: maven:3.8.1-jdk-8\n        command:\n        - sleep\n        args:\n        - 99d\n      - name: kaniko\n        image: gcr.io/kaniko-project/executor:debug\n        command:\n        - sleep\n        args:\n        - 9999999\n        volumeMounts:\n        - name: kaniko-secret\n          mountPath: /kaniko/.docker\n      restartPolicy: Never\n      volumes:\n      - name: kaniko-secret\n        secret:\n            secretName: dockercred\n            items:\n            - key: .dockerconfigjson\n              path: config.json\n''') {\n  node(POD_LABEL) {\n    stage('Get the project') {\n      git url: 'https://github.com/scriptcamp/kubernetes-kaniko.git', branch: 'main'\n      container('maven') {\n        stage('Test the project') {\n          sh '''\n          echo pwd\n          '''\n        }\n      }\n    }\n\n    stage('Build &amp; Test the Docker Image') {\n      container('kaniko') {\n        stage('Deploy to DockerHub') {\n          sh '''\n            /kaniko/executor --context `pwd` --destination michaelcade1/helloworld:latest\n          '''\n        }\n      }\n    }\n\n  }\n}\n</code></pre> <p>To kick things on the Jenkins dashboard we need to select \"New Item\"</p> <p></p> <p>We are then going to give our item a name, select Pipeline and then hit ok.</p> <p></p> <p>We are not going to be selecting any of the general or build triggers but have a play with these as there are some interesting schedules and other configurations that might be useful.</p> <p></p> <p>We are only interested in the Pipeline tab at the end.</p> <p></p> <p>In the Pipeline definition, we are going to copy and paste the pipeline script that we have above into the Script section and hit save.</p> <p></p> <p>Next, we will select the \"Build Now\" option on the left side of the page.</p> <p></p> <p>You should now wait a short amount of time, less than a minute. and you should see under status the stages that we defined above in our script.</p> <p></p> <p>More importantly, if we now head on over to our DockerHub and check that we have a new build.</p> <p></p> <p>Overall did take a while to figure out but I wanted to stick with it to get hands-on and work through a scenario that anyone can run through using minikube and access to GitHub and dockerhub.</p> <p>The DockerHub repository I used for this demo was a private one. But in the next section, I want to advance some of these stages and have them do something vs just printing out <code>pwd</code> and running some tests and build stages.</p>"},{"location":"90DaysOfDevOps/day73/#resources","title":"Resources","text":"<ul> <li>Jenkins is the way to build, test, deploy</li> <li>Jenkins.io</li> <li>ArgoCD</li> <li>ArgoCD Tutorial for Beginners</li> <li>What is Jenkins?</li> <li>Complete Jenkins Tutorial</li> <li>GitHub Actions</li> <li>GitHub Actions CI/CD</li> </ul> <p>See you on Day 74</p>"},{"location":"90DaysOfDevOps/day74/","title":"#90DaysOfDevOps - Hello World - Jenkinsfile App Pipeline - Day 74","text":""},{"location":"90DaysOfDevOps/day74/#hello-world-jenkinsfile-app-pipeline","title":"Hello World - Jenkinsfile App Pipeline","text":"<p>In the last section, we built a simple Pipeline in Jenkins that would push our docker image from our dockerfile in a public GitHub repository to our private Dockerhub repository.</p> <p>In this section, we want to take this one step further and we want to achieve the following with our simple application.</p>"},{"location":"90DaysOfDevOps/day74/#objective","title":"Objective","text":"<ul> <li>Dockerfile (Hello World)</li> <li>Jenkinsfile</li> <li>Jenkins Pipeline to trigger when GitHub Repository is updated</li> <li>Use GitHub Repository as the source.</li> <li>Run - Clone/Get Repository, Build, Test, Deploy Stages</li> <li>Deploy to DockerHub with incremental version numbers</li> <li>Stretch Goal to deploy to our Kubernetes Cluster (This will involve another job and manifest repository using GitHub credentials)</li> </ul>"},{"location":"90DaysOfDevOps/day74/#step-one","title":"Step One","text":"<p>We have our GitHub repository This currently contains our Dockerfile and our index.html</p> <p></p> <p>With the above this is what we were using as our source in our Pipeline, now we want to add that Jenkins Pipeline script to our GitHub repository as well.</p> <p></p> <p>Now back in our Jenkins dashboard, we are going to create a new pipeline but now instead of pasting our script, we are going to use \"Pipeline script from SCM\" We are then going to use the configuration options below.</p> <p>For reference, we are going to use <code>https://github.com/MichaelCade/Jenkins-HelloWorld.git</code> as the repository URL.</p> <p></p> <p>We could at this point hit save and apply and we would then be able to manually run our Pipeline building our new Docker image that is uploaded to our DockerHub repository.</p> <p>However, I also want to make sure that we set a schedule that whenever our repository or our source code is changed, I want to trigger a build. we could use webhooks or we could use a scheduled pull.</p> <p>This is a big consideration because if you are using costly cloud resources to hold your pipeline and you have lots of changes to your code repository then you will incur a lot of costs. We know that this is a demo environment which is why I am using the \"poll scm\" option. (Also I believe that using minikube I am lacking the ability to use webhooks)</p> <p></p> <p>One thing I have changed since yesterday's session is I want to now upload my image to a public repository which in this case would be michaelcade1\\90DaysOfDevOps, my Jenkinsfile has this change already. And from the previous sections, I have removed any existing demo container images.</p> <p></p> <p>Going backwards here, we created our Pipeline and then as previously shown we added our configuration.</p> <p></p> <p>At this stage, our Pipeline has never run and your stage view will look something like this.</p> <p></p> <p>Now let's trigger the \"Build Now\" button. and our stage view will display our stages.</p> <p></p> <p>If we then head over to our DockerHub repository, we should have 2 new Docker images. We should have a Build ID of 1 and a latest because for every build that we create based on the \"Upload to DockerHub\" we send a version using the Jenkins Build_ID environment variable and we also issue a latest.</p> <p></p> <p>Let's go and create an update to our index.html file in our GitHub repository as per below, I will let you go and find out what version 1 of the index.html was saying.</p> <p></p> <p>If we head back to Jenkins and select \"Build Now\" again. We will see if our #2 build is successful.</p> <p></p> <p>Then a quick look at DockerHub, we can see that we have our tagged version 2 and our latest tag.</p> <p></p> <p>It is worth noting here that I have added into my Kubernetes cluster a secret that enables my access and authentication to push my docker builds into DockerHub. If you are following along you should repeat this process for your account, and also make a change to the Jenkinsfile that is associated with my repository and account.</p>"},{"location":"90DaysOfDevOps/day74/#resources","title":"Resources","text":"<ul> <li>Jenkins is the way to build, test, deploy</li> <li>Jenkins.io</li> <li>ArgoCD</li> <li>ArgoCD Tutorial for Beginners</li> <li>What is Jenkins?</li> <li>Complete Jenkins Tutorial</li> <li>GitHub Actions</li> <li>GitHub Actions CI/CD</li> </ul> <p>See you on Day 75</p>"},{"location":"90DaysOfDevOps/day75/","title":"#90DaysOfDevOps - GitHub Actions Overview - Day 75","text":""},{"location":"90DaysOfDevOps/day75/#github-actions-overview","title":"GitHub Actions Overview","text":"<p>In this section, I wanted to move on and take a look at maybe a different approach to what we just spent time on. GitHub Actions is what we will focus on in this session.</p> <p>GitHub Actions is a CI/CD platform that allows us to build, test and deploy amongst other tasks in our pipeline. It has the concept of workflows that build and test against a GitHub repository. You could also use GitHub Actions to drive other workflows based on events that happen within your repository.</p>"},{"location":"90DaysOfDevOps/day75/#workflows","title":"Workflows","text":"<p>Overall, in GitHub Actions, our task is called a Workflow.</p> <ul> <li>A workflow is the configurable automated process.</li> <li>Defined as YAML files.</li> <li>Contain and run one or more jobs</li> <li>Will run when triggered by an event in your repository or can be run manually</li> <li>You can multiple workflows per repository</li> <li>A workflow will contain a job and then steps to achieve that job</li> <li>Within our workflow we will also have a runner on which our workflow runs.</li> </ul> <p>For example, you can have one workflow to build and test pull requests, another workflow to deploy your application every time a release is created, and still another workflow that adds a label every time someone opens a new issue.</p>"},{"location":"90DaysOfDevOps/day75/#events","title":"Events","text":"<p>Events are specific event in a repository that triggers the workflow to run.</p>"},{"location":"90DaysOfDevOps/day75/#jobs","title":"Jobs","text":"<p>A job is a set of steps in the workflow that execute on a runner.</p>"},{"location":"90DaysOfDevOps/day75/#steps","title":"Steps","text":"<p>Each step within the job can be a shell script that gets executed or an action. Steps are executed in order and they are dependent on each other.</p>"},{"location":"90DaysOfDevOps/day75/#actions","title":"Actions","text":"<p>A repeatable custom application is used for frequently repeated tasks.</p>"},{"location":"90DaysOfDevOps/day75/#runners","title":"Runners","text":"<p>A runner is a server that runs the workflow, each runner runs a single job at a time. GitHub Actions provides the ability to run Ubuntu Linux, Microsoft Windows, and macOS runners. You can also host your own on a specific OS or hardware.</p> <p>Below you can see how this looks, we have our event triggering our workflow &gt; our workflow consists of two jobs &gt; within our jobs we then have steps and then we have actions.</p> <p></p>"},{"location":"90DaysOfDevOps/day75/#yaml","title":"YAML","text":"<p>Before we get going with a real use case let's take a quick look at the above image in the form of an example YAML file.</p> <p>I have added # to the comment where we can find the components of the YAML workflow.</p> <pre><code>#Workflow\nname: 90DaysOfDevOps\n#Event\non: [push]\n#Jobs\njobs:\n  check-bats-version:\n    #Runners\n    runs-on: ubuntu-latest\n    #Steps\n    steps:\n        #Actions\n      - uses: actions/checkout@v2\n      - uses: actions/setup-node@v2\n        with:\n          node-version: '14'\n      - run: npm install -g bats\n      - run: bats -v\n</code></pre>"},{"location":"90DaysOfDevOps/day75/#getting-hands-on-with-github-actions","title":"Getting Hands-On with GitHub Actions","text":"<p>I think there are a lot of options when it comes to GitHub Actions, yes it will satisfy your CI/CD needs when it comes to Building, Test, and Deploying your code and the continued steps thereafter.</p> <p>I can see lots of options and other automated tasks that we could use GitHub Actions for.</p>"},{"location":"90DaysOfDevOps/day75/#using-github-actions-for-linting-your-code","title":"Using GitHub Actions for Linting your code","text":"<p>One option is making sure your code is clean and tidy within your repository. This will be our first example demo.</p> <p>I am going to be using some example code linked in one of the resources for this section, we are going to use <code>GitHub/super-linter</code> to check against our code.</p> <pre><code>name: Super-Linter\n\non: push\n\njobs:\n  super-lint:\n    name: Lint code base\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v2\n\n      - name: Run Super-Linter\n        uses: github/super-linter@v3\n        env:\n          DEFAULT_BRANCH: main\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n</code></pre> <p>github/super-linter You can see from the above that for one of our steps we have an action called GitHub/super-linter and this is referring to a step that has already been written by the community. You can find out more about this here Super-Linter</p> <p>\"This repository is for the GitHub Action to run a Super-Linter. It is a simple combination of various linters, written in bash, to help validate your source code.\"</p> <p>Also in the code snippet above it mentions GITHUB_TOKEN so I was interested to find out why and what this does and is needed for.</p> <p>\"NOTE: If you pass the Environment variable <code>GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}</code> in your workflow, then the GitHub Super-Linter will mark the status of each linter run in the Checks section of a pull request. Without this, you will only see the overall status of the full run. There is no need to set the GitHub Secret as it is automatically set by GitHub, it only needs to be passed to the action.\"</p> <p>The bold text is important to note at this stage. We are using it but we do not need to set any environment variable within our repository.</p> <p>We will use the repository that we used in our Jenkins demo to test against.Jenkins-HelloWorld</p> <p>Here is our repository as we left it in the Jenkins sessions.</p> <p></p> <p>For us to take advantage, we have to use the Actions tab above to choose from the marketplace which I will cover shortly or we can create our files using our super-linter code above, to create your own, you must create a new file in your repository at this exact location. <code>.github/workflows/workflow_name</code> obviously making sure the workflow_name is something useful for you to recognise, within here we can have many different workflows performing different jobs and tasks against our repository.</p> <p>We are going to create <code>.github/workflows/super-linter.yml</code></p> <p></p> <p>We can then paste our code and commit the code to our repository, if we then head to the Actions tab we will now see our Super-Linter workflow listed below,</p> <p></p> <p>We defined in our code that this workflow would run when we pushed anything to our repository, so in pushing the super-linter.yml to our repository we triggered the workflow.</p> <p></p> <p>As you can see from the above we have some errors most likely with my hacking ability vs my coding ability.</p> <p>Although it was not my code at least not yet, in running this and getting an error I found this issue</p> <p>Take #2 I changed the version of Super-Linter from version 3 to 4 and have run the task again.</p> <p></p> <p>As expected my hacker coding brought up some issues and you can see them here in the workflow</p> <p>I wanted to show the look now on our repository when something within the workflow has failed or reported back an error.</p> <p></p> <p>Now if we resolve the issue with my code and push the changes our workflow will run again (you can see from the image it took a while to iron out our \"bugs\") Deleting a file is probably not recommended but it is a very quick way to show the issue being resolved.</p> <p></p> <p>If you hit the new workflow button highlighted above, this is going to open the door to a huge plethora of actions. One thing you might have noticed throughout this challenge is that we don't want to reinvent the wheel we want to stand on the shoulders of giants and share our code, automation and skills far and wide to make our lives easier.</p> <p></p> <p>Oh, I didn't show you the green tick on the repository when our workflow was successful.</p> <p></p> <p>I think that covers things from a foundational point of view for GitHub Actions but if you are anything like me then you are probably seeing how else GitHub Actions can be used to automate a lot of tasks.</p> <p>Next up we will cover another area of CD, we will be looking into ArgoCD to deploy our applications out into our environments.</p>"},{"location":"90DaysOfDevOps/day75/#resources","title":"Resources","text":"<ul> <li>Jenkins is the way to build, test, deploy</li> <li>Jenkins.io</li> <li>ArgoCD</li> <li>ArgoCD Tutorial for Beginners</li> <li>What is Jenkins?</li> <li>Complete Jenkins Tutorial</li> <li>GitHub Actions</li> <li>GitHub Actions CI/CD</li> </ul> <p>See you on Day 76</p>"},{"location":"90DaysOfDevOps/day76/","title":"#90DaysOfDevOps - ArgoCD Overview - Day 76","text":""},{"location":"90DaysOfDevOps/day76/#argocd-overview","title":"ArgoCD Overview","text":"<p>\u201cArgo CD is a declarative, GitOps continuous delivery tool for Kubernetes\u201d</p> <p>Version control is the key here, ever made a change to your environment on the fly and have no recollection of that change and because the lights are on and everything is green you continue to keep plodding along? Ever made a change and broken everything or some of everything? You might have known you made the change and you can quickly roll back your change, that bad script or misspelling. Now ever done this on a massive scale and maybe it was not you or maybe it was not found straight away and now the business is suffering. Therefore, version control is important. Not only that but \u201cApplication definitions, configurations, and environments should be declarative, and version controlled.\u201d On top of this (which comes from ArgoCD), they also mention that \u201cApplication deployment and lifecycle management should be automated, auditable, and easy to understand.\u201d</p> <p>From an Operations background but having played a lot around Infrastructure as Code this is the next step to ensuring all of that good stuff is taken care of along the way with continuous deployment/delivery workflows.</p> <p>What is ArgoCD</p>"},{"location":"90DaysOfDevOps/day76/#deploying-argocd","title":"Deploying ArgoCD","text":"<p>We are going to be using our trusty minikube Kubernetes cluster locally again for this deployment.</p> <pre><code>kubectl create namespace argocd\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\n</code></pre> <p></p> <p>Make sure all the ArgoCD pods are up and running with <code>kubectl get pods -n argocd</code></p> <p></p> <p>Also, let's check everything that we deployed in the namespace with <code>kubectl get all -n argocd</code></p> <p></p> <p>When the above is looking good, we then should consider accessing this via the port forward. Using the <code>kubectl port-forward svc/argocd-server -n argocd 8080:443</code> command. Do this in a new terminal.</p> <p>Then open a new web browser and head to <code>https://localhost:8080</code></p> <p></p> <p>To log in you will need a username of admin and then grab your created secret as your password use the <code>kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d &amp;&amp; echo</code></p> <p></p> <p>Once you have logged in you will have your blank CD canvas.</p> <p></p>"},{"location":"90DaysOfDevOps/day76/#deploying-our-application","title":"Deploying our application","text":"<p>Now we have ArgoCD up and running we can now start using it to deploy our applications from our Git repositories as well as Helm.</p> <p>The application I want to deploy is Pac-Man, yes that's right the famous game and something I use in a lot of demos when it comes to data management, this will not be the last time we see Pac-Man.</p> <p>You can find the repository for Pac-Man here.</p> <p>Instead of going through each step using screenshots, I thought it would be easier to create a walkthrough video covering the steps taken for this one particular application deployment.</p> <p>ArgoCD Demo - 90DaysOfDevOps</p> <p>Note - During the video, there is a service that is never satisfied as the app health is healthy this is because the LoadBalancer type set for the Pacman service is pending, in Minikube we do not have a loadbalancer configured. If you would like to test this you could change the YAML for the service to ClusterIP and use port forwarding to play the game.</p> <p>This wraps up the CICD Pipelines section, I feel there is a lot of focus on this area in the industry at the moment and you will also hear terms around GitOps also related to the methodologies used within CICD in general.</p> <p>The next section we move into is around Observability, another concept or area that is not new but it is more and more important as we look at our environments differently.</p>"},{"location":"90DaysOfDevOps/day76/#resources","title":"Resources","text":"<ul> <li>Jenkins is the way to build, test, deploy</li> <li>Jenkins.io</li> <li>ArgoCD</li> <li>ArgoCD Tutorial for Beginners</li> <li>What is Jenkins?</li> <li>Complete Jenkins Tutorial</li> <li>GitHub Actions</li> <li>GitHub Actions CI/CD</li> </ul> <p>See you on Day 77</p>"},{"location":"90DaysOfDevOps/day77/","title":"#90DaysOfDevOps - The Big Picture: Monitoring - Day 77","text":""},{"location":"90DaysOfDevOps/day77/#the-big-picture-monitoring","title":"The Big Picture: Monitoring","text":"<p>In this section we are going to talk about monitoring, what is it and why do we need it?</p>"},{"location":"90DaysOfDevOps/day77/#what-is-monitoring","title":"What is Monitoring?","text":"<p>Monitoring is the process of keeping a close eye on the entire infrastructure</p>"},{"location":"90DaysOfDevOps/day77/#and-why-do-we-need-it","title":"and why do we need it?","text":"<p>Let's assume we're managing a thousand servers these include a variety of specialised servers like application servers, database servers and web servers. We could also complicate this further with additional services and different platforms including public cloud offerings and Kubernetes.</p> <p></p> <p>We are responsible for ensuring that all the services, applications and resources on the servers are running as they should be.</p> <p></p> <p>How do we do it? there are three ways:</p> <ul> <li>Login manually to all of our servers and check all the data about service processes and resources.</li> <li>Write a script that logs in to the servers for us and checks on the data.</li> </ul> <p>Both of these options would require a considerable amount of work on our part,</p> <p>The third option is easier, we could use a monitoring solution that is available in the market.</p> <p>Nagios and Zabbix are possible solutions that are readily available which allow us to upscale our monitoring infrastructure to include as many servers as we want.</p>"},{"location":"90DaysOfDevOps/day77/#nagios","title":"Nagios","text":"<p>Nagios is an infrastructure monitoring tool that is made by a company that goes by the same name. The open-source version of this tool is called Nagios core while the commercial version is called Nagios XI. Nagios Website</p> <p>The tool allows us to monitor our servers and see if they are being sufficiently utilised or if there are any tasks of failure that need addressing.</p> <p></p> <p>Essentially monitoring allows us to achieve these two goals, check the status of our servers and services and determine the health of our infrastructure it also gives us a 40,000ft view of the complete infrastructure to see if our servers are up and running if the applications are working properly and the web servers are reachable or not.</p> <p>It will tell us that our disk has been increasing by 10 per cent for the last 10 weeks in a particular server, that it will exhaust entirely within the next four or five days and we'll fail to respond soon it will alert us when your disk or server is in a critical state so that we can take appropriate actions to avoid possible outages.</p> <p>In this case, we can free up some disk space and ensure that our servers don't fail and that our users are not affected.</p> <p>The difficult question for most monitoring engineers is what do we monitor? and alternately what do we not?</p> <p>Every system has several resources, which of these should we keep a close eye on and which ones can we turn a blind eye to for instance is it necessary to monitor CPU usage the answer is yes obviously nevertheless it is still a decision that has to be made is it necessary to monitor the number of open ports in the system we may or may not have to depend on the situation if it is a general-purpose server we probably won't have to but then again if it is a webserver we probably would have to.</p>"},{"location":"90DaysOfDevOps/day77/#continuous-monitoring","title":"Continuous Monitoring","text":"<p>Monitoring is not a new item and even continuous monitoring has been an ideal that many enterprises have adopted for many years.</p> <p>There are three key areas of focus when it comes to monitoring.</p> <ul> <li>Infrastructure Monitoring</li> <li>Application Monitoring</li> <li>Network Monitoring</li> </ul> <p>The important thing to note is that there are many tools available we have mentioned two generic systems and tools in this session but there are lots. The real benefit of a monitoring solution comes when you have spent the time making sure you are answering the question of what should we be monitoring and what shouldn't we?</p> <p>We could turn on a monitoring solution in any of our platforms and it will start grabbing information but if that information is simply too much then you are going to struggle to benefit from that solution, you have to spend the time to configure it.</p> <p>In the next session, we will get hands-on with a monitoring tool and see what we can start monitoring.</p>"},{"location":"90DaysOfDevOps/day77/#resources","title":"Resources","text":"<ul> <li>The Importance of Monitoring in DevOps</li> <li>Understanding Continuous Monitoring in DevOps?</li> <li>DevOps Monitoring Tools</li> <li>Top 5 - DevOps Monitoring Tools</li> <li>How Prometheus Monitoring works</li> <li>Introduction to Prometheus monitoring</li> </ul> <p>See you on Day 78</p>"},{"location":"90DaysOfDevOps/day78/","title":"#90DaysOfDevOps - Hands-On Monitoring Tools - Day 78","text":""},{"location":"90DaysOfDevOps/day78/#hands-on-monitoring-tools","title":"Hands-On Monitoring Tools","text":"<p>In the last session, I spoke about the big picture of monitoring and I took a look into Nagios, there were two reasons for doing this. The first was this is a piece of software I have heard a lot of over the years so wanted to know a little more about its capabilities.</p> <p>Today I am going to be going into Prometheus, I have seen more and more of Prometheus in the Cloud-Native landscape but it can also be used to look after those physical resources as well outside of Kubernetes and the like.</p>"},{"location":"90DaysOfDevOps/day78/#prometheus-monitors-nearly-everything","title":"Prometheus - Monitors nearly everything","text":"<p>First of all, Prometheus is Open-Source that can help you monitor containers and microservice-based systems as well as physical, virtual and other services. There is a large community behind Prometheus.</p> <p>Prometheus has a large array of integrations and exporters The key is to export existing metrics as Prometheus metrics. On top of this, it also supports multiple programming languages.</p> <p>Pull approach - If you are talking to thousands of microservices or systems and services a push method is going to be where you generally see the service pushing to the monitoring system. This brings some challenges around flooding the network, high CPU and also a single point of failure. Where Pull gives us a much better experience where Prometheus will pull from the metrics endpoint on every service.</p> <p>Once again we see YAML for configuration for Prometheus.</p> <p></p> <p>Later on, you are going to see how this looks when deployed into Kubernetes, in particular, we have the PushGateway which pulls our metrics from our jobs/exporters.</p> <p>We have the AlertManager which pushes alerts and this is where we can integrate into external services such as email, slack and other tooling.</p> <p>Then we have the Prometheus server which manages the retrieval of those pull metrics from the PushGateway and then sends those push alerts to the AlertManager. The Prometheus server also stores data on a local disk. Although can leverage remote storage solutions.</p> <p>We then also have PromQL which is the language used to interact with the metrics, this can be seen later on with the Prometheus Web UI but you will also see later on in this section how this is also used within Data visualisation tools such as Grafana.</p>"},{"location":"90DaysOfDevOps/day78/#ways-to-deploy-prometheus","title":"Ways to Deploy Prometheus","text":"<p>Various ways of installing Prometheus, Download Section Docker images are also available.</p> <p><code>docker run --name prometheus -d -p 127.0.0.1:9090:9090 prom/prometheus</code></p> <p>But we are going to focus our efforts on deploying to Kubernetes. Which also has some options.</p> <ul> <li>Create configuration YAML files</li> <li>Using an Operator (manager of all Prometheus components)</li> <li>Using helm chart to deploy operator</li> </ul>"},{"location":"90DaysOfDevOps/day78/#deploying-to-kubernetes","title":"Deploying to Kubernetes","text":"<p>We will be using our minikube cluster locally again for this quick and simple installation. As with previous touch points with minikube, we will be using helm to deploy the Prometheus helm chart.</p> <p><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts</code></p> <p></p> <p>As you can see from the above we have also run a helm repo update, we are now ready to deploy Prometheus into our minikube environment using the <code>helm install stable prometheus-community/prometheus</code> command.</p> <p></p> <p>After a couple of minutes, you will see several new pods appear, for this demo, I have deployed into the default namespace, I would normally push this to its namespace.</p> <p></p> <p>Once all the pods are running we can also take a look at all the deployed aspects of Prometheus.</p> <p></p> <p>Now for us to access the Prometheus Server UI we can use the following command to port forward.</p> <pre><code>export POD_NAME=$(kubectl get pods --namespace default -l \"app=prometheus,component=server\" -o jsonpath=\"{.items[0].metadata.name}\")\n  kubectl --namespace default port-forward $POD_NAME 9090\n</code></pre> <p>When we first open our browser to <code>http://localhost:9090</code> we see the following very blank screen.</p> <p></p> <p>Because we have deployed to our Kubernetes cluster we will automatically be picking up metrics from our Kubernetes API so we can use some PromQL to at least make sure we are capturing metrics <code>container_cpu_usage_seconds_total</code></p> <p></p> <p>Short on learning PromQL and putting that into practice this is very much like I mentioned previously in that gaining metrics is great, and so is monitoring but you have to know what you are monitoring and why and what you are not monitoring and why!</p> <p>I want to come back to Prometheus but for now, I think we need to think about Log Management and Data Visualisation to bring us back to Prometheus later on.</p>"},{"location":"90DaysOfDevOps/day78/#resources","title":"Resources","text":"<ul> <li>The Importance of Monitoring in DevOps</li> <li>Understanding Continuous Monitoring in DevOps?</li> <li>DevOps Monitoring Tools</li> <li>Top 5 - DevOps Monitoring Tools</li> <li>How Prometheus Monitoring works</li> <li>Introduction to Prometheus monitoring</li> <li>Promql cheat sheet with examples</li> </ul> <p>See you on Day 79</p>"},{"location":"90DaysOfDevOps/day79/","title":"#90DaysOfDevOps - The Big Picture: Log Management - Day 79","text":""},{"location":"90DaysOfDevOps/day79/#the-big-picture-log-management","title":"The Big Picture: Log Management","text":"<p>A continuation of the infrastructure monitoring challenges and solutions, log management is another puzzle piece to the overall observability jigsaw.</p>"},{"location":"90DaysOfDevOps/day79/#log-management-aggregation","title":"Log Management &amp; Aggregation","text":"<p>Let's talk about two core concepts the first of which is log aggregation and it's a way of collecting and tagging application logs from many different services and to a single dashboard that can easily be searched.</p> <p>One of the first systems that have to be built out in an application performance management system is log aggregation. Application performance management is the part of the DevOps lifecycle where things have been built and deployed and you need to make sure that they're continuously working so they have enough resources allocated to them and errors aren't being shown to users. In most production deployments many related events emit logs across services at google a single search might hit ten different services before being returned to the user if you got unexpected search results that might mean a logic problem in any of the ten services and log aggregation helps companies like google diagnose problems in production, they've built a single dashboard where they can map every request to unique id so if you search something your search will get a unique id and then every time that search is passing through a different service that service will connect that id to what they're currently doing.</p> <p>This is the essence of a good log aggregation platform efficiently collects logs from everywhere that emits them and makes them easily searchable in the case of a fault again.</p>"},{"location":"90DaysOfDevOps/day79/#example-app","title":"Example App","text":"<p>Our example application is a web app, we have a typical front end and backend storing our critical data in a MongoDB database.</p> <p>If a user told us the page turned all white and printed an error message we would be hard-pressed to diagnose the problem with our current stack the user would need to manually send us the error and we'd need to match it with relevant logs in the other three services.</p>"},{"location":"90DaysOfDevOps/day79/#elk","title":"ELK","text":"<p>Let's take a look at ELK, a popular open source log aggregation stack named after its three components elasticsearch, logstash and kibana if we installed it in the same environment as our example app.</p> <p>The web application would connect to the frontend which then connects to the backend, the backend would send logs to logstash and then the way that these three components work</p>"},{"location":"90DaysOfDevOps/day79/#the-components-of-elk","title":"The components of elk","text":"<p>Elasticsearch, logstash and Kibana are that all the services send logs to logstash, logstash takes these logs which are text emitted by the application. For example, in the web application when you visit a web page, the web page might log this visitor's access to this page at this time and that's an example of a log message those logs would be sent to logstash.</p> <p>Logstash would then extract things from them so for that log message user did thing, at time. It would extract the time and extract the message and extract the user and include those all as tags so the message would be an object of tags and message so that you could search them easily could find all of the requests made by a specific user but logstash doesn't store things itself it stores things in elasticsearch which is an efficient database for querying text and elasticsearch exposes the results as Kibana and Kibana is a web server that connects to elasticsearch and allows administrators as the DevOps person or other people on your team, the on-call engineer to view the logs in production whenever there's a major fault. You as the administrator would connect to Kibana, and Kibana would query elasticsearch for logs matching whatever you wanted.</p> <p>You could say hey Kibana in the search bar I want to find errors and kibana would say elasticsearch find the messages which contain the string error and then elasticsearch would return results that had been populated by logstash. Logstash would have been sent those results from all of the other services.</p>"},{"location":"90DaysOfDevOps/day79/#how-would-we-use-elk-to-diagnose-a-production-problem","title":"how would we use elk to diagnose a production problem","text":"<p>A user says I saw error code one two three four five six seven when I tried to do this with elk setup we'd have to go to kibana enter one two three four five six seven in the search bar press enter and then that would show us the logs that corresponded to that and one of the logs might say internal server error returning one two three four five six seven and we'd see that the service that emitted that log was the backend and we'd see what time that log was emitted at so we could go to the time in that log and we could look at the messages above and below it in the backend and then we could see a better picture of what happened for the user's request and we'd be able to repeat this process going to other services until we found what caused the problem for the user.</p>"},{"location":"90DaysOfDevOps/day79/#security-and-access-to-logs","title":"Security and Access to Logs","text":"<p>An important piece of the puzzle is ensuring that logs are only visible to administrators (or the users and groups that need to have access), logs can contain sensitive information like tokens only authenticated users should have access to them, you wouldn't want to expose Kibana to the internet without some way of authenticating.</p>"},{"location":"90DaysOfDevOps/day79/#examples-of-log-management-tools","title":"Examples of Log Management Tools","text":"<p>Examples of log management platforms there's</p> <ul> <li>Elasticsearch</li> <li>Logstash</li> <li>Kibana</li> <li>Fluentd - popular open source choice</li> <li>Datadog - hosted offering, commonly used at larger enterprises,</li> <li>LogDNA - hosted offering</li> <li>Splunk</li> </ul> <p>Cloud providers also provide logging such as AWS CloudWatch Logs, Microsoft Azure Monitor and Google Cloud Logging.</p> <p>Log Management is a key aspect of the overall observability of your applications and infrastructure environment for diagnosing problems in production it's relatively simple to install a turnkey solution like ELK or CloudWatch and it makes diagnosing and triaging problems in production significantly easier.</p>"},{"location":"90DaysOfDevOps/day79/#resources","title":"Resources","text":"<ul> <li>The Importance of Monitoring in DevOps</li> <li>Understanding Continuous Monitoring in DevOps?</li> <li>DevOps Monitoring Tools</li> <li>Top 5 - DevOps Monitoring Tools</li> <li>How Prometheus Monitoring works</li> <li>Introduction to Prometheus monitoring</li> <li>Promql cheat sheet with examples</li> <li>Log Management for DevOps | Manage application, server, and cloud logs with Site24x7</li> <li>Log Management what DevOps need to know</li> <li>What is ELK Stack?</li> <li>Fluentd simply explained</li> </ul> <p>See you on Day 80</p>"},{"location":"90DaysOfDevOps/day80/","title":"#90DaysOfDevOps - ELK Stack - Day 80","text":""},{"location":"90DaysOfDevOps/day80/#elk-stack","title":"ELK Stack","text":"<p>In this session, we are going to get a little more hands-on with some of the options we have mentioned.</p> <p>ELK Stack is the combination of 3 separate tools:</p> <ul> <li> <p>Elasticsearch is a distributed, free and open search and analytics engine for all types of data, including textual, numerical, geospatial, structured, and unstructured.</p> </li> <li> <p>Logstash is a free and open server-side data processing pipeline that ingests data from a multitude of sources, transforms it, and then sends it to your favourite \"stash.\"</p> </li> <li> <p>Kibana is a free and open user interface that lets you visualize your Elasticsearch data and navigate the Elastic Stack. Do anything from tracking query load to understanding the way requests flow through your apps.</p> </li> </ul> <p>ELK stack lets us reliably and securely take data from any source, in any format, then search, analyze, and visualize it in real time.</p> <p>On top of the above-mentioned components, you might also see Beats which are lightweight agents that are installed on edge hosts to collect different types of data for forwarding into the stack.</p> <ul> <li> <p>Logs: Server logs that need to be analysed are identified</p> </li> <li> <p>Logstash: Collect logs and events data. It even parses and transforms data</p> </li> <li> <p>ElasticSearch: The transformed data from Logstash is Store, Search, and indexed.</p> </li> <li> <p>Kibana uses Elasticsearch DB to Explore, Visualize, and Share</p> </li> </ul> <p></p> <p>Picture taken from Guru99</p> <p>A good resource explaining this Is the Complete Guide to the ELK Stack</p> <p>With the addition of beats, the ELK Stack is also now known as Elastic Stack.</p> <p>For the hands-on scenario, there are many places you can deploy the Elastic Stack but we are going to be using docker-compose to deploy locally on our system.</p> <p>Start the Elastic Stack with Docker Compose</p> <p></p> <p>You will find the original files and walkthrough that I used here deviantony/docker-elk</p> <p>Now we can run <code>docker-compose up -d</code>, the first time this has been running will require the pulling of images.</p> <p></p> <p>If you follow either this repository or the one that I used you will have either the password of \"changeme\" or in my repository the password of \"90DaysOfDevOps\". The username is \"elastic\"</p> <p>After a few minutes, we can navigate to <code>http://localhost:5601/</code> which is our Kibana server / Docker container.</p> <p></p> <p>Your initial home screen is going to look something like this.</p> <p></p> <p>Under the section titled \"Get started by adding integrations\" there is a \"try sample data\" click this and we can add one of the shown below.</p> <p></p> <p>I am going to select \"Sample weblogs\" but this is really to get a look and feel of what data sets you can get into the ELK stack.</p> <p>When you have selected \"Add Data\" it takes a while to populate some of that data and then you have the \"View Data\" option and a list of the available ways to view that data in the dropdown.</p> <p></p> <p>As it states on the dashboard view:</p> <p>Sample Logs Data</p> <p>This dashboard contains sample data for you to play with. You can view it, search it, and interact with the visualizations. For more information about Kibana, check our docs.</p> <p></p> <p>This is using Kibana to visualise data that has been added into ElasticSearch via Logstash. This is not the only option but I wanted to deploy and look at this.</p> <p>We are going to cover Grafana at some point and you are going to see some data visualisation similarities between the two, you have also seen Prometheus.</p> <p>The key takeaway I have had between the Elastic Stack and Prometheus + Grafana is that Elastic Stack or ELK Stack is focused on Logs and Prometheus is focused on metrics.</p> <p>I was reading this article from MetricFire Prometheus vs. ELK to get a better understanding of the different offerings.</p>"},{"location":"90DaysOfDevOps/day80/#resources","title":"Resources","text":"<ul> <li>Understanding Logging: Containers &amp; Microservices</li> <li>The Importance of Monitoring in DevOps</li> <li>Understanding Continuous Monitoring in DevOps?</li> <li>DevOps Monitoring Tools</li> <li>Top 5 - DevOps Monitoring Tools</li> <li>How Prometheus Monitoring works</li> <li>Introduction to Prometheus monitoring</li> <li>Promql cheat sheet with examples</li> <li>Log Management for DevOps | Manage application, server, and cloud logs with Site24x7</li> <li>Log Management what DevOps need to know</li> <li>What is ELK Stack?</li> <li>Fluentd simply explained</li> </ul> <p>See you on Day 81</p>"},{"location":"90DaysOfDevOps/day81/","title":"#90DaysOfDevOps - Fluentd & FluentBit - Day 81","text":""},{"location":"90DaysOfDevOps/day81/#fluentd-fluentbit","title":"Fluentd &amp; FluentBit","text":"<p>Another data collector that I wanted to explore as part of this observability section was Fluentd. An Open-Source unified logging layer.</p> <p>Fluentd has four key features that make it suitable to build clean, reliable logging pipelines:</p> <p>Unified Logging with JSON: Fluentd tries to structure data as JSON as much as possible. This allows Fluentd to unify all facets of processing log data: collecting, filtering, buffering, and outputting logs across multiple sources and destinations. The downstream data processing is much easier with JSON since it has enough structure to be accessible without forcing rigid schemas.</p> <p>Pluggable Architecture: Fluentd has a flexible plugin system that allows the community to extend its functionality. Over 300 community-contributed plugins connect dozens of data sources to dozens of data outputs, manipulating the data as needed. By using plugins, you can make better use of your logs right away.</p> <p>Minimum Resources Required: A data collector should be lightweight so that it runs comfortably on a busy machine. Fluentd is written in a combination of C and Ruby and requires minimal system resources. The vanilla instance runs on 30-40MB of memory and can process 13,000 events/second/core.</p> <p>Built-in Reliability: Data loss should never happen. Fluentd supports memory- and file-based buffering to prevent inter-node data loss. Fluentd also supports robust failover and can be set up for high availability.</p> <p>Installing Fluentd</p>"},{"location":"90DaysOfDevOps/day81/#how-do-apps-log-data","title":"How do apps log data?","text":"<ul> <li>Write to files. <code>.log</code> files (difficult to analyse without a tool and at scale)</li> <li>Log directly to a database (each application must be configured with the correct format)</li> <li>Third-party applications (NodeJS, NGINX, PostgreSQL)</li> </ul> <p>This is why we want a unified logging layer.</p> <p>FluentD allows for the 3 logging data types shown above and gives us the ability to collect, process and send those to a destination, this could be sending them logs to Elastic, MongoDB, or Kafka databases for example.</p> <p>Any Data, Any Data source can be sent to FluentD and that can be sent to any destination. FluentD is not tied to any particular source or destination.</p> <p>In my research of Fluentd, I kept stumbling across Fluent bit as another option and it looks like if you were looking to deploy a logging tool into your Kubernetes environment then fluent bit would give you that capability, even though fluentd can also be deployed to containers as well as servers.</p> <p>Fluentd &amp; Fluent Bit</p> <p>Fluentd and Fluentbit will use the input plugins to transform that data to Fluent Bit format, then we have output plugins to whatever that output target is such as elasticsearch.</p> <p>We can also use tags and matches between configurations.</p> <p>I cannot see a good reason for using fluentd and it seems that Fluent Bit is the best way to get started. Although they can be used together in some architectures.</p>"},{"location":"90DaysOfDevOps/day81/#fluent-bit-in-kubernetes","title":"Fluent Bit in Kubernetes","text":"<p>Fluent Bit in Kubernetes is deployed as a DaemonSet, which means it will run on each node in the cluster. Each Fluent Bit pod on each node will then read each container on that node and gather all of the logs available. It will also gather the metadata from the Kubernetes API Server.</p> <p>Kubernetes annotations can be used within the configuration YAML of our applications.</p> <p>First of all, we can deploy from the fluent helm repository. <code>helm repo add fluent https://fluent.github.io/helm-charts</code> and then install using the <code>helm install fluent-bit fluent/fluent-bit</code> command.</p> <p></p> <p>In my cluster, I am also running Prometheus in my default namespace (for test purposes) we need to make sure our fluent-bit pod is up and running. we can do this using <code>kubectl get all | grep fluent</code> this is going to show us our running pod, service and daemonset that we mentioned earlier.</p> <p></p> <p>So that fluentbit knows where to get logs from we have a configuration file, in this Kubernetes deployment of fluentbit, we have a configmap which resembles the configuration file.</p> <p></p> <p>That ConfigMap will look something like:</p> <pre><code>Name:         fluent-bit\nNamespace:    default\nLabels:       app.kubernetes.io/instance=fluent-bit\n              app.kubernetes.io/managed-by=Helm\n              app.kubernetes.io/name=fluent-bit\n              app.kubernetes.io/version=1.8.14\n              helm.sh/chart=fluent-bit-0.19.21\nAnnotations:  meta.helm.sh/release-name: fluent-bit\n              meta.helm.sh/release-namespace: default\n\nData\n====\ncustom_parsers.conf:\n----\n[PARSER]\n    Name docker_no_time\n    Format json\n    Time_Keep Off\n    Time_Key time\n    Time_Format %Y-%m-%dT%H:%M:%S.%L\n\nfluent-bit.conf:\n----\n[SERVICE]\n    Daemon Off\n    Flush 1\n    Log_Level info\n    Parsers_File parsers.conf\n    Parsers_File custom_parsers.conf\n    HTTP_Server On\n    HTTP_Listen 0.0.0.0\n    HTTP_Port 2020\n    Health_Check On\n\n[INPUT]\n    Name tail\n    Path /var/log/containers/*.log\n    multiline.parser docker, cri\n    Tag kube.*\n    Mem_Buf_Limit 5MB\n    Skip_Long_Lines On\n\n[INPUT]\n    Name systemd\n    Tag host.*\n    Systemd_Filter _SYSTEMD_UNIT=kubelet.service\n    Read_From_Tail On\n\n[FILTER]\n    Name Kubernetes\n    Match kube.*\n    Merge_Log On\n    Keep_Log Off\n    K8S-Logging.Parser On\n    K8S-Logging.Exclude On\n\n[OUTPUT]\n    Name es\n    Match kube.*\n    Host elasticsearch-master\n    Logstash_Format On\n    Retry_Limit False\n\n[OUTPUT]\n    Name es\n    Match host.*\n    Host elasticsearch-master\n    Logstash_Format On\n    Logstash_Prefix node\n    Retry_Limit False\n\nEvents:  &lt;none&gt;\n</code></pre> <p>We can now port-forward our pod to our localhost to ensure that we have connectivity. Firstly get the name of your pod with <code>kubectl get pods | grep fluent</code> and then use <code>kubectl port-forward fluent-bit-8kvl4 2020:2020</code> to open a web browser to <code>http://localhost:2020/</code></p> <p></p> <p>I also found this great medium article covering more about Fluent Bit</p>"},{"location":"90DaysOfDevOps/day81/#resources","title":"Resources","text":"<ul> <li>Understanding Logging: Containers &amp; Microservices</li> <li>The Importance of Monitoring in DevOps</li> <li>Understanding Continuous Monitoring in DevOps?</li> <li>DevOps Monitoring Tools</li> <li>Top 5 - DevOps Monitoring Tools</li> <li>How Prometheus Monitoring works</li> <li>Introduction to Prometheus monitoring</li> <li>Promql cheat sheet with examples</li> <li>Log Management for DevOps | Manage application, server, and cloud logs with Site24x7</li> <li>Log Management what DevOps need to know</li> <li>What is ELK Stack?</li> <li>Fluentd simply explained</li> <li>Fluent Bit explained | Fluent Bit vs Fluentd</li> </ul> <p>See you on Day 82</p>"},{"location":"90DaysOfDevOps/day82/","title":"#90DaysOfDevOps - EFK Stack - Day 82","text":""},{"location":"90DaysOfDevOps/day82/#efk-stack","title":"EFK Stack","text":"<p>In the previous section, we spoke about ELK Stack, which uses Logstash as the log collector in the stack, in the EFK Stack we are swapping that out for FluentD or FluentBit.</p> <p>Our mission in this section is to monitor our Kubernetes logs using EFK.</p>"},{"location":"90DaysOfDevOps/day82/#overview-of-efk","title":"Overview of EFK","text":"<p>We will be deploying the following into our Kubernetes cluster.</p> <p></p> <p>The EFK stack is a collection of 3 software bundled together, including:</p> <ul> <li> <p>Elasticsearch: NoSQL database is used to store data and provides an interface for searching and query logs.</p> </li> <li> <p>Fluentd: Fluentd is an open source data collector for a unified logging layer. Fluentd allows you to unify data collection and consumption for better use and understanding of data.</p> </li> <li> <p>Kibana: Interface for managing and statistics logs. Responsible for reading information from elasticsearch.</p> </li> </ul>"},{"location":"90DaysOfDevOps/day82/#deploying-efk-on-minikube","title":"Deploying EFK on Minikube","text":"<p>We will be using our trusty minikube cluster to deploy our EFK stack. Let's start a cluster using <code>minikube start</code> on our system. I am using a Windows OS with WSL2 enabled.</p> <p></p> <p>I have created efk-stack.yaml which contains everything we need to deploy the EFK stack into our cluster, using the <code>kubectl create -f efk-stack.yaml</code> command we can see everything being deployed.</p> <p></p> <p>Depending on your system and if you have run this already and have images pulled you should now watch the pods into a ready state before we can move on, you can check the progress with the following command. <code>kubectl get pods -n kube-logging -w</code> This can take a few minutes.</p> <p></p> <p>The above command lets us keep an eye on things but I like to clarify that things are all good by just running the following <code>kubectl get pods -n kube-logging</code> command to ensure all pods are now up and running.</p> <p></p> <p>Once we have all our pods up and running and at this stage, we should see</p> <ul> <li>3 pods associated with ElasticSearch</li> <li>1 pod associated with Fluentd</li> <li>1 pod associated with Kibana</li> </ul> <p>We can also use <code>kubectl get all -n kube-logging</code> to show all in our namespace, fluentd as explained previously is deployed as a daemonset, kibana as deployment and Elasticsearch as a statefulset.</p> <p></p> <p>Now all of our pods are up and running we can now issue in a new terminal the port-forward command so that we can access our kibana dashboard. Note that your pod name will be different to the command we see here. <code>kubectl port-forward kibana-84cf7f59c-v2l8v 5601:5601 -n kube-logging</code></p> <p></p> <p>We can now open up a browser and navigate to this address, <code>http://localhost:5601</code> you will be greeted with either the screen you see below or you might indeed see a sample data screen or continue and configure yourself. Either way and by all means look at that test data, it is what we covered when we looked at the ELK stack in a previous session.</p> <p></p> <p>Next, we need to hit the \"discover\" tab on the left menu and add \"*\" to our index pattern. Continue to the next step by hitting \"Next step\".</p> <p></p> <p>In Step 2 of 2, we are going to use the @timestamp option from the dropdown as this will filter our data by time. When you hit create pattern it might take a few seconds to complete.</p> <p></p> <p>If we now head back to our \"discover\" tab after a few seconds you should start to see data coming in from your Kubernetes cluster.</p> <p></p> <p>Now that we have the EFK stack up and running and we are gathering logs from our Kubernetes cluster via Fluentd we can also take a look at other sources we can choose from if you navigate to the home screen by hitting the Kibana logo on the top left you will be greeted with the same page we saw when we first logged in.</p> <p>We can add APM, Log data, metric data and security events from other plugins or sources.</p> <p></p> <p>If we select \"Add log data\" then we can see below that we have a lot of choices on where we want to get our logs from, you can see that Logstash is mentioned there which is part of the ELK stack.</p> <p></p> <p>Under the metrics data, you will find that you can add sources for Prometheus and lots of other services.</p>"},{"location":"90DaysOfDevOps/day82/#apm-application-performance-monitoring","title":"APM (Application Performance Monitoring)","text":"<p>There is also the option to gather APM (Application Performance Monitoring) which collects in-depth performance metrics and errors from inside your application. It allows you to monitor the performance of thousands of applications in real time.</p> <p>I am not going to get into APM here but you can find out more on the Elastic site</p>"},{"location":"90DaysOfDevOps/day82/#resources","title":"Resources","text":"<ul> <li>Understanding Logging: Containers &amp; Microservices</li> <li>The Importance of Monitoring in DevOps</li> <li>Understanding Continuous Monitoring in DevOps?</li> <li>DevOps Monitoring Tools</li> <li>Top 5 - DevOps Monitoring Tools</li> <li>How Prometheus Monitoring works</li> <li>Introduction to Prometheus monitoring</li> <li>Promql cheat sheet with examples</li> <li>Log Management for DevOps | Manage application, server, and cloud logs with Site24x7</li> <li>Log Management what DevOps need to know</li> <li>What is ELK Stack?</li> <li>Fluentd simply explained</li> </ul> <p>See you on Day 83</p>"},{"location":"90DaysOfDevOps/day83/","title":"#90DaysOfDevOps - Data Visualisation - Grafana - Day 83","text":""},{"location":"90DaysOfDevOps/day83/#data-visualisation-grafana","title":"Data Visualisation - Grafana","text":"<p>We saw a lot of Kibana over this section around Observability. But we have to also take some time to cover Grafana. But also they are not the same and they are not completely competing against each other.</p> <p>Kibana\u2019s core feature is data querying and analysis. Using various methods, users can search the data indexed in Elasticsearch for specific events or strings within their data for root cause analysis and diagnostics. Based on these queries, users can use Kibana\u2019s visualisation features which allow users to visualize data in a variety of different ways, using charts, tables, geographical maps and other types of visualizations.</p> <p>Grafana started as a fork of Kibana, Grafana had an aim to supply support for metrics aka monitoring, which at that time Kibana did not provide.</p> <p>Grafana is a free and Open-Source data visualisation tool. We commonly see Prometheus and Grafana together out in the field but we might also see Grafana alongside Elasticsearch and Graphite.</p> <p>The key difference between the two tools is Logging vs Monitoring, we started the section off covering monitoring with Nagios and then into Prometheus before moving into Logging where we covered the ELK and EFK stacks.</p> <p>Grafana caters to analysing and visualising metrics such as system CPU, memory, disk and I/O utilisation. The platform does not allow full-text data querying. Kibana runs on top of Elasticsearch and is used primarily for analyzing log messages.</p> <p>As we have already discovered with Kibana it is quite easy to deploy as well as having the choice of where to deploy, this is the same for Grafana.</p> <p>Both support installation on Linux, Mac, Windows, Docker or building from source.</p> <p>There are no doubt others but Grafana is a tool that I have seen spanning the virtual, cloud and cloud-native platforms so I wanted to cover this here in this section.</p>"},{"location":"90DaysOfDevOps/day83/#prometheus-operator-grafana-deployment","title":"Prometheus Operator + Grafana Deployment","text":"<p>We have covered Prometheus already in this section but as we see these paired so often I wanted to spin up an environment that would allow us to at least see what metrics we could have displayed in a visualisation. We know that monitoring our environments is important but going through those metrics alone in Prometheus or any metric tool is going to be cumbersome and it is not going to scale. This is where Grafana comes in and provides us with that interactive visualisation of those metrics collected and stored in the Prometheus database.</p> <p>With that visualisation, we can create custom charts, graphs and alerts for our environment. In this walkthrough, we will be using our minikube cluster.</p> <p>We are going to start by cloning this down to our local system. Using <code>git clone https://github.com/prometheus-operator/kube-prometheus.git</code> and <code>cd kube-prometheus</code></p> <p></p> <p>The first job is to create our namespace within our minikube cluster <code>kubectl create -f manifests/setup</code> if you have not been following along in previous sections we can use <code>minikube start</code> to bring up a new cluster here.</p> <p></p> <p>Next, we are going to deploy everything we need for our demo using the <code>kubectl create -f manifests/</code> command, as you can see this is going to deploy a lot of different resources within our cluster.</p> <p></p> <p>We then need to wait for our pods to come up and being in the running state we can use the <code>kubectl get pods -n monitoring -w</code> command to keep an eye on the pods.</p> <p></p> <p>When everything is running we can check all pods are in a running and healthy state using the <code>kubectl get pods -n monitoring</code> command.</p> <p></p> <p>With the deployment, we deployed several services that we are going to be using later on in the demo you can check these by using the <code>kubectl get svc -n monitoring</code> command.</p> <p></p> <p>And finally, let's check on all resources deployed in our new monitoring namespace using the <code>kubectl get all -n monitoring</code> command.</p> <p></p> <p>Opening a new terminal we are now ready to access our Grafana tool and start gathering and visualising some of our metrics, the command to use is<code>kubectl --namespace monitoring port-forward svc/grafana 3000</code></p> <p></p> <p>Open a browser and navigate to http://localhost:3000 you will be prompted for a username and password.</p> <p> The default username and password to access is</p> <pre><code>Username: admin\nPassword: admin\n</code></pre> <p>However, you will be asked to provide a new password at first login. The initial screen or home page you will see will give you some areas to explore as well as some useful resources to get up to speed with Grafana and its capabilities. Notice the \"Add your first data source\" and \"create your first dashboard\" widgets we will be using later.</p> <p></p> <p>You will find that there is already a prometheus data source already added to our Grafana data sources, however, because we are using minikube we need to also port forward prometheus so that this is available on our localhost, opening a new terminal we can run the following command. <code>kubectl --namespace monitoring port-forward svc/prometheus-k8s 9090</code> if on the home page of Grafana we now enter into the widget \"Add your first data source\" and from here we are going to select Prometheus.</p> <p></p> <p>For our new data source, we can use the address http://localhost:9090 and we will also need to change the dropdown to the browser as highlighted below.</p> <p></p> <p>At the bottom of the page, we can now hit save and test. This should give us the outcome you see below if the port forward for Prometheus is working.</p> <p></p> <p>Head back to the home page and find the option to \"Create your first dashboard\" select \"Add a new panel\"</p> <p></p> <p>You will see from below that we are already gathering from our Grafana data source, but we would like to gather metrics from our Prometheus data source, select the data source drop down and select our newly created \"Prometheus-1\"</p> <p></p> <p>If you then select the Metrics browser you will have a long list of metrics being gathered from Prometheus related to our minikube cluster.</p> <p></p> <p>For the demo I am going to find a metric that gives us some output around our system resources, <code>cluster:node_cpu:ratio{}</code> gives us some detail on the nodes in our cluster and proves that this integration is working.</p> <p></p> <p>Once you are happy with this as your visualisation then you can hit the apply button in the top right and you will then add this graph to your dashboard. You can go ahead and add additional graphs and other charts to give you the visuals that you need.</p> <p></p> <p>We can however take advantage of thousands of previously created dashboards that we can use so that we do not need to reinvent the wheel.</p> <p></p> <p>If we search Kubernetes we will see a long list of pre-built dashboards that we can choose from.</p> <p></p> <p>We have chosen the Kubernetes API Server dashboard and changed the data source to suit our newly added Prometheus-1 data source and we get to see some of the metrics displayed below.</p> <p></p>"},{"location":"90DaysOfDevOps/day83/#alerting","title":"Alerting","text":"<p>You could also leverage the alertmanager that we deployed to then send alerts out to slack or other integrations, to do this you would need to port forward the alertmanager service using the below details.</p> <p><code>kubectl --namespace monitoring port-forward svc/alertmanager-main 9093</code> <code>http://localhost:9093</code></p> <p>That wraps up our section on all things observability, I have personally found that this section has highlighted how broad this topic is but equally how important this is for our roles and that be it metrics, logging or tracing you are going to need to have a good idea of what is happening in our broad environments moving forward, especially when they can change so dramatically with all the automation that we have already covered in the other sections.</p> <p>Next up we are going to be taking a look into data management and how DevOps principles also need to be considered when it comes to Data Management.</p>"},{"location":"90DaysOfDevOps/day83/#resources","title":"Resources","text":"<ul> <li>Understanding Logging: Containers &amp; Microservices</li> <li>The Importance of Monitoring in DevOps</li> <li>Understanding Continuous Monitoring in DevOps?</li> <li>DevOps Monitoring Tools</li> <li>Top 5 - DevOps Monitoring Tools</li> <li>How Prometheus Monitoring works</li> <li>Introduction to Prometheus monitoring</li> <li>Promql cheat sheet with examples</li> <li>Log Management for DevOps | Manage application, server, and cloud logs with Site24x7</li> <li>Log Management what DevOps need to know</li> <li>What is ELK Stack?</li> <li>Fluentd simply explained</li> </ul> <p>See you on Day 84</p>"},{"location":"90DaysOfDevOps/day84/","title":"#90DaysOfDevOps - The Big Picture: Data Management - Day 84","text":""},{"location":"90DaysOfDevOps/day84/#the-big-picture-data-management","title":"The Big Picture: Data Management","text":"<p>Data Management is by no means a new wall to climb, although we do know that data is more important than it may be was a few years ago. Valuable and ever-changing it can also be a massive nightmare when we are talking about automation and continuously integrating, testing and deploying frequent software releases. Enter the persistent data and underlying data services are often the main culprit when things go wrong.</p> <p>But before I get into Cloud-Native Data Management, we need to go up a level. We have touched on many different platforms throughout this challenge. Be it Physical, Virtual, Cloud or Cloud-Native obviously including Kubernetes there is none of these platforms that provide the lack of requirement for data management.</p> <p>Whatever our business it is more than likely you will find a database lurking in the environment somewhere, be it for the most mission-critical system in the business or at least some cog in the chain is storing that persistent data on some level of the system.</p>"},{"location":"90DaysOfDevOps/day84/#devops-and-data","title":"DevOps and Data","text":"<p>Much like the very start of this series where we spoke about the DevOps principles, for a better process when it comes to data you have to include the right people. This might be the DBAs but equally, that is going to include people that care about the backup of those data services as well.</p> <p>Secondly, we also need to identify the different data types, domains, and boundaries that we have associated with our data. This way it is not just dealt with in a silo approach amongst Database administrators, storage engineers or Backup focused engineers. This way the whole team can determine the best route of action when it comes to developing and hosting applications for the wider business and focus on the data architecture vs it being an afterthought.</p> <p>Now, this can span many different areas of the data lifecycle, we could be talking about data ingest, where and how will data be ingested into our service or application? How will the service, application or users access this data? But then it also requires us to understand how we will secure the data and then how will we protect that data.</p>"},{"location":"90DaysOfDevOps/day84/#data-management-101","title":"Data Management 101","text":"<p>Data management according to the Data Management Body of Knowledge is \u201cthe development, execution and supervision of plans, policies, programs and practices that control, protect, deliver and enhance the value of data and information assets.\u201d</p> <ul> <li> <p>Data is the most important aspect of your business - Data is only one part of your overall business. I have seen the term \"Data is the lifeblood of our business\" and most likely true. This then got me thinking about blood being pretty important to the body but alone it is nothing we still need the aspects of the body to make the blood something other than a liquid.</p> </li> <li> <p>Data quality is more important than ever - We are having to treat data as a business asset, meaning that we have to give it the considerations it needs and requires to work with our automation and DevOps principles.</p> </li> <li> <p>Accessing data in a timely fashion - Nobody has the patience to not have access to the right data at the right time to make effective decisions. Data must be available in a streamlined and timely manner regardless of presentation.</p> </li> <li> <p>Data Management has to be an enabler to DevOps - I mentioned streamlining previously, we have to include the data management requirements into our cycle and ensure not just availability of that data but also include other important policy-based protection of those data points along with fully tested recovery models with that as well.</p> </li> </ul>"},{"location":"90DaysOfDevOps/day84/#dataops","title":"DataOps","text":"<p>Both DataOps and DevOps apply the best practices of technology development and operations to improve quality, increase speed, reduce security threats, delight customers and provide meaningful and challenging work for skilled professionals. DevOps and DataOps share goals to accelerate product delivery by automating as many process steps as possible. For DataOps, the objective is a resilient data pipeline and trusted insights from data analytics.</p> <p>Some of the most common higher-level areas that focus on DataOps are going to be Machine Learning, Big Data and Data Analytics including Artificial Intelligence.</p>"},{"location":"90DaysOfDevOps/day84/#data-management-is-the-management-of-information","title":"Data Management is the management of information","text":"<p>My focus throughout this section is not going to be getting into Machine Learning or Artificial Intelligence but focus on the protecting the data from a data protection point of view, the title of this subsection is \"Data management is the management of information\" and we can relate that information = data.</p> <p>Three key areas that we should consider along this journey with data are:</p> <ul> <li>Accuracy - Making sure that production data is accurate, equally we need to ensure that our data in the form of backups are also working and tested against recovery to be sure if a failure or a reason comes up we need to be able to get back up and running as fast as possible.</li> <li> <p>Consistent - If our data services span multiple locations then for production we need to make sure we have consistency across all data locations so that we are getting accurate data, this also spans into data protection when it comes to protecting these data services, especially data services we need to ensure consistency at different levels to make sure we are taking a good clean copy of that data for our backups, replicas etc.</p> </li> <li> <p>Secure - Access Control but equally just keeping data, in general, is a topical theme at the moment across the globe. Making sure the right people have access to your data is paramount, again this leads to data protection where we must make sure that only the required personnel have access to backups and the ability to restore from those as well clone and provide other versions of the business data.</p> </li> </ul> <p>Better Data = Better Decisions</p>"},{"location":"90DaysOfDevOps/day84/#data-management-days","title":"Data Management Days","text":"<p>During the next 6 sessions we are going to be taking a closer look at Databases, Backup &amp; Recovery, Disaster Recovery, and Application Mobility all with an element of demo and hands-on throughout.</p>"},{"location":"90DaysOfDevOps/day84/#resources","title":"Resources","text":"<ul> <li>Kubernetes Backup and Restore made easy!</li> <li>Kubernetes Backups, Upgrades, Migrations - with Velero</li> <li>7 Database Paradigms</li> <li>Disaster Recovery vs. Backup: What's the difference?</li> <li>Veeam Portability &amp; Cloud Mobility</li> </ul> <p>See you on Day 85</p>"},{"location":"90DaysOfDevOps/day85/","title":"#90DaysOfDevOps - Data Services - Day 85","text":""},{"location":"90DaysOfDevOps/day85/#data-services","title":"Data Services","text":"<p>Databases are going to be the most common data service that we come across in our environments. I wanted to take this session to explore some of those different types of Databases and some of the use cases they each have. Some we have used and seen throughout the challenge.</p> <p>From an application development point of view choosing the right data service or database is going to be a huge decision when it comes to the performance and scalability of your application.</p> <p>https://www.youtube.com/watch?v=W2Z7fbCLSTw</p>"},{"location":"90DaysOfDevOps/day85/#key-value","title":"Key-value","text":"<p>A key-value database is a type of nonrelational database that uses a simple key-value method to store data. A key-value database stores data as a collection of key-value pairs in which a key serves as a unique identifier. Both keys and values can be anything, ranging from simple objects to complex compound objects. Key-value databases are highly partitionable and allow horizontal scaling at scales that other types of databases cannot achieve.</p> <p>An example of a Key-Value database is Redis.</p> <p>Redis is an in-memory data structure store, used as a distributed, in-memory key\u2013value database, cache and message broker, with optional durability. Redis supports different kinds of abstract data structures, such as strings, lists, maps, sets, sorted sets, HyperLogLogs, bitmaps, streams, and spatial indices.</p> <p></p> <p>As you can see from the description of Redis this means that our database is fast but we are limited on space as a trade-off. Also, no queries or joins which means data modelling options are very limited.</p> <p>Best for:</p> <ul> <li>Caching</li> <li>Pub/Sub</li> <li>Leaderboards</li> <li>Shopping carts</li> </ul> <p>Generally used as a cache above another persistent data layer.</p>"},{"location":"90DaysOfDevOps/day85/#wide-column","title":"Wide Column","text":"<p>A wide-column database is a NoSQL database that organises data storage into flexible columns that can be spread across multiple servers or database nodes, using multi-dimensional mapping to reference data by column, row, and timestamp.</p> <p>Cassandra is a free and open-source, distributed, wide-column store, NoSQL database management system designed to handle large amounts of data across many commodity servers, providing high availability with no single point of failure.</p> <p></p> <p>No schema which means can handle unstructured data however this can be seen as a benefit to some workloads.</p> <p>Best for:</p> <ul> <li>Time-Series</li> <li>Historical Records</li> <li>High-Write, Low-Read</li> </ul>"},{"location":"90DaysOfDevOps/day85/#document","title":"Document","text":"<p>A document database (also known as a document-oriented database or a document store) is a database that stores information in documents.</p> <p>MongoDB is a source-available cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with optional schemas. MongoDB is developed by MongoDB Inc. and licensed under the Server Side Public License.</p> <p></p> <p>NoSQL document databases allow businesses to store simple data without using complex SQL codes. Quickly store with no compromise to reliability.</p> <p>Best for:</p> <ul> <li>Most Applications</li> <li>Games</li> <li>Internet of Things</li> </ul>"},{"location":"90DaysOfDevOps/day85/#relational","title":"Relational","text":"<p>If you are new to databases but you know of them I guess that you have come across a relational database.</p> <p>A relational database is a digital database based on the relational model of data, as proposed by E. F. Codd in 1970. A system used to maintain relational databases is a relational database management system. Many relational database systems have the option of using SQL for querying and maintaining the database.</p> <p>MySQL is an open-source relational database management system. Its name is a combination of \"My\", the name of co-founder Michael Widenius's daughter, and \"SQL\", the abbreviation for Structured Query Language.</p> <p>MySQL is one example of a relational database there are lots of other options.</p> <p></p> <p>Whilst researching relational databases the term or abbreviation ACID has been mentioned a lot, (atomicity, consistency, isolation, durability) is a set of properties of database transactions intended to guarantee data validity despite errors, power failures, and other mishaps. In the context of databases, a sequence of database operations that satisfies the ACID properties (which can be perceived as a single logical operation on the data) is called a transaction. For example, a transfer of funds from one bank account to another, even involving multiple changes such as debiting one account and crediting another, is a single transaction.</p> <p>Best for:</p> <ul> <li>Most Applications (It has been around for years, doesn't mean it is the best)</li> </ul> <p>It is not ideal for unstructured data or the ability to scale is where some of the other NoSQL mentions give a better ability to scale for certain workloads.</p>"},{"location":"90DaysOfDevOps/day85/#graph","title":"Graph","text":"<p>A graph database stores nodes and relationships instead of tables, or documents. Data is stored just like you might sketch ideas on a whiteboard. Your data is stored without restricting it to a pre-defined model, allowing a very flexible way of thinking about and using it.</p> <p>Neo4j is a graph database management system developed by Neo4j, Inc. Described by its developers as an ACID-compliant transactional database with native graph storage and processing</p> <p>Best for:</p> <ul> <li>Graphs</li> <li>Knowledge Graphs</li> <li>Recommendation Engines</li> </ul>"},{"location":"90DaysOfDevOps/day85/#search-engine","title":"Search Engine","text":"<p>In the last section, we used a Search Engine database in the way of Elasticsearch.</p> <p>A search-engine database is a type of non-relational database that is dedicated to the search for data content. Search-engine databases use indexes to categorise similar characteristics among data and facilitate search capability.</p> <p>Elasticsearch is a search engine based on the Lucene library. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents.</p> <p>Best for:</p> <ul> <li>Search Engines</li> <li>Typeahead</li> <li>Log search</li> </ul>"},{"location":"90DaysOfDevOps/day85/#multi-model","title":"Multi-model","text":"<p>A multi-model database is a database management system designed to support multiple data models against a single, integrated backend. In contrast, most database management systems are organized around a single data model that determines how data can be organized, stored, and manipulated. Document, graph, relational, and key\u2013value models are examples of data models that may be supported by a multi-model database.</p> <p>Fauna is a flexible, developer-friendly, transactional database delivered as a secure and scalable cloud API with native GraphQL.</p> <p>Best for:</p> <ul> <li>You are not stuck on having to choose a data model</li> <li>ACID Compliant</li> <li>Fast</li> <li>No provisioning overhead</li> <li>How do you want to consume your data and let the cloud do the heavy lifting</li> </ul> <p>That is going to wrap up this database overview session, no matter what industry you are in you are going to come across one area of databases. We are then going to take some of these examples and look at the data management and in particular the protection and storing of these data services later on in the section.</p> <p>There are a ton of resources I have linked below, you could honestly spend 90 years probably deep diving into all database types and everything that comes with this.</p>"},{"location":"90DaysOfDevOps/day85/#resources","title":"Resources","text":"<ul> <li>Redis Crash Course - the What, Why and How to use Redis as your primary database</li> <li>Redis: How to setup a cluster - for beginners</li> <li>Redis on Kubernetes for beginners</li> <li>Intro to Cassandra - Cassandra Fundamentals</li> <li>MongoDB Crash Course</li> <li>MongoDB in 100 Seconds</li> <li>What is a Relational Database?</li> <li>Learn PostgreSQL Tutorial - Full Course for Beginners</li> <li>MySQL Tutorial for Beginners [Full Course]</li> <li>What is a graph database? (in 10 minutes)</li> <li>What is Elasticsearch?</li> <li>FaunaDB Basics - The Database of your Dreams</li> <li>Fauna Crash Course - Covering the Basics</li> </ul> <p>See you on Day 86</p>"},{"location":"90DaysOfDevOps/day86/","title":"#90DaysOfDevOps - Backup all the platforms - Day 86","text":""},{"location":"90DaysOfDevOps/day86/#backup-all-the-platforms","title":"Backup all the platforms","text":"<p>During this whole challenge, we discussed many different platforms and environments. One thing all of those have in common is the fact they all need some level of data protection!</p> <p>Data Protection has been around for many many years but the wealth of data that we have today and the value that this data brings means we have to make sure we are not only resilient to infrastructure failure by having multiple nodes and high availability across applications but we must also consider that we need a copy of that data, that important data in a safe and secure location if a failure scenario was to occur.</p> <p>We hear a lot these days it seems about cybercrime and ransomware, and don't get me wrong this is a massive threat and I stand by the fact that you will be attacked by ransomware. It is not a matter of if it is a matter of when. So even more reason to make sure you have your data secure for when that time arises. However, the most common cause for data loss is not ransomware or cybercrime it is simply accidental deletion!</p> <p>We have all done it, deleted something we shouldn't have and had that instant regret.</p> <p>With all of the technology and automation we have discussed during the challenge, the requirement to protect any stateful data or even complex stateless configuration is still there, regardless of the platform.</p> <p></p> <p>But we should be able to perform that protection of the data with automation in mind and be able to integrate it into our workflows.</p> <p>If we look at what backup is:</p> <p>In information technology, a backup, or data backup is a copy of computer data taken and stored elsewhere so that it may be used to restore the original after a data loss event. The verb form, referring to the process of doing so, is \"back up\", whereas the noun and adjective form is \"backup\".</p> <p>If we break this down to the simplest form, a backup is a copy and paste of data to a new location. Simply put I could take a backup right now by copying a file from my C: drive to my D: drive and I would then have a copy in case something happened to the C: drive or something was edited wrongly within the files. I could revert to the copy I have on the D: drive. Now if my computer dies where both the C &amp; D drives live then I am not protected so I have to consider a solution or a copy of data outside of my system maybe onto a NAS drive in my house? But then what happens if something happens to my house, maybe I need to consider storing it on another system in another location, maybe the cloud is an option. Maybe I could store a copy of my important files in several locations to mitigate the risk of failure?</p>"},{"location":"90DaysOfDevOps/day86/#3-2-1-backup-methodolgy","title":"3-2-1 Backup Methodolgy","text":"<p>Now seems a good time to talk about the 3-2-1 rule or backup methodology. I did a lightning talk covering this topic.</p> <p>We have already mentioned before some of the extreme ends of why we need to protect our data but a few more are listed below:</p> <p></p> <p>Which then allows me to talk about the 3-2-1 methodology. My first copy or backup of my data should be as close to my production system as possible, the reason for this is based on speed to recovery and again going back to that original point about accidental deletion this is going to be the most common reason for recovery. But I want to be storing that on a suitable second media outside of the original or production system.</p> <p>We then want to make sure we also send a copy of our data external or offsite this is where a second location comes in be it another house, building, data centre or the public cloud.</p> <p></p>"},{"location":"90DaysOfDevOps/day86/#backup-responsibility","title":"Backup Responsibility","text":"<p>We have most likely heard all of the myths when it comes to not having to backup, things like \"Everything is stateless\" I mean if everything is stateless then what is the business? no databases? word documents? There is a level of responsibility on every individual within the business to ensure they are protected but it is going to come down most likely to the operations teams to provide the backup process for the mission-critical applications and data.</p> <p>Another good one is that \"High availability is my backup, we have built in multiple nodes into our cluster there is no way this is going down!\" apart from when you make a mistake to the database and this is replicated over all the nodes in the cluster, or there is fire, flood or blood scenario that means the cluster is no longer available and with it the important data. It's not about being stubborn it is about being aware of the data and the services, absolutely everyone should factor in high availability and fault tolerance into their architecture but that does not substitute the need for backup!</p> <p>Replication can also seem to give us the offsite copy of the data and maybe that cluster mentioned above does live across multiple locations, however, the first accidental mistake would still be replicated there. But again a Backup requirement should stand alongside application replication or system replication within the environment.</p> <p>Now with all this said you can go to the extreme on the other end as well and send copies of data to too many locations which is going to not only cost but also increase the risk of being attacked as your surface area is now massively expanded.</p> <p>Anyway, who looks after backup? It will be different within each business but someone should be taking it upon themselves to understand the backup requirements. But also understand the recovery plan!</p>"},{"location":"90DaysOfDevOps/day86/#nobody-cares-till-everybody-cares","title":"Nobody cares till everybody cares","text":"<p>Backup is a prime example, nobody cares about backup until you need to restore something. Alongside the requirement to back our data up we also need to consider how we restore!</p> <p>With our text document example, we are talking about very small files so the ability to copy back and forth is easy and fast. But if we are talking about 100GB plus files then this is going to take time. Also, we have to consider the level at which we need to recover if we take a virtual machine for example.</p> <p>We have the whole Virtual Machine, we have the Operating System, Application installation and then if this is a database server we will have some database files as well. If we have made a mistake and inserted the wrong line of code into our database I probably don't need to restore the whole virtual machine, I want to be granular on what I recover back.</p>"},{"location":"90DaysOfDevOps/day86/#backup-scenario","title":"Backup Scenario","text":"<p>I want to now start building on a scenario to protect some data, specifically, I want to protect some files on my local machine (in this case Windows but the tool I am going to use is not only free and open-source but also cross-platform) I would like to make sure they are protected to a NAS device I have locally in my home but also into an Object Storage bucket in the cloud.</p> <p>I want to back up this important data, it just so happens to be the repository for the 90DaysOfDevOps, which yes is also being sent to GitHub which is probably where you are reading this now but what if my machine was to die and GitHub was down? How would anyone be able to read the content but also how would I potentially be able to restore that data to another service?</p> <p></p> <p>There are lots of tools that can help us achieve this but I am going to be using a tool called Kopia an Open-Source backup tool which will enable us to encrypt, dedupe and compress our backups whilst being able to send them to many locations.</p> <p>You will find the releases to download here at the time of writing I will be using v0.10.6.</p>"},{"location":"90DaysOfDevOps/day86/#installing-kopia","title":"Installing Kopia","text":"<p>There is a Kopia CLI and GUI, we will be using the GUI but know that you can have a CLI version of this as well for those Linux servers that do not give you a GUI.</p> <p>I will be using <code>KopiaUI-Setup-0.10.6.exe</code></p> <p>Really quick next next installation and then when you open the application you are greeted with the choice of selecting the storage type that you wish to use as your backup repository.</p> <p></p>"},{"location":"90DaysOfDevOps/day86/#setting-up-a-repository","title":"Setting up a Repository","text":"<p>Firstly we would like to set up a repository using our local NAS device and we are going to do this using SMB, but we could also use NFS I believe.</p> <p></p> <p>On the next screen, we are going to define a password, this password is used to encrypt the repository contents.</p> <p></p> <p>Now that we have the repository configured we can trigger an ad-hoc snapshot to start writing data to it.</p> <p></p> <p>First up we need to enter a path to what we want to snapshot and in our case we want to take a copy of our <code>90DaysOfDevOps</code> folder. We will get back to the scheduling aspect shortly.</p> <p></p> <p>We can define our snapshot retention.</p> <p></p> <p>Maybe there are files or file types that we wish to exclude.</p> <p></p> <p>If we wanted to define a schedule we could do this on this next screen, when you first create this snapshot this is the opening page to define.</p> <p></p> <p>And you will see several other settings that can be handled here.</p> <p></p> <p>Select snapshot now and the data will be written to your repository.</p> <p></p>"},{"location":"90DaysOfDevOps/day86/#offsite-backup-to-s3","title":"Offsite backup to S3","text":"<p>With Kopia we can through the UI it seems only to have one repository configured at a time. But through the UI we can be creative and have multiple repository configuration files to choose from to achieve our goal of having a copy local and offsite in Object Storage.</p> <p>The Object Storage I am choosing to send my data to is going to Google Cloud Storage. I firstly logged into my Google Cloud Platform account and created a storage bucket. I already had the Google Cloud SDK installed on my system but running the <code>gcloud auth application-default login</code> authenticated me with my account.</p> <p></p> <p>I then used the CLI of Kopia to show me the current status of my repository after we added our SMB repository in the previous steps. I did this using the <code>\"C:\\Program Files\\KopiaUI\\resources\\server\\kopia.exe\" --config-file=C:\\Users\\micha\\AppData\\Roaming\\kopia\\repository.config repository status</code> command.</p> <p></p> <p>We are now ready to replace for the demo the configuration for the repository, what we would probably do if we wanted a long-term solution to hit both of these repositories is we would create an <code>smb.config</code> file and a <code>object.config</code> file and be able to run both of these commands to send our copies of data to each location. To add our repository we ran <code>\"C:\\Program Files\\KopiaUI\\resources\\server\\kopia.exe\" --config-file=C:\\Users\\micha\\AppData\\Roaming\\kopia\\repository.config repository create gcs --bucket 90daysofdevops</code></p> <p>The above command is taking into account that the Google Cloud Storage bucket we created is called <code>90daysofdevops</code></p> <p></p> <p>Now that we have created our new repository we can then run the <code>\"C:\\Program Files\\KopiaUI\\resources\\server\\kopia.exe\" --config-file=C:\\Users\\micha\\AppData\\Roaming\\kopia\\repository.config repository status</code> command again and will now show the GCS repository configuration.</p> <p></p> <p>The next thing we need to do is create a snapshot and send that to our newly created repository. Using the <code>\"C:\\Program Files\\KopiaUI\\resources\\server\\kopia.exe\" --config-file=C:\\Users\\micha\\AppData\\Roaming\\kopia\\repository.config kopia snapshot create \"C:\\Users\\micha\\demo\\90DaysOfDevOps\"</code> command we can kick off this process. You can see in the below browser that our Google Cloud Storage bucket now has kopia files based on our backup in place.</p> <p></p> <p>With the above process we can settle our requirement of sending our important data to 2 different locations, 1 of which is offsite in Google Cloud Storage and of course we still have our production copy of our data on a different media type.</p>"},{"location":"90DaysOfDevOps/day86/#restore","title":"Restore","text":"<p>Restore is another consideration and is very important, Kopia gives us the capability to not only restore to the existing location but also a new location.</p> <p>If we run the command <code>\"C:\\Program Files\\KopiaUI\\resources\\server\\kopia.exe\" --config-file=C:\\Users\\micha\\AppData\\Roaming\\kopia\\repository.config snapshot list</code> this will list the snapshots that we have currently in our configured repository (GCS)</p> <p></p> <p>We can then mount those snapshots directly from GCS using the <code>\"C:\\Program Files\\KopiaUI\\resources\\server\\kopia.exe\" --config-file=C:\\Users\\micha\\AppData\\Roaming\\kopia\\repository.config mount all Z:</code> command.</p> <p></p> <p>We could also restore the snapshot contents using <code>kopia snapshot restore kdbd9dff738996cfe7bcf99b45314e193</code></p> <p>The commands above are very long and this is because I was using the KopiaUI version of the kopia.exe as explained at the top of the walkthrough you can download the kopia.exe and put it into a path so you can just use the <code>kopia</code> command.</p> <p>In the next session, we will be focusing on protecting workloads within Kubernetes.</p>"},{"location":"90DaysOfDevOps/day86/#resources","title":"Resources","text":"<ul> <li>Kubernetes Backup and Restore made easy!</li> <li>Kubernetes Backups, Upgrades, Migrations - with Velero</li> <li>7 Database Paradigms</li> <li>Disaster Recovery vs. Backup: What's the difference?</li> <li>Veeam Portability &amp; Cloud Mobility</li> </ul> <p>See you on Day 87</p>"},{"location":"90DaysOfDevOps/day87/","title":"#90DaysOfDevOps - Hands-On Backup & Recovery - Day 87","text":""},{"location":"90DaysOfDevOps/day87/#hands-on-backup-recovery","title":"Hands-On Backup &amp; Recovery","text":"<p>In the last session, we touched on Kopia an Open-Source backup tool that we used to get some important data off to a local NAS and off to some cloud-based object storage.</p> <p>In this section, I want to get into the world of Kubernetes backup. It is a platform we covered The Big Picture: Kubernetes earlier in the challenge.</p> <p>We will again be using our minikube cluster but this time we are going to take advantage of some of those addons that are available.</p>"},{"location":"90DaysOfDevOps/day87/#kubernetes-cluster-setup","title":"Kubernetes cluster setup","text":"<p>To set up our minikube cluster we will be issuing the <code>minikube start --addons volumesnapshots,csi-hostpath-driver --apiserver-port=6443 --container-runtime=containerd -p 90daysofdevops --kubernetes-version=1.21.2</code> you will notice that we are using the <code>volumesnapshots</code> and <code>csi-hostpath-driver</code> as we will make full use of these for when we are taking our backups.</p> <p>At this point I know we have not deployed Kasten K10 yet but we want to issue the following command when your cluster is up, we want to annotate the volumesnapshotclass so that Kasten K10 can use this.</p> <pre><code>kubectl annotate volumesnapshotclass csi-hostpath-snapclass \\\n    k10.kasten.io/is-snapshot-class=true\n</code></pre> <p>We are also going to change over the default storageclass from the standard default storageclass to the csi-hostpath storageclass using the following.</p> <pre><code>kubectl patch storageclass csi-hostpath-sc -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'\n\nkubectl patch storageclass standard -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"false\"}}}'\n</code></pre> <p></p>"},{"location":"90DaysOfDevOps/day87/#deploy-kasten-k10","title":"Deploy Kasten K10","text":"<p>Add the Kasten Helm repository</p> <p><code>helm repo add kasten https://charts.kasten.io/</code></p> <p>We could use <code>arkade kasten install k10</code> here as well but for the demo, we will run through the following steps. More Details</p> <p>Create the namespace and deploy K10, note that this will take around 5 mins</p> <p><code>helm install k10 kasten/k10 --namespace=kasten-io --set auth.tokenAuth.enabled=true --set injectKanisterSidecar.enabled=true --set-string injectKanisterSidecar.namespaceSelector.matchLabels.k10/injectKanisterSidecar=true --create-namespace</code></p> <p></p> <p>You can watch the pods come up by running the following command.</p> <p><code>kubectl get pods -n kasten-io -w</code></p> <p></p> <p>Port forward to access the K10 dashboard, open a new terminal to run the below command</p> <p><code>kubectl --namespace kasten-io port-forward service/gateway 8080:8000</code></p> <p>The Kasten dashboard will be available at <code>http://127.0.0.1:8080/k10/#/</code></p> <p></p> <p>To authenticate with the dashboard we now need the token which we can get with the following commands.</p> <pre><code>TOKEN_NAME=$(kubectl get secret --namespace kasten-io|grep k10-k10-token | cut -d \" \" -f 1)\nTOKEN=$(kubectl get secret --namespace kasten-io $TOKEN_NAME -o jsonpath=\"{.data.token}\" | base64 --decode)\n\necho \"Token value: \"\necho $TOKEN\n</code></pre> <p></p> <p>Now we take this token and we input that into our browser, you will then be prompted for an email and company name.</p> <p></p> <p>Then we get access to the Kasten K10 dashboard.</p> <p></p>"},{"location":"90DaysOfDevOps/day87/#deploy-our-stateful-application","title":"Deploy our stateful application","text":"<p>Use the stateful application that we used in the Kubernetes section.</p> <p></p> <p>You can find the YAML configuration file for this application here-&gt; pacman-stateful-demo.yaml</p> <p></p> <p>We can use <code>kubectl get all -n pacman</code> to check on our pods coming up.</p> <p></p> <p>In a new terminal we can then port forward the pacman front end. <code>kubectl port-forward svc/pacman 9090:80 -n pacman</code></p> <p>Open another tab on your browser to http://localhost:9090/</p> <p></p> <p>Take the time to clock up some high scores in the backend MongoDB database.</p> <p></p>"},{"location":"90DaysOfDevOps/day87/#protect-our-high-scores","title":"Protect our High Scores","text":"<p>Now we have some mission-critical data in our database and we do not want to lose it. We can use Kasten K10 to protect this whole application.</p> <p>If we head back into the Kasten K10 dashboard tab you will see that our number of applications has now increased from 1 to 2 with the addition of our Pacman application to our Kubernetes cluster.</p> <p></p> <p>If you click on the Applications card you will see the automatically discovered applications in our cluster.</p> <p></p> <p>With Kasten K10 we can leverage storage-based snapshots as well export our copies out to object storage options.</p> <p>For the demo, we will create a manual storage snapshot in our cluster and then we can add some rogue data to our high scores to simulate an accidental mistake being made or is it?</p> <p>Firstly we can use the manual snapshot option below.</p> <p></p> <p>For the demo, I am going to leave everything as the default</p> <p></p> <p>Back on the dashboard, you get a status report on the job as it is running and then when complete it should look as successful as this one.</p> <p></p>"},{"location":"90DaysOfDevOps/day87/#failure-scenario","title":"Failure Scenario","text":"<p>We can now make that fatal change to our mission-critical data by simply adding in a prescriptive bad change to our application.</p> <p>As you can see below we have two inputs that we probably don't want in our production mission-critical database.</p> <p></p>"},{"location":"90DaysOfDevOps/day87/#restore-the-data","title":"Restore the data","text":"<p>This is a simple demo and in a way not realistic although have you seen how easy it is to drop databases?</p> <p>Now we want to get that high score list looking a little cleaner and how we had it before the mistakes were made.</p> <p>Back in the Applications card and on the Pacman tab, we now have 1 restore point we can use to restore from.</p> <p></p> <p>When you select restore you can see all the associated snapshots and exports to that application.</p> <p></p> <p>Select that restore and a side window will appear, we will keep the default settings and hit restore.</p> <p></p> <p>Confirm that you want to make this happen.</p> <p></p> <p>You can then go back to the dashboard and see the progress of the restore. You should see something like this.</p> <p></p> <p>But more importantly, how is our High-Score list looking in our mission-critical application. You will have to start the port forward again to Pacman as we previously covered.</p> <p></p> <p>A super simple demo and only really touching the surface of what Kasten K10 can achieve when it comes to backup. I will be creating some more in-depth video content on some of these areas in the future. We will also be using Kasten K10 to highlight some of the other prominent areas around Data Management when it comes to Disaster Recovery and the mobility of your data.</p> <p>Next, we will take a look at Application consistency.</p>"},{"location":"90DaysOfDevOps/day87/#resources","title":"Resources","text":"<ul> <li>Kubernetes Backup and Restore made easy!</li> <li>Kubernetes Backups, Upgrades, Migrations - with Velero</li> <li>7 Database Paradigms</li> <li>Disaster Recovery vs. Backup: What's the difference?</li> <li>Veeam Portability &amp; Cloud Mobility</li> </ul> <p>See you on Day 88</p>"},{"location":"90DaysOfDevOps/day88/","title":"#90DaysOfDevOps - Application Focused Backup - Day 88","text":""},{"location":"90DaysOfDevOps/day88/#application-focused-backups","title":"Application-Focused Backups","text":"<p>We have already spent some time talking about data services or data-intensive applications such as databases on Day 85. For these data services, we have to consider how we manage consistency, especially when it comes to application consistency.</p> <p>In this post, we are going to dive into that requirement around consistently protecting the application data.</p> <p>To do this our tool of choice will be Kanister</p> <p></p>"},{"location":"90DaysOfDevOps/day88/#introducing-kanister","title":"Introducing Kanister","text":"<p>Kanister is an open-source project by Kasten, that enables us to manage (backup and restore) application data on Kubernetes. You can deploy Kanister as a helm application into your Kubernetes cluster.</p> <p>Kanister uses Kubernetes custom resources, the main custom resources that are installed when Kanister is deployed are</p> <ul> <li><code>Profile</code> - is a target location to store your backups and recover from. Most commonly this will be object storage.</li> <li><code>Blueprint</code> - steps that are to be taken to backup and restore the database should be maintained in the Blueprint</li> <li><code>ActionSet</code> - is the motion to move our target backup to our profile as well as restore actions.</li> </ul>"},{"location":"90DaysOfDevOps/day88/#execution-walkthrough","title":"Execution Walkthrough","text":"<p>Before we get hands-on we should take a look at the workflow that Kanister takes in protecting application data. Firstly our controller is deployed using helm into our Kubernetes cluster, Kanister lives within its namespace. We take our Blueprint of which there are many community-supported blueprints available, we will cover this in more detail shortly. We then have our database workload.</p> <p></p> <p>We then create our ActionSet.</p> <p></p> <p>The ActionSet allows us to run the actions defined in the blueprint against the specific data service.</p> <p></p> <p>The ActionSet in turn uses the Kanister functions (KubeExec, KubeTask, Resource Lifecycle) and pushes our backup to our target repository (Profile).</p> <p></p> <p>If that action is completed/failed the respective status is updated in the Actionset.</p> <p></p>"},{"location":"90DaysOfDevOps/day88/#deploying-kanister","title":"Deploying Kanister","text":"<p>Once again we will be using the minikube cluster to achieve this application backup. If you have it still running from the previous session then we can continue to use this.</p> <p>At the time of writing, we are up to image version <code>0.75.0</code> with the following helm command we will install kanister into our Kubernetes cluster.</p> <p><code>helm install kanister --namespace kanister kanister/kanister-operator --set image.tag=0.75.0 --create-namespace</code></p> <p></p> <p>We can use <code>kubectl get pods -n kanister</code> to ensure the pod is up and running and then we can also check our custom resource definitions are now available (If you have only installed Kanister then you will see the highlighted 3)</p> <p></p>"},{"location":"90DaysOfDevOps/day88/#deploy-a-database","title":"Deploy a Database","text":"<p>Deploying MySQL via helm:</p> <pre><code>APP_NAME=my-production-app\nkubectl create ns ${APP_NAME}\nhelm repo add bitnami https://charts.bitnami.com/bitnami\nhelm install mysql-store bitnami/mysql --set primary.persistence.size=1Gi,volumePermissions.enabled=true --namespace=${APP_NAME}\nkubectl get pods -n ${APP_NAME} -w\n</code></pre> <p></p> <p>Populate the MySQL database with initial data, and run the following:</p> <pre><code>MYSQL_ROOT_PASSWORD=$(kubectl get secret --namespace ${APP_NAME} mysql-store -o jsonpath=\"{.data.mysql-root-password}\" | base64 --decode)\nMYSQL_HOST=mysql-store.${APP_NAME}.svc.cluster.local\nMYSQL_EXEC=\"mysql -h ${MYSQL_HOST} -u root --password=${MYSQL_ROOT_PASSWORD} -DmyImportantData -t\"\necho MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}\n</code></pre>"},{"location":"90DaysOfDevOps/day88/#create-a-mysql-client","title":"Create a MySQL CLIENT","text":"<p>We will run another container image to act as our client</p> <pre><code>APP_NAME=my-production-app\nkubectl run mysql-client --rm --env APP_NS=${APP_NAME} --env MYSQL_EXEC=\"${MYSQL_EXEC}\" --env MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD} --env MYSQL_HOST=${MYSQL_HOST} --namespace ${APP_NAME} --tty -i --restart='Never' --image  docker.io/bitnami/mysql:latest --command -- bash\n</code></pre> <pre><code>Note: if you already have an existing MySQL client pod running, delete with the command\n\nkubectl delete pod -n ${APP_NAME} mysql-client\n</code></pre>"},{"location":"90DaysOfDevOps/day88/#add-data-to-mysql","title":"Add Data to MySQL","text":"<pre><code>echo \"create database myImportantData;\" | mysql -h ${MYSQL_HOST} -u root --password=${MYSQL_ROOT_PASSWORD}\nMYSQL_EXEC=\"mysql -h ${MYSQL_HOST} -u root --password=${MYSQL_ROOT_PASSWORD} -DmyImportantData -t\"\necho \"drop table Accounts\" | ${MYSQL_EXEC}\necho \"create table if not exists Accounts(name text, balance integer); insert into Accounts values('nick', 0);\" |  ${MYSQL_EXEC}\necho \"insert into Accounts values('albert', 112);\" | ${MYSQL_EXEC}\necho \"insert into Accounts values('alfred', 358);\" | ${MYSQL_EXEC}\necho \"insert into Accounts values('beatrice', 1321);\" | ${MYSQL_EXEC}\necho \"insert into Accounts values('bartholomew', 34);\" | ${MYSQL_EXEC}\necho \"insert into Accounts values('edward', 5589);\" | ${MYSQL_EXEC}\necho \"insert into Accounts values('edwin', 144);\" | ${MYSQL_EXEC}\necho \"insert into Accounts values('edwina', 233);\" | ${MYSQL_EXEC}\necho \"insert into Accounts values('rastapopoulos', 377);\" | ${MYSQL_EXEC}\necho \"select * from Accounts;\" |  ${MYSQL_EXEC}\nexit\n</code></pre> <p>You should be able to see some data as per below.</p> <p></p>"},{"location":"90DaysOfDevOps/day88/#create-kanister-profile","title":"Create Kanister Profile","text":"<p>Kanister provides a CLI, <code>kanctl</code> and another utility <code>kando</code> that is used to interact with your object storage provider from the blueprint and both of these utilities.</p> <p>CLI Download</p> <p>I have gone and I have created an AWS S3 Bucket that we will use as our profile target and restore location. I am going to be using environment variables so that I can still show you the commands I am running with <code>kanctl</code> to create our kanister profile.</p> <p><code>kanctl create profile s3compliant --access-key $ACCESS_KEY --secret-key $SECRET_KEY --bucket $BUCKET --region eu-west-2 --namespace my-production-app</code></p> <p></p>"},{"location":"90DaysOfDevOps/day88/#blueprint-time","title":"Blueprint time","text":"<p>Don't worry you don't need to create your one from scratch unless your data service is not listed here in the Kanister Examples but by all means, community contributions are how this project gains awareness.</p> <p>The blueprint we will be using will be the below.</p> <pre><code>apiVersion: cr.kanister.io/v1alpha1\nkind: Blueprint\nmetadata:\n  name: mysql-blueprint\nactions:\n  backup:\n    outputArtifacts:\n      mysqlCloudDump:\n        keyValue:\n          s3path: \"{{ .Phases.dumpToObjectStore.Output.s3path }}\"\n    phases:\n    - func: KubeTask\n      name: dumpToObjectStore\n      objects:\n        mysqlSecret:\n          kind: Secret\n          name: '{{ index .Object.metadata.labels \"app.kubernetes.io/instance\" }}'\n          namespace: '{{ .StatefulSet.Namespace }}'\n      args:\n        image: ghcr.io/kanisterio/mysql-sidecar:0.75.0\n        namespace: \"{{ .StatefulSet.Namespace }}\"\n        command:\n        - bash\n        - -o\n        - errexit\n        - -o\n        - pipefail\n        - -c\n        - |\n          s3_path=\"/mysql-backups/{{ .StatefulSet.Namespace }}/{{ index .Object.metadata.labels \"app.kubernetes.io/instance\" }}/{{ toDate \"2006-01-02T15:04:05.999999999Z07:00\" .Time  | date \"2006-01-02T15-04-05\" }}/dump.sql.gz\"\n          root_password=\"{{ index .Phases.dumpToObjectStore.Secrets.mysqlSecret.Data \"mysql-root-password\" | toString }}\"\n          mysqldump --column-statistics=0 -u root --password=${root_password} -h {{ index .Object.metadata.labels \"app.kubernetes.io/instance\" }} --single-transaction --all-databases | gzip - | kando location push --profile '{{ toJson .Profile }}' --path ${s3_path} -\n          kando output s3path ${s3_path}\n  restore:\n    inputArtifactNames:\n    - mysqlCloudDump\n    phases:\n    - func: KubeTask\n      name: restoreFromBlobStore\n      objects:\n        mysqlSecret:\n          kind: Secret\n          name: '{{ index .Object.metadata.labels \"app.kubernetes.io/instance\" }}'\n          namespace: '{{ .StatefulSet.Namespace }}'\n      args:\n        image: ghcr.io/kanisterio/mysql-sidecar:0.75.0\n        namespace: \"{{ .StatefulSet.Namespace }}\"\n        command:\n        - bash\n        - -o\n        - errexit\n        - -o\n        - pipefail\n        - -c\n        - |\n          s3_path=\"{{ .ArtifactsIn.mysqlCloudDump.KeyValue.s3path }}\"\n          root_password=\"{{ index .Phases.restoreFromBlobStore.Secrets.mysqlSecret.Data \"mysql-root-password\" | toString }}\"\n          kando location pull --profile '{{ toJson .Profile }}' --path ${s3_path} - | gunzip | mysql -u root --password=${root_password} -h {{ index .Object.metadata.labels \"app.kubernetes.io/instance\" }}\n  delete:\n    inputArtifactNames:\n    - mysqlCloudDump\n    phases:\n    - func: KubeTask\n      name: deleteFromBlobStore\n      args:\n        image: ghcr.io/kanisterio/mysql-sidecar:0.75.0\n        namespace: \"{{ .Namespace.Name }}\"\n        command:\n        - bash\n        - -o\n        - errexit\n        - -o\n        - pipefail\n        - -c\n        - |\n          s3_path=\"{{ .ArtifactsIn.mysqlCloudDump.KeyValue.s3path }}\"\n          kando location delete --profile '{{ toJson .Profile }}' --path ${s3_path}\n</code></pre> <p>To add this we will use the <code>kubectl create -f mysql-blueprint.yml -n kanister</code> command</p> <p></p>"},{"location":"90DaysOfDevOps/day88/#create-our-actionset-and-protect-our-application","title":"Create our ActionSet and Protect our application","text":"<p>We will now take a backup of the MySQL data using an ActionSet defining backup for this application. Create an ActionSet in the same namespace as the controller.</p> <p><code>kubectl get profiles.cr.kanister.io -n my-production-app</code> This command will show us the profile we previously created, we can have multiple profiles configured here so we might want to use specific ones for different ActionSets</p> <p>We are then going to create our ActionSet with the following command using <code>kanctl</code></p> <p><code>kanctl create actionset --action backup --namespace kanister --blueprint mysql-blueprint --statefulset my-production-app/mysql-store --profile my-production-app/s3-profile-dc5zm --secrets mysql=my-production-app/mysql-store</code></p> <p>You can see from the command above we are defining the blueprint we added to the namespace, the statefulset in our <code>my-production-app</code> namespace and also the secrets to get into the MySQL application.</p> <p></p> <p>Check the status of the ActionSet by taking the ActionSet name and using this command <code>kubectl --namespace kanister describe actionset backup-qpnqv</code></p> <p>Finally, we can go and confirm that we now have data in our AWS S3 bucket.</p> <p></p>"},{"location":"90DaysOfDevOps/day88/#restore","title":"Restore","text":"<p>We need to cause some damage before we can restore anything, we can do this by dropping our table, maybe it was an accident, maybe it wasn't.</p> <p>Connect to our MySQL pod.</p> <pre><code>APP_NAME=my-production-app\nkubectl run mysql-client --rm --env APP_NS=${APP_NAME} --env MYSQL_EXEC=\"${MYSQL_EXEC}\" --env MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD} --env MYSQL_HOST=${MYSQL_HOST} --namespace ${APP_NAME} --tty -i --restart='Never' --image  docker.io/bitnami/mysql:latest --command -- bash\n</code></pre> <p>You can see that our importantdata DB is there with <code>echo \"SHOW DATABASES;\" | ${MYSQL_EXEC}</code></p> <p>Then to drop we ran <code>echo \"DROP DATABASE myImportantData;\" | ${MYSQL_EXEC}</code></p> <p>And confirmed that this was gone with a few attempts to show our database.</p> <p></p> <p>We can now use Kanister to get our important data back in business using the <code>kubectl get actionset -n kanister</code> to find out the ActionSet name that we took earlier. Then we will create a restore ActionSet to restore our data using <code>kanctl create actionset -n kanister --action restore --from \"backup-qpnqv\"</code></p> <p></p> <p>We can confirm our data is back by using the below command to connect to our database.</p> <pre><code>APP_NAME=my-production-app\nkubectl run mysql-client --rm --env APP_NS=${APP_NAME} --env MYSQL_EXEC=\"${MYSQL_EXEC}\" --env MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD} --env MYSQL_HOST=${MYSQL_HOST} --namespace ${APP_NAME} --tty -i --restart='Never' --image  docker.io/bitnami/mysql:latest --command -- bash\n</code></pre> <p>Now we are inside the MySQL Client, we can issue the <code>echo \"SHOW DATABASES;\" | ${MYSQL_EXEC}</code> and we can see the database is back. We can also issue the <code>echo \"select * from Accounts;\" | ${MYSQL_EXEC}</code> to check the contents of the database and our important data is restored.</p> <p></p> <p>In the next post, we take a look at Disaster Recovery within Kubernetes.</p>"},{"location":"90DaysOfDevOps/day88/#resources","title":"Resources","text":"<ul> <li>Kanister Overview - An extensible open-source framework for app-lvl data management on Kubernetes</li> <li>Application Level Data Operations on Kubernetes</li> <li>Kubernetes Backup and Restore made easy!</li> <li>Kubernetes Backups, Upgrades, Migrations - with Velero</li> <li>7 Database Paradigms</li> <li>Disaster Recovery vs. Backup: What's the difference?</li> <li>Veeam Portability &amp; Cloud Mobility</li> </ul> <p>See you on Day 89</p>"},{"location":"90DaysOfDevOps/day89/","title":"#90DaysOfDevOps - Disaster Recovery - Day 89","text":""},{"location":"90DaysOfDevOps/day89/#disaster-recovery","title":"Disaster Recovery","text":"<p>We have mentioned already how different failure scenarios will warrant different recovery requirements. When it comes to Fire, Flood and Blood scenarios we can consider these mostly disaster situations where we might need our workloads up and running in a completely different location as fast as possible or at least with near-zero recovery time objectives (RTO).</p> <p>This can only be achieved at scale when you automate the replication of the complete application stack to a standby environment.</p> <p>This allows for fast failovers across cloud regions, cloud providers or between on-premises and cloud infrastructure.</p> <p>Keeping with the theme so far, we are going to concentrate on how this can be achieved using Kasten K10 using the minikube cluster that we deployed and configured a few sessions ago.</p> <p>We will then create another minikube cluster with Kasten K10 also installed to act as our standby cluster which in theory could be any location.</p> <p>Kasten K10 also has built-in functionality to ensure if something was to happen to the Kubernetes cluster it is running on that the catalogue data is replicated and available in a new one K10 Disaster Recovery.</p>"},{"location":"90DaysOfDevOps/day89/#add-object-storage-to-k10","title":"Add object storage to K10","text":"<p>The first thing we need to do is add an object storage bucket as a target location for our backups to land. Not only does this act as an offsite location but we can also leverage this as our disaster recovery source data to recover from.</p> <p>I have cleaned out the S3 bucket that we created for the Kanister demo in the last session.</p> <p></p> <p>Port forward to access the K10 dashboard, open a new terminal to run the below command:</p> <p><code>kubectl --namespace kasten-io port-forward service/gateway 8080:8000</code></p> <p>The Kasten dashboard will be available at <code>http://127.0.0.1:8080/k10/#/</code></p> <p></p> <p>To authenticate with the dashboard, we now need the token which we can get with the following commands.</p> <pre><code>TOKEN_NAME=$(kubectl get secret --namespace kasten-io|grep k10-k10-token | cut -d \" \" -f 1)\nTOKEN=$(kubectl get secret --namespace kasten-io $TOKEN_NAME -o jsonpath=\"{.data.token}\" | base64 --decode)\n\necho \"Token value: \"\necho $TOKEN\n</code></pre> <p></p> <p>Now we take this token and we input that into our browser, you will then be prompted for an email and company name.</p> <p></p> <p>Then we get access to the Kasten K10 dashboard.</p> <p></p> <p>Now that we are back in the Kasten K10 dashboard we can add our location profile, select \"Settings\" at the top of the page and \"New Profile\".</p> <p></p> <p>You can see from the image below that we have a choice when it comes to where this location profile is, we are going to select Amazon S3, and we are going to add our sensitive access credentials, region and bucket name.</p> <p></p> <p>If we scroll down on the New Profile creation window you will see, that we also can enable immutable backups which leverage the S3 Object Lock API. For this demo, we won't be using that.</p> <p></p> <p>Hit \"Save Profile\" and you can now see our newly created or added location profile as per below.</p> <p></p>"},{"location":"90DaysOfDevOps/day89/#create-a-policy-to-protect-the-pac-man-app-to-object-storage","title":"Create a policy to protect the Pac-Man app to object storage","text":"<p>In the previous session, we created only an ad-hoc snapshot of our Pac-Man application, therefore we need to create a backup policy that will send our application backups to our newly created object storage location.</p> <p>If you head back to the dashboard and select the Policy card you will see a screen as per below. Select \"Create New Policy\".</p> <p></p> <p>First, we can give our policy a useful name and description. We can also define our backup frequency for demo purposes I am using on-demand.</p> <p></p> <p>Next, we want to enable backups via Snapshot exports meaning that we want to send our data out to our location profile. If you have multiple you can select which one you would like to send your backups to.</p> <p></p> <p>Next, we select the application by either name or labels, I am going to choose by name and all resources.</p> <p></p> <p>Under Advanced settings we are not going to be using any of these but based on our walkthrough of Kanister yesterday, we can leverage Kanister as part of Kasten K10 as well to take those application consistent copies of our data.</p> <p></p> <p>Finally, select \"Create Policy\" and you will now see the policy in our Policy window.</p> <p></p> <p>At the bottom of the created policy, you will have \"Show import details\" we need this string to be able to import into our standby cluster. Copy this somewhere safe for now.</p> <p></p> <p>Before we move on, we just need to select \"run once\" to get a backup sent to our object storage bucket.</p> <p></p> <p>Below, the screenshot is just to show the successful backup and export of our data.</p> <p></p>"},{"location":"90DaysOfDevOps/day89/#create-a-new-minikube-cluster-deploy-k10","title":"Create a new MiniKube cluster &amp; deploy K10","text":"<p>We then need to deploy a second Kubernetes cluster and where this could be any supported version of Kubernetes including OpenShift, for education we will use the very free version of MiniKube with a different name.</p> <p>Using <code>minikube start --addons volumesnapshots,csi-hostpath-driver --apiserver-port=6443 --container-runtime=containerd -p standby --kubernetes-version=1.21.2</code> we can create our new cluster.</p> <p></p> <p>We then can deploy Kasten K10 in this cluster using:</p> <p><code>helm install k10 kasten/k10 --namespace=kasten-io --set auth.tokenAuth.enabled=true --set injectKanisterSidecar.enabled=true --set-string injectKanisterSidecar.namespaceSelector.matchLabels.k10/injectKanisterSidecar=true --create-namespace</code></p> <p>This will take a while but in the meantime, we can use <code>kubectl get pods -n kasten-io -w</code> to watch the progress of our pods getting to the running status.</p> <p>It is worth noting that because we are using MiniKube our application will just run when we run our import policy, our storageclass is the same on this standby cluster. However, something we will cover in the final session is mobility and transformation.</p> <p>When the pods are up and running, we can follow the steps we went through on the previous steps in the other cluster.</p> <p>Port forward to access the K10 dashboard, open a new terminal to run the below command</p> <p><code>kubectl --namespace kasten-io port-forward service/gateway 8080:8000</code></p> <p>The Kasten dashboard will be available at <code>http://127.0.0.1:8080/k10/#/</code></p> <p></p> <p>To authenticate with the dashboard, we now need the token which we can get with the following commands.</p> <pre><code>TOKEN_NAME=$(kubectl get secret --namespace kasten-io|grep k10-k10-token | cut -d \" \" -f 1)\nTOKEN=$(kubectl get secret --namespace kasten-io $TOKEN_NAME -o jsonpath=\"{.data.token}\" | base64 --decode)\n\necho \"Token value: \"\necho $TOKEN\n</code></pre> <p></p> <p>Now we take this token and we input that into our browser, you will then be prompted for an email and company name.</p> <p></p> <p>Then we get access to the Kasten K10 dashboard.</p> <p></p>"},{"location":"90DaysOfDevOps/day89/#import-pac-man-into-new-the-minikube-cluster","title":"Import Pac-Man into new the MiniKube cluster","text":"<p>At this point, we are now able to create an import policy in that standby cluster and connect to the object storage backups and determine what and how we want this to look.</p> <p>First, we add in our Location Profile that we walked through earlier on the other cluster, showing off dark mode here to show the difference between our production system and our DR standby location.</p> <p></p> <p>Now we go back to the dashboard and into the policies tab to create a new policy.</p> <p></p> <p>Create the import policy as per the below image. When complete, we can create a policy. There are options here to restore after import and some people might want this option, this will go and be restored into our standby cluster on completion. We also can change the configuration of the application as it is restored and this is what I have documented in Day 90.</p> <p></p> <p>I selected to import on demand, but you can set a schedule on when you want this import to happen. Because of this, I am going to run once.</p> <p></p> <p>You can see below the successful import policy job.</p> <p></p> <p>If we now head back to the dashboard and into the Applications card, we can then select the drop-down where you see below \"Removed\" you will see our application here. Select Restore</p> <p></p> <p>Here we can see the restore points we have available to us; this was the backup job that we ran on the primary cluster against our Pac-Man application.</p> <p></p> <p>I am not going to change any of the defaults as I want to cover this in more detail in the next session.</p> <p></p> <p>When you hit \"Restore\" it will prompt you with a confirmation.</p> <p></p> <p>We can see below that we are in the standby cluster and if we check on our pods, we can see that we have our running application.</p> <p></p> <p>We can then port forward (in real-life/production environments, you would not need this step to access the application, you would be using ingress)</p> <p></p> <p>Next, we will take a look at Application mobility and transformation.</p>"},{"location":"90DaysOfDevOps/day89/#resources","title":"Resources","text":"<ul> <li>Kubernetes Backup and Restore made easy!</li> <li>Kubernetes Backups, Upgrades, Migrations - with Velero</li> <li>7 Database Paradigms</li> <li>Disaster Recovery vs. Backup: What's the difference?</li> <li>Veeam Portability &amp; Cloud Mobility</li> </ul> <p>See you on Day 90</p>"},{"location":"90DaysOfDevOps/day90/","title":"#90DaysOfDevOps - Data & Application Mobility - Day 90","text":""},{"location":"90DaysOfDevOps/day90/#data-application-mobility","title":"Data &amp; Application Mobility","text":"<p>Day 90 of the #90DaysOfDevOps Challenge! In this final session, I am going to cover the mobility of our data and applications. I am specifically going to focus on Kubernetes but the requirement across platforms and between platforms is something that is an ever-growing requirement and is seen in the field.</p> <p>The use case being \"I want to move my workload, application and data from one location to another\" for many different reasons, could be cost, risk or to provide the business with a better service.</p> <p>In this session, we are going to take our workload and we are going to look at moving a Kubernetes workload from one cluster to another, but in doing so we are going to change how our application is on the target location.</p> <p>It uses a lot of the characteristics that we went through with Disaster Recovery</p>"},{"location":"90DaysOfDevOps/day90/#the-requirement","title":"The Requirement","text":"<p>Our current Kubernetes cluster cannot handle demand and our costs are rocketing through the roof, it is a business decision that we wish to move our production Kubernetes cluster to our Disaster Recovery location, located on a different public cloud which will provide the ability to expand but also at a cheaper rate. We could also take advantage of some of the native cloud services available in the target cloud.</p> <p>Our current mission-critical application (Pac-Man) has a database (MongoDB) and is running on slow storage, we would like to move to a newer faster storage tier.</p> <p>The current Pac-Man (NodeJS) front-end is not scaling very well, and we would like to increase the number of available pods in the new location.</p>"},{"location":"90DaysOfDevOps/day90/#getting-to-it","title":"Getting to IT","text":"<p>We have our brief and in fact, we have our imports already hitting the Disaster Recovery Kubernetes cluster.</p> <p>The first job we need to do is remove the restore operation we carried out on Day 89 for the Disaster Recovery testing.</p> <p>We can do this using <code>kubectl delete ns pacman</code> on the \"standby\" minikube cluster.</p> <p></p> <p>To get started head into the Kasten K10 Dashboard, and select the Applications card. From the dropdown choose \"Removed\"</p> <p></p> <p>We then get a list of the available restore points. We will select the one that is available as this contains our mission-critical data. (In this example we only have a single restore point.)</p> <p></p> <p>When we worked on the Disaster Recovery process, we left everything as default. However, these additional restore options are there if you have a Disaster Recovery process that requires the transformation of your application. In this instance, we have the requirement to change our storage and number of replicas.</p> <p></p> <p>Select the \"Apply transforms to restored resources\" option.</p> <p></p> <p>It just so happens that the two built-in examples for the transformation that we want to perform are what we need for our requirements.</p> <p></p> <p>The first requirement is that on our primary cluster we were using a Storage Class called <code>csi-hostpath-sc</code> and in our new cluster we would like to use <code>standard</code> so we can make that change here.</p> <p></p> <p>Looks good, hit the create transform button at the bottom.</p> <p></p> <p>The next requirement is that we would like to scale our Pac-Man frontend deployment to \"5\"</p> <p></p> <p>If you are following along you should see both of our transforms as per below.</p> <p></p> <p>You can now see from the below image that we are going to restore all of the artefacts listed below, if we wanted to we could also be granular about what we wanted to restore. Hit the \"Restore\" button</p> <p></p> <p>Again, we will be asked to confirm the actions.</p> <p></p> <p>The final thing to show is now if we head back into the terminal and we take a look at our cluster, you can see we have 5 pods now for the Pacman pods and our storageclass is now set to standard vs the csi-hostpath-sc</p> <p></p> <p>Many different options can be achieved through transformation. This can span not only migration but also Disaster Recovery, test and development type scenarios and more.</p>"},{"location":"90DaysOfDevOps/day90/#api-and-automation","title":"API and Automation","text":"<p>I have not spoken about the ability to leverage the API and automate some of these tasks, but these options are present and throughout the UI some breadcrumbs provide the command sets to take advantage of the APIs for automation tasks.</p> <p>The important thing to note about Kasten K10 is that on deployment it is deployed inside the Kubernetes cluster and then can be called through the Kubernetes API.</p> <p>This then brings us to a close on the section around Storing and Protecting your data.</p>"},{"location":"90DaysOfDevOps/day90/#resources","title":"Resources","text":"<ul> <li>Kubernetes Backup and Restore made easy!</li> <li>Kubernetes Backups, Upgrades, Migrations - with Velero</li> <li>7 Database Paradigms</li> <li>Disaster Recovery vs. Backup: What's the difference?</li> <li>Veeam Portability &amp; Cloud Mobility</li> </ul>"},{"location":"90DaysOfDevOps/day90/#closing","title":"Closing","text":"<p>As I wrap up this challenge, I want to continue to ask for feedback to make sure that the information is always relevant.</p> <p>I also appreciate there are a lot of topics that I was not able to cover or not able to dive deeper into around the topics of DevOps.</p> <p>This means that we can always make another attempt that this challenge next year and find another 90 days' worth of content and walkthroughs to work through.</p>"},{"location":"90DaysOfDevOps/day90/#what-is-next","title":"What is next?","text":"<p>Firstly, a break from writing for a little while, I started this challenge on the 1st of January 2022 and I finished on the 31st of March 2022 at 19:50 BST! It has been a slog. But as I say and have said for a long time, if this content helps one person, then it is always worth learning in public!</p> <p>I have some ideas on where to take this next and hopefully, it has a life outside of a GitHub repository and we can look at creating an eBook and possibly even a physical book.</p> <p>I also know that we need to revisit each post and make sure everything is grammatically correct before making anything like that happen. If anyone does know about how to take markdown to print or to an eBook it would be greatly appreciated feedback.</p> <p>As always keep the issues and PRs coming.</p> <p>Thanks! @MichaelCade1</p> <ul> <li>GitHub</li> <li>Twitter</li> </ul>"},{"location":"90DaysOfDevOps/CICD/Jenkins/steps/","title":"Steps","text":""},{"location":"90DaysOfDevOps/CICD/Jenkins/steps/#steps-taken-to-deploy-jenkins","title":"Steps taken to deploy Jenkins","text":"<p>minikube start</p> <p>kubectl create namespace jenkins or kubectl create -f jenkins-namespace.yml </p> <p>kubectl get namespaces</p> <p>helm repo list</p> <p>helm repo add jenkinsci https://charts.jenkins.io</p> <p>helm repo update</p> <p>kubectl apply -f jenkins-volume.yml </p> <p>kubectl apply -f jenkins-sa.yml  </p> <p>chart=jenkinsci/jenkins helm install jenkins -n jenkins -f jenkins-values.yml $chart</p> <p>minikube ssh sudo chown -R 1000:1000 /data/jenkins-volume</p> <p>kubectl delete pod jenkins-0 -n jenkins</p> <p>kubectl get pods -n jenkins -w</p> <p>kubectl exec --namespace jenkins -it svc/jenkins -c jenkins -- /bin/cat /run/secrets/chart-admin-password &amp;&amp; echo</p> <p>kubectl --namespace jenkins port-forward svc/jenkins 8080:8080</p> <p>open browser and login to http://localhost:8080</p> <p>perform plugin updates </p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario3/roles/apache2/","title":"Role Name","text":"<p>A brief description of the role goes here.</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario3/roles/apache2/#requirements","title":"Requirements","text":"<p>Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required.</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario3/roles/apache2/#role-variables","title":"Role Variables","text":"<p>A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well.</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario3/roles/apache2/#dependencies","title":"Dependencies","text":"<p>A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles.</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario3/roles/apache2/#example-playbook","title":"Example Playbook","text":"<p>Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too:</p> <pre><code>- hosts: servers\n  roles:\n     - { role: username.rolename, x: 42 }\n</code></pre>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario3/roles/apache2/#license","title":"License","text":"<p>BSD</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario3/roles/apache2/#author-information","title":"Author Information","text":"<p>An optional section for the role authors to include contact information, or a website (HTML is not allowed).</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario4/roles/apache2/","title":"Role Name","text":"<p>A brief description of the role goes here.</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario4/roles/apache2/#requirements","title":"Requirements","text":"<p>Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required.</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario4/roles/apache2/#role-variables","title":"Role Variables","text":"<p>A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well.</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario4/roles/apache2/#dependencies","title":"Dependencies","text":"<p>A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles.</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario4/roles/apache2/#example-playbook","title":"Example Playbook","text":"<p>Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too:</p> <pre><code>- hosts: servers\n  roles:\n     - { role: username.rolename, x: 42 }\n</code></pre>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario4/roles/apache2/#license","title":"License","text":"<p>BSD</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario4/roles/apache2/#author-information","title":"Author Information","text":"<p>An optional section for the role authors to include contact information, or a website (HTML is not allowed).</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario5/roles/apache2/","title":"Role Name","text":"<p>A brief description of the role goes here.</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario5/roles/apache2/#requirements","title":"Requirements","text":"<p>Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required.</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario5/roles/apache2/#role-variables","title":"Role Variables","text":"<p>A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well.</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario5/roles/apache2/#dependencies","title":"Dependencies","text":"<p>A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles.</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario5/roles/apache2/#example-playbook","title":"Example Playbook","text":"<p>Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too:</p> <pre><code>- hosts: servers\n  roles:\n     - { role: username.rolename, x: 42 }\n</code></pre>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario5/roles/apache2/#license","title":"License","text":"<p>BSD</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario5/roles/apache2/#author-information","title":"Author Information","text":"<p>An optional section for the role authors to include contact information, or a website (HTML is not allowed).</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario6/roles/apache2/","title":"Role Name","text":"<p>A brief description of the role goes here.</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario6/roles/apache2/#requirements","title":"Requirements","text":"<p>Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required.</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario6/roles/apache2/#role-variables","title":"Role Variables","text":"<p>A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well.</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario6/roles/apache2/#dependencies","title":"Dependencies","text":"<p>A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles.</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario6/roles/apache2/#example-playbook","title":"Example Playbook","text":"<p>Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too:</p> <pre><code>- hosts: servers\n  roles:\n     - { role: username.rolename, x: 42 }\n</code></pre>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario6/roles/apache2/#license","title":"License","text":"<p>BSD</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario6/roles/apache2/#author-information","title":"Author Information","text":"<p>An optional section for the role authors to include contact information, or a website (HTML is not allowed).</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario7/roles/apache2/","title":"Role Name","text":"<p>A brief description of the role goes here.</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario7/roles/apache2/#requirements","title":"Requirements","text":"<p>Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required.</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario7/roles/apache2/#role-variables","title":"Role Variables","text":"<p>A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well.</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario7/roles/apache2/#dependencies","title":"Dependencies","text":"<p>A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles.</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario7/roles/apache2/#example-playbook","title":"Example Playbook","text":"<p>Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too:</p> <pre><code>- hosts: servers\n  roles:\n     - { role: username.rolename, x: 42 }\n</code></pre>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario7/roles/apache2/#license","title":"License","text":"<p>BSD</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario7/roles/apache2/#author-information","title":"Author Information","text":"<p>An optional section for the role authors to include contact information, or a website (HTML is not allowed).</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario7/roles/mysql/","title":"Role Name","text":"<p>A brief description of the role goes here.</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario7/roles/mysql/#requirements","title":"Requirements","text":"<p>Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required.</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario7/roles/mysql/#role-variables","title":"Role Variables","text":"<p>A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well.</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario7/roles/mysql/#dependencies","title":"Dependencies","text":"<p>A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles.</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario7/roles/mysql/#example-playbook","title":"Example Playbook","text":"<p>Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too:</p> <pre><code>- hosts: servers\n  roles:\n     - { role: username.rolename, x: 42 }\n</code></pre>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario7/roles/mysql/#license","title":"License","text":"<p>BSD</p>"},{"location":"90DaysOfDevOps/Configmgmt/ansible-scenario7/roles/mysql/#author-information","title":"Author Information","text":"<p>An optional section for the role authors to include contact information, or a website (HTML is not allowed).</p>"},{"location":"90DaysOfDevOps/Containers/elasticsearch-logstash-kibana/","title":"Index","text":""},{"location":"90DaysOfDevOps/Containers/elasticsearch-logstash-kibana/#compose-sample-application","title":"Compose sample application","text":""},{"location":"90DaysOfDevOps/Containers/elasticsearch-logstash-kibana/#elasticsearch-logstash-and-kibana-elk-in-single-node","title":"Elasticsearch, Logstash, and Kibana (ELK) in single-node","text":"<p>Project structure: <pre><code>.\n\u2514\u2500\u2500 docker-compose.yml\n</code></pre></p> <p>docker-compose.yml <pre><code>services:\n  elasticsearch:\n    image: elasticsearch:7.8.0\n    ...\n  logstash:\n    image: logstash:7.8.0\n    ...\n  kibana:\n    image: kibana:7.8.0\n    ...\n</code></pre></p>"},{"location":"90DaysOfDevOps/Containers/elasticsearch-logstash-kibana/#deploy-with-docker-compose","title":"Deploy with docker-compose","text":"<pre><code>$ docker-compose up -d\nCreating network \"elasticsearch-logstash-kibana_elastic\" with driver \"bridge\"\nCreating es ... done\nCreating log ... done\nCreating kib ... done\n</code></pre>"},{"location":"90DaysOfDevOps/Containers/elasticsearch-logstash-kibana/#expected-result","title":"Expected result","text":"<p>Listing containers must show three containers running and the port mapping as below: <pre><code>$ docker ps\nCONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS                    PORTS                                                                                            NAMES\n173f0634ed33        logstash:7.8.0        \"/usr/local/bin/dock\u2026\"   43 seconds ago      Up 41 seconds             0.0.0.0:5000-&gt;5000/tcp, 0.0.0.0:5044-&gt;5044/tcp, 0.0.0.0:9600-&gt;9600/tcp, 0.0.0.0:5000-&gt;5000/udp   log\nb448fd3e9b30        kibana:7.8.0          \"/usr/local/bin/dumb\u2026\"   43 seconds ago      Up 42 seconds             0.0.0.0:5601-&gt;5601/tcp                                                                           kib\n366d358fb03d        elasticsearch:7.8.0   \"/tini -- /usr/local\u2026\"   43 seconds ago      Up 42 seconds (healthy)   0.0.0.0:9200-&gt;9200/tcp, 0.0.0.0:9300-&gt;9300/tcp                                                   es\n</code></pre></p> <p>After the application starts, navigate to below links in your web browser:</p> <ul> <li>Elasticsearch: <code>http://localhost:9200</code></li> <li>Logstash: <code>http://localhost:9600</code></li> <li>Kibana: <code>http://localhost:5601/api/status</code></li> </ul> <p>Stop and remove the containers <pre><code>$ docker-compose down\n</code></pre></p>"},{"location":"90DaysOfDevOps/Containers/elasticsearch-logstash-kibana/#attribution","title":"Attribution","text":"<p>The example Nginx logs are copied from here.</p>"},{"location":"90DaysOfDevOps/Monitoring/Elastic%20Stack/extensions/","title":"Extensions","text":"<p>Third-party extensions that enable extra integrations with the Elastic stack.</p>"},{"location":"90DaysOfDevOps/Monitoring/Elastic%20Stack/extensions/apm-server/","title":"APM Server extension","text":"<p>The APM Server receives data from APM agents and transforms them into Elasticsearch documents that can be visualised in Kibana.</p>"},{"location":"90DaysOfDevOps/Monitoring/Elastic%20Stack/extensions/apm-server/#usage","title":"Usage","text":"<p>To include APM Server in the stack, run Docker Compose from the root of the repository with an additional command line argument referencing the <code>apm-server-compose.yml</code> file:</p> <pre><code>$ docker-compose -f docker-compose.yml -f extensions/apm-server/apm-server-compose.yml up\n</code></pre> <p>Meanwhile, you can navigate to the APM application in Kibana and follow the setup instructions to get started.</p>"},{"location":"90DaysOfDevOps/Monitoring/Elastic%20Stack/extensions/apm-server/#connecting-an-agent-to-apm-server","title":"Connecting an agent to APM Server","text":"<p>The most basic configuration to send traces to APM server is to specify the <code>SERVICE_NAME</code> and <code>SERVICE_URL</code>. Here is an example Python Flask configuration:</p> <pre><code>import elasticapm\nfrom elasticapm.contrib.flask import ElasticAPM\n\nfrom flask import Flask\n\napp = Flask(__name__)\napp.config['ELASTIC_APM'] = {\n    # Set required service name. Allowed characters:\n    # a-z, A-Z, 0-9, -, _, and space\n    'SERVICE_NAME': 'PYTHON_FLASK_TEST_APP',\n\n    # Set custom APM Server URL (default: http://localhost:8200)\n    'SERVER_URL': 'http://apm-server:8200',\n\n    'DEBUG': True,\n}\n</code></pre> <p>Configuration settings for each supported language are available in the APM documentation: APM Agents.</p>"},{"location":"90DaysOfDevOps/Monitoring/Elastic%20Stack/extensions/apm-server/#checking-connectivity-and-importing-default-apm-dashboards","title":"Checking connectivity and importing default APM dashboards","text":"<ol> <li>On the Kibana home page, click <code>Add APM</code> under the Observability panel.</li> <li>Click <code>Check APM Server status</code> to confirm the server is up and running.</li> <li>Click <code>Check agent status</code> to verify your agent has registered properly.</li> <li>Click <code>Load Kibana objects</code> to create an index pattern for APM.</li> <li>Click <code>Launch APM</code> to be taken to the APM dashboard.</li> </ol>"},{"location":"90DaysOfDevOps/Monitoring/Elastic%20Stack/extensions/apm-server/#see-also","title":"See also","text":"<p>Running APM Server on Docker</p>"},{"location":"90DaysOfDevOps/Monitoring/Elastic%20Stack/extensions/curator/","title":"Curator","text":"<p>Elasticsearch Curator helps you curate or manage your indices.</p>"},{"location":"90DaysOfDevOps/Monitoring/Elastic%20Stack/extensions/curator/#usage","title":"Usage","text":"<p>If you want to include the Curator extension, run Docker Compose from the root of the repository with an additional command line argument referencing the <code>curator-compose.yml</code> file:</p> <pre><code>$ docker-compose -f docker-compose.yml -f extensions/curator/curator-compose.yml up\n</code></pre> <p>This sample setup demonstrates how to run <code>curator</code> every minute using <code>cron</code>.</p> <p>All configuration files are available in the <code>config/</code> directory.</p>"},{"location":"90DaysOfDevOps/Monitoring/Elastic%20Stack/extensions/curator/#documentation","title":"Documentation","text":"<p>Curator Reference</p>"},{"location":"90DaysOfDevOps/Monitoring/Elastic%20Stack/extensions/enterprise-search/","title":"Enterprise Search extension","text":"<p>Elastic Enterprise Search is a suite of products for search applications backed by the Elastic Stack.</p>"},{"location":"90DaysOfDevOps/Monitoring/Elastic%20Stack/extensions/enterprise-search/#requirements","title":"Requirements","text":"<ul> <li>2 GB of free RAM, on top of the resources required by the other stack components and extensions.</li> </ul> <p>Enterprise Search exposes the TCP port <code>3002</code> for its Web UI and API.</p>"},{"location":"90DaysOfDevOps/Monitoring/Elastic%20Stack/extensions/enterprise-search/#usage","title":"Usage","text":""},{"location":"90DaysOfDevOps/Monitoring/Elastic%20Stack/extensions/enterprise-search/#generate-an-encryption-key","title":"Generate an encryption key","text":"<p>Enterprise Search requires one or more encryption keys to be configured before the initial startup. Failing to do so prevents the server from starting.</p> <p>Encryption keys can contain any series of characters. Elastic recommends using 256-bit keys for optimal security.</p> <p>Those encryption keys must be added manually to the <code>config/enterprise-search.yml</code> file. By default, the list of encryption keys is empty and must be populated using one of the following formats:</p> <pre><code>secret_management.encryption_keys:\n  - my_first_encryption_key\n  - my_second_encryption_key\n  - ...\n</code></pre> <pre><code>secret_management.encryption_keys: [my_first_encryption_key, my_second_encryption_key, ...]\n</code></pre> <p>:information_source: To generate a strong encryption key, for example using the AES-256 cipher, you can use the OpenSSL utility or any other online/offline tool of your choice:</p> <pre><code>$ openssl enc -aes-256 -P\n\nenter aes-256-cbc encryption password: &lt;a strong password&gt;\nVerifying - enter aes-256-cbc encryption password: &lt;repeat your strong password&gt;\n...\n\nkey=&lt;generated AES key&gt;\n</code></pre>"},{"location":"90DaysOfDevOps/Monitoring/Elastic%20Stack/extensions/enterprise-search/#enable-elasticsearchs-api-key-service","title":"Enable Elasticsearch's API key service","text":"<p>Enterprise Search requires Elasticsearch's built-in API key service to be enabled in order to start. Unless Elasticsearch is configured to enable TLS on the HTTP interface (disabled by default), this service is disabled by default.</p> <p>To enable it, modify the Elasticsearch configuration file in <code>elasticsearch/config/elasticsearch.yml</code> and add the following setting:</p> <pre><code>xpack.security.authc.api_key.enabled: true\n</code></pre>"},{"location":"90DaysOfDevOps/Monitoring/Elastic%20Stack/extensions/enterprise-search/#configure-the-enterprise-search-host-in-kibana","title":"Configure the Enterprise Search host in Kibana","text":"<p>Kibana acts as the management interface to Enterprise Search.</p> <p>To enable the management experience for Enterprise Search, modify the Kibana configuration file in <code>kibana/config/kibana.yml</code> and add the following setting:</p> <pre><code>enterpriseSearch.host: http://enterprise-search:3002\n</code></pre>"},{"location":"90DaysOfDevOps/Monitoring/Elastic%20Stack/extensions/enterprise-search/#start-the-server","title":"Start the server","text":"<p>To include Enterprise Search in the stack, run Docker Compose from the root of the repository with an additional command line argument referencing the <code>enterprise-search-compose.yml</code> file:</p> <pre><code>$ docker-compose -f docker-compose.yml -f extensions/enterprise-search/enterprise-search-compose.yml up\n</code></pre> <p>Allow a few minutes for the stack to start, then open your web browser at the address http://localhost:3002 to see the Enterprise Search home page.</p> <p>Enterprise Search is configured on first boot with the following default credentials:</p> <ul> <li>user: enterprise_search</li> <li>password: changeme</li> </ul>"},{"location":"90DaysOfDevOps/Monitoring/Elastic%20Stack/extensions/enterprise-search/#security","title":"Security","text":"<p>The Enterprise Search password is defined inside the Compose file via the <code>ENT_SEARCH_DEFAULT_PASSWORD</code> environment variable. We highly recommend choosing a more secure password than the default one for security reasons.</p> <p>To do so, change the value <code>ENT_SEARCH_DEFAULT_PASSWORD</code> environment variable inside the Compose file before the first boot:</p> <pre><code>enterprise-search:\n\n  environment:\n    ENT_SEARCH_DEFAULT_PASSWORD: {{some strong password}}\n</code></pre> <p>:warning: The default Enterprise Search password can only be set during the initial boot. Once the password is persisted in Elasticsearch, it can only be changed via the Elasticsearch API.</p> <p>For more information, please refer to User Management and Security.</p>"},{"location":"90DaysOfDevOps/Monitoring/Elastic%20Stack/extensions/enterprise-search/#configuring-enterprise-search","title":"Configuring Enterprise Search","text":"<p>The Enterprise Search configuration is stored in <code>config/enterprise-search.yml</code>. You can modify this file using the Default Enterprise Search configuration as a reference.</p> <p>You can also specify the options you want to override by setting environment variables inside the Compose file:</p> <pre><code>enterprise-search:\n\n  environment:\n    ent_search.auth.source: standard\n    worker.threads: '6'\n</code></pre> <p>Any change to the Enterprise Search configuration requires a restart of the Enterprise Search container:</p> <pre><code>$ docker-compose -f docker-compose.yml -f extensions/enterprise-search/enterprise-search-compose.yml restart enterprise-search\n</code></pre> <p>Please refer to the following documentation page for more details about how to configure Enterprise Search inside a Docker container: Running Enterprise Search Using Docker.</p>"},{"location":"90DaysOfDevOps/Monitoring/Elastic%20Stack/extensions/enterprise-search/#see-also","title":"See also","text":"<p>Enterprise Search documentation</p>"},{"location":"90DaysOfDevOps/Monitoring/Elastic%20Stack/extensions/filebeat/","title":"Filebeat","text":"<p>Filebeat is a lightweight shipper for forwarding and centralizing log data. Installed as an agent on your servers, Filebeat monitors the log files or locations that you specify, collects log events, and forwards them either to Elasticsearch or Logstash for indexing.</p>"},{"location":"90DaysOfDevOps/Monitoring/Elastic%20Stack/extensions/filebeat/#usage","title":"Usage","text":"<p>To include Filebeat in the stack, run Docker Compose from the root of the repository with an additional command line argument referencing the <code>filebeat-compose.yml</code> file:</p> <pre><code>$ docker-compose -f docker-compose.yml -f extensions/filebeat/filebeat-compose.yml up\n</code></pre>"},{"location":"90DaysOfDevOps/Monitoring/Elastic%20Stack/extensions/filebeat/#configuring-filebeat","title":"Configuring Filebeat","text":"<p>The Filebeat configuration is stored in <code>config/filebeat.yml</code>. You can modify this file with the help of the Configuration reference.</p> <p>Any change to the Filebeat configuration requires a restart of the Filebeat container:</p> <pre><code>$ docker-compose -f docker-compose.yml -f extensions/filebeat/filebeat-compose.yml restart filebeat\n</code></pre> <p>Please refer to the following documentation page for more details about how to configure Filebeat inside a Docker container: Run Filebeat on Docker.</p>"},{"location":"90DaysOfDevOps/Monitoring/Elastic%20Stack/extensions/filebeat/#see-also","title":"See also","text":"<p>Filebeat documentation</p>"},{"location":"90DaysOfDevOps/Monitoring/Elastic%20Stack/extensions/logspout/","title":"Logspout extension","text":"<p>Logspout collects all Docker logs using the Docker logs API, and forwards them to Logstash without any additional configuration.</p>"},{"location":"90DaysOfDevOps/Monitoring/Elastic%20Stack/extensions/logspout/#usage","title":"Usage","text":"<p>If you want to include the Logspout extension, run Docker Compose from the root of the repository with an additional command line argument referencing the <code>logspout-compose.yml</code> file:</p> <pre><code>$ docker-compose -f docker-compose.yml -f extensions/logspout/logspout-compose.yml up\n</code></pre> <p>In your Logstash pipeline configuration, enable the <code>udp</code> input and set the input codec to <code>json</code>:</p> <pre><code>input {\n  udp {\n    port  =&gt; 5000\n    codec =&gt; json\n  }\n}\n</code></pre>"},{"location":"90DaysOfDevOps/Monitoring/Elastic%20Stack/extensions/logspout/#documentation","title":"Documentation","text":"<p>https://github.com/looplab/logspout-logstash</p>"},{"location":"90DaysOfDevOps/Monitoring/Elastic%20Stack/extensions/metricbeat/","title":"Metricbeat","text":"<p>Metricbeat is a lightweight shipper that you can install on your servers to periodically collect metrics from the operating system and from services running on the server. Metricbeat takes the metrics and statistics that it collects and ships them to the output that you specify, such as Elasticsearch or Logstash.</p>"},{"location":"90DaysOfDevOps/Monitoring/Elastic%20Stack/extensions/metricbeat/#usage","title":"Usage","text":"<p>To include Metricbeat in the stack, run Docker Compose from the root of the repository with an additional command line argument referencing the <code>metricbeat-compose.yml</code> file:</p> <pre><code>$ docker-compose -f docker-compose.yml -f extensions/metricbeat/metricbeat-compose.yml up\n</code></pre>"},{"location":"90DaysOfDevOps/Monitoring/Elastic%20Stack/extensions/metricbeat/#configuring-metricbeat","title":"Configuring Metricbeat","text":"<p>The Metricbeat configuration is stored in <code>config/metricbeat.yml</code>. You can modify this file with the help of the Configuration reference.</p> <p>Any change to the Metricbeat configuration requires a restart of the Metricbeat container:</p> <pre><code>$ docker-compose -f docker-compose.yml -f extensions/metricbeat/metricbeat-compose.yml restart metricbeat\n</code></pre> <p>Please refer to the following documentation page for more details about how to configure Metricbeat inside a Docker container: Run Metricbeat on Docker.</p>"},{"location":"90DaysOfDevOps/Monitoring/Elastic%20Stack/extensions/metricbeat/#see-also","title":"See also","text":"<p>Metricbeat documentation</p>"},{"location":"ServerAdministration/DatabaseServer/NoSQL/Redis/Benmark-Redis/redis-benchmark.md/","title":"Redis benchmark.md","text":"<p>Demo Redis cache + nodejs</p> <pre><code>mvn spring-boot:run\n\n#springboot-no-caching\nhttp://103.126.163.19:8030/products/search?name=Iphone\n\n#springboot-caching\nhttp://103.126.163.19:8040/products/search?name=Iphone\n</code></pre>"},{"location":"ServerAdministration/DatabaseServer/NoSQL/Redis/Benmark-Redis/redis-benchmark.md/#benchmark-redis","title":"benchmark redis","text":"<pre><code>redis-benchmark -h 10.10.100.100 -p 6379 -c 100 -n 1000000 -a 'P@ssw0rd@2023\n</code></pre>"},{"location":"ServerAdministration/DatabaseServer/NoSQL/Redis/Benmark-Redis/redis-benchmark.md/#benchmark-mysql","title":"benchmark mysql","text":"<pre><code>sysbench --db-driver=mysql --mysql-user=sbtest_user --mysql_password=password --mysql-db=sbtest --mysql-host=0.0.0.0 --mysql-port=3306 --tables=16 --table-size=100000 --threads=100 --time=60 --events=0 --report-interval=1 --rate=10000 /usr/share/sysbench/oltp_read_write.lua run\n</code></pre> <p><code>mysql -u root -p'8cae272adb86a3e2 '</code></p>"},{"location":"ServerAdministration/DatabaseServer/NoSQL/Redis/Benmark-Redis/redis-benchmark.md/#demo-pubsub","title":"Demo PUB/SUB","text":"<pre><code>redis-cli -h 10.10.100.100 -p 6379 -a 'P@ssw0rd!@#2023'\n\nsubscribe mychanel\n\npublish mychanel 'Nguyen Van A da oder goi dich vu VPS PROSSD'\n</code></pre>"},{"location":"ServerAdministration/DatabaseServer/NoSQL/Redis/Benmark-Redis/redis-benchmark.md/#redis-haproxy","title":"Redis HAProxy","text":"<p>Read <pre><code>redis-cli -h 10.10.100.10 -p 6379 -a 'P@ssw0rd!@#2023'\n</code></pre></p> <p>Write <pre><code>redis-cli -h 10.10.100.10 -p 6380 -a 'P@ssw0rd!@#2023'\n</code></pre></p>"},{"location":"ServerAdministration/DatabaseServer/NoSQL/Redis/Benmark-Redis/redis-benchmark.md/#check-haproxy-repliaction","title":"Check HAProxy + Repliaction","text":"<pre><code>for i in {1..3}; do redis-cli -h 10.10.100.10 -p 6379 -a 'P@ssw0rd!@#2023' info replication; done\n\nfor i in {1..3}; do redis-cli -h 10.10.100.10 -p 6380 -a 'P@ssw0rd!@#2023' info replication; done\n</code></pre> <ol> <li> <p>Hashes:</p> <ul> <li>\u0110\u1ec3 set gi\u00e1 tr\u1ecb cho m\u1ed9t tr\u01b0\u1eddng trong m\u1ed9t hash:\u00a0<code>HSET key field value</code></li> <li>V\u00ed d\u1ee5:\u00a0<code>HSET myhash field1 \"Hello\"</code></li> <li>\u0110\u1ec3 xem gi\u00e1 tr\u1ecb c\u1ee7a m\u1ed9t tr\u01b0\u1eddng trong m\u1ed9t hash:\u00a0<code>HGET key field</code></li> <li>V\u00ed d\u1ee5:\u00a0<code>HGET myhash field1</code></li> <li> <p>Lists:</p> </li> <li> <p>\u0110\u1ec3 th\u00eam gi\u00e1 tr\u1ecb v\u00e0o m\u1ed9t list:\u00a0<code>LPUSH key value</code></p> </li> <li>V\u00ed d\u1ee5:\u00a0<code>LPUSH mylist \"World\"</code></li> <li>\u0110\u1ec3 xem gi\u00e1 tr\u1ecb c\u1ee7a m\u1ed9t list:\u00a0<code>LRANGE key start stop</code></li> <li>V\u00ed d\u1ee5:\u00a0<code>LRANGE mylist 0 -1</code></li> <li> <p>Strings:</p> </li> <li> <p>\u0110\u1ec3 set gi\u00e1 tr\u1ecb cho m\u1ed9t chu\u1ed7i:\u00a0<code>SET key value</code></p> </li> <li>V\u00ed d\u1ee5:\u00a0<code>SET mystring \"Hello\"</code></li> <li>\u0110\u1ec3 xem gi\u00e1 tr\u1ecb c\u1ee7a m\u1ed9t chu\u1ed7i:\u00a0<code>GET key</code></li> <li>V\u00ed d\u1ee5:\u00a0<code>GET mystring</code></li> <li> <p>Sets:</p> </li> <li> <p>\u0110\u1ec3 th\u00eam gi\u00e1 tr\u1ecb v\u00e0o m\u1ed9t set:\u00a0<code>SADD key member</code></p> </li> <li>V\u00ed d\u1ee5:\u00a0<code>SADD myset \"Value1\"</code></li> <li>\u0110\u1ec3 xem gi\u00e1 tr\u1ecb c\u1ee7a m\u1ed9t set:\u00a0<code>SMEMBERS key</code></li> <li>V\u00ed d\u1ee5:\u00a0<code>SMEMBERS myset</code></li> <li> <p>Bits:</p> </li> <li> <p>\u0110\u1ec3 set gi\u00e1 tr\u1ecb cho m\u1ed9t bit trong m\u1ed9t chu\u1ed7i bits:\u00a0<code>SETBIT key offset value</code></p> </li> <li>V\u00ed d\u1ee5:\u00a0<code>SETBIT mybits 0 1</code></li> <li>\u0110\u1ec3 xem gi\u00e1 tr\u1ecb c\u1ee7a m\u1ed9t chu\u1ed7i bits:\u00a0<code>GETBIT key offset</code></li> <li>V\u00ed d\u1ee5:\u00a0<code>GETBIT mybits 0</code></li> </ul> </li> </ol>"},{"location":"ServerAdministration/DatabaseServer/NoSQL/Redis/Benmark-Redis/redis-benchmark.md/#heading-1","title":"Heading 1","text":"<p>V\u00ed d\u1ee5:\u00a0<code>GETBIT mybits 0</code></p>"},{"location":"ServerAdministration/DatabaseServer/NoSQL/Redis/Benmark-Redis/redis-benchmark.md/#heading-2","title":"Heading 2","text":"<p>V\u00ed d\u1ee5:\u00a0<code>GETBIT mybits 0</code></p>"},{"location":"ServerAdministration/DatabaseServer/NoSQL/Redis/Benmark-Redis/redis-benchmark.md/#heading-3","title":"Heading 3","text":"<p>V\u00ed d\u1ee5:\u00a0<code>GETBIT mybits 0</code></p>"},{"location":"ServerAdministration/DatabaseServer/NoSQL/Redis/Benmark-Redis/redis-benchmark.md/#heading-4","title":"Heading 4","text":""},{"location":"ServerAdministration/DatabaseServer/NoSQL/Redis/ban-va-toi-hieu-gi-ve-redis/","title":"B\u1ea1n v\u00e0 T\u00f4i hi\u1ec3u g\u00ec v\u1ec1 Redis?","text":""},{"location":"ServerAdministration/DatabaseServer/NoSQL/Redis/ban-va-toi-hieu-gi-ve-redis/#muc-luc","title":"M\u1ee5c l\u1ee5c","text":"<ul> <li>[\u2714\ufe0f] &gt; Redis l\u00e0 g\u00ec? </li> <li>[\u2714\ufe0f] &gt; Redis l\u00e0m \u0111\u01b0\u1ee3c nh\u1eefng g\u00ec? </li> <li>[\u2714\ufe0f] &gt; C\u00e1c ki\u1ec3u d\u1eef li\u1ec7u m\u00e0 Redis c\u00f3 th\u1ec3 h\u1ed7 tr\u1ee3.</li> <li>[\u2714\ufe0f] &gt; Redis ho\u1ea1t \u0111\u1ed9ng nh\u01b0 th\u1ebf n\u00e0o?</li> <li>[\u2714\ufe0f] &gt; Redis Persistence l\u00e0 g\u00ec? </li> <li>[\u2714\ufe0f] &gt; Redis Replication l\u00e0 g\u00ec? </li> <li>[\u2714\ufe0f] &gt; Redis Sentinel l\u00e0 g\u00ec? </li> <li>[\u2714\ufe0f] &gt; Redis Cluster l\u00e0 g\u00ec? </li> <li>[\u2714\ufe0f] &gt; C\u00e1ch c\u00e0i \u0111\u1eb7t v\u00e0 c\u1ea5u h\u00ecnh m\u1ed9t Redis server tr\u00ean Server Linux. </li> <li>[\u2714\ufe0f] &gt; C\u1ea5u h\u00ecnh Redis n\u00e2ng cao. </li> </ul>"},{"location":"ServerAdministration/DatabaseServer/NoSQL/Redis/ban-va-toi-hieu-gi-ve-redis/#1-redis-la-gi","title":"1. Redis l\u00e0 g\u00ec?","text":"<p>Redis (Remote Dictionary Server) l\u00e0 m\u1ed9t h\u1ec7 th\u1ed1ng c\u01a1 s\u1edf d\u1eef li\u1ec7u NoSQL, hay n\u00f3i c\u00e1ch kh\u00e1c l\u00e0 m\u1ed9t In-memory database ho\u1ea1t \u0111\u1ed9ng v\u1edbi ph\u01b0\u01a1ng th\u1ee9c key-value v\u00e0 \u0111\u01b0\u1ee3c l\u01b0u trong b\u1ed9 nh\u1edb Ram thay v\u00ec l\u01b0u d\u1eef li\u1ec7u trong Disk. Th\u01b0\u1eddng \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng r\u1ed9ng r\u00e3i trong c\u00e1c \u1ee9ng d\u1ee5ng web v\u00e0 c\u00e1c h\u1ec7 th\u1ed1ng ph\u00e2n t\u00e1n.  </p> <p>T\u1eadn d\u1ee5ng t\u1ed1c \u0111\u1ed9 x\u1eed l\u00fd cao c\u1ee7a Ram, Redis cung c\u1ea5p th\u1eddi gian ph\u1ea3n h\u1ed3i trong v\u00f2ng 10<sup>3</sup>s gi\u00e2y, cho ph\u00e9p h\u00e0ng tri\u1ec7u y\u00eau c\u1ea7u m\u1ed7i gi\u00e2y b\u1edfi c\u00e1c \u1ee9ng d\u1ee5ng y\u00eau c\u1ea7u th\u1eddi gian th\u1ef1c nh\u01b0 tr\u00f2 ch\u01a1i, t\u00e0i ch\u00ednh, y t\u1ebf ,IoT ...  </p> <p>Hi\u1ec7n nay, Redis l\u00e0 m\u1ed9t trong nh\u1eefng c\u00f4ng c\u1ee5 m\u00e3 ngu\u1ed3n m\u1edf ph\u1ed5 bi\u1ebfn nh\u1ea5t. Nh\u1edd hi\u1ec7u su\u1ea5t m\u1ea1nh m\u1ebd v\u00e0 th\u1eddi gian ph\u1ea3n h\u1ed3i nhanh ch\u00f3ng, Redis l\u00e0 s\u1ef1 l\u1ef1a ch\u1ecdn ph\u1ed5 bi\u1ebfn cho c\u00e1c l\u1eadp tr\u00ecnh vi\u00ean trong vi\u1ec7c caching, qu\u1ea3n l\u00fd session, gaming, b\u1ea3ng x\u1ebfp h\u1ea1ng, s\u1ed1 li\u1ec7u th\u1ed1ng k\u00ea, ph\u00e2n t\u00edch th\u1eddi gian th\u1ef1c, chat/messaging, media streaming, v\u00e0 c\u00e1c \u1ee9ng d\u1ee5ng pub/sub ....  </p>"},{"location":"ServerAdministration/DatabaseServer/NoSQL/Redis/ban-va-toi-hieu-gi-ve-redis/#2-redis-lam-uoc-nhung-gi","title":"2. Redis l\u00e0m \u0111\u01b0\u1ee3c nh\u1eefng g\u00ec?","text":"<p>Nh\u1eafc \u0111\u1ebfn Redis s\u1ebd kh\u00f4ng \u00edt ng\u01b0\u1eddi hi\u1ec3u nh\u1ea7m r\u1eb1ng Redis ch\u1ec9 d\u00f9ng \u0111\u1ec3 l\u01b0u cache. Tuy nhi\u00ean Redis c\u00f3 th\u1ec3 l\u00e0m \u0111\u01b0\u1ee3c nhi\u1ec1u h\u01a1n th\u1ebf. Ch\u00fang ta h\u00e3y c\u00f9ng xem \u0111\u00f3 l\u00e0 g\u00ec nh\u00e9!</p>"},{"location":"ServerAdministration/DatabaseServer/NoSQL/Redis/ban-va-toi-hieu-gi-ve-redis/#21-caching","title":"2.1. Caching","text":"<p>\u0110\u1eb7c \u0111i\u1ec3m c\u1ee7a Redis l\u00e0 d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c l\u01b0u trong Ram, do \u0111\u00f3 d\u1eef li\u1ec7u s\u1ebd b\u1ecb m\u1ea5t n\u1ebfu service Redis b\u1ecb l\u1ed7i, hay khi server b\u1ecb restart. N\u00ean c\u00f3 nhi\u1ec1u l\u1eadp tr\u00ecnh vi\u00ean s\u1eed d\u1ee5ng Redis nh\u01b0 b\u1ed9t b\u1ed9 nh\u1edb Cache, nh\u1eb1m gi\u1ea3m \u0111\u1ed9 tr\u1ec5 truy c\u1eadp/x\u1eed l\u00fd d\u1eef li\u1ec7u, gi\u1ea3m t\u1ea3i cho server/service kh\u00f4ng ph\u1ea3i x\u1eed l\u00fd \u0111i x\u1eed l\u00fd l\u1ea1i c\u00e1c y\u00eau c\u1ea7u m\u1ea5t kh\u00e1 nhi\u1ec1u th\u1eddi gian \u0111\u1ec3 t\u00ednh to\u00e1n.  </p> <p>C\u0169ng c\u00f3 th\u1ec3 n\u00f3i \u0111\u00e2y l\u00e0 t\u00ednh n\u0103ng ph\u1ed5 bi\u1ebfn v\u00e0 chu\u1ed9ng nhi\u1ec1u ng\u01b0\u1eddi d\u00f9ng nh\u1ea5t hi\u1ec7n nay.  </p>"},{"location":"ServerAdministration/DatabaseServer/NoSQL/Redis/ban-va-toi-hieu-gi-ve-redis/#22-chat-messaging-va-queue","title":"2.2. Chat, messaging v\u00e0 queue","text":"<p>H\u1ed7 tr\u1ee3 c\u00e1c \u1ee9ng d\u1ee5ng c\u00f3 t\u00ednh n\u0103ng Pub/Sub (Publish/Subscribe), c\u00e1c \u1ee9ng d\u1ee5ng chatbot, tr\u00f2 chuy\u1ec7n tr\u1ef1c ti\u1ebfp, real-time comment streaming. Gi\u00fap l\u00e0m gi\u1ea3m \u0111\u1ed9 tr\u1ec5 t\u01b0\u01a1ng t\u00e1c gi\u1eefa c\u00e1c ng\u01b0\u1eddi d\u00f9ng.  </p> <p>T\u1ea1o h\u00e0ng \u0111\u1ee3i \u0111\u1ec3 x\u1eed l\u00fd l\u1ea7n l\u01b0\u1ee3t c\u00e1c request. Redis cho ph\u00e9p l\u01b0u tr\u1eef theo list v\u00e0 cung c\u1ea5p r\u1ea5t nhi\u1ec1u thao t\u00e1c v\u1edbi c\u00e1c ph\u1ea7n t\u1eed trong list, v\u00ec v\u1eady n\u00f3 c\u00f2n \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng nh\u01b0 m\u1ed9t message queue.  </p>"},{"location":"ServerAdministration/DatabaseServer/NoSQL/Redis/ban-va-toi-hieu-gi-ve-redis/#23-session","title":"2.3. Session","text":"<p>Gi\u00fap x\u1eed l\u00fd v\u00e0 l\u01b0u tr\u1eef h\u00e0ng ngh\u00ecn, h\u1ea1ng tri\u1ec7u Session c\u1ee7a ng\u01b0\u1eddi d\u00f9ng ch\u1eb3ng h\u1ea1n nh\u01b0 h\u1ed3 s\u01a1 ng\u01b0\u1eddi d\u00f9ng, th\u00f4ng tin x\u00e1c th\u1ef1c \u0111\u0103ng nh\u1eadp, tr\u1ea1ng th\u00e1i phi\u00ean...  </p>"},{"location":"ServerAdministration/DatabaseServer/NoSQL/Redis/ban-va-toi-hieu-gi-ve-redis/#24-counter","title":"2.4. Counter","text":"<p>S\u1eed d\u1ee5ng l\u00e0m b\u1ed9 \u0111\u1ebfm. V\u1edbi thu\u1ed9c t\u00ednh t\u0103ng gi\u1ea3m th\u00f4ng s\u1ed1 r\u1ea5t nhanh trong khi d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c l\u01b0u tr\u00ean RAM, sets v\u00e0 sorted sets \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng th\u1ef1c hi\u1ec7n \u0111\u1ebfm l\u01b0\u1ee3t view c\u1ee7a m\u1ed9t website, c\u00e1c b\u1ea3ng x\u1ebfp h\u1ea1ng trong game.</p>"},{"location":"ServerAdministration/DatabaseServer/NoSQL/Redis/ban-va-toi-hieu-gi-ve-redis/#25-real-time-analytics","title":"2.5. Real-time analytics","text":"<p>Redis l\u00e0 l\u1ef1a ch\u1ecdn l\u00fd t\u01b0\u1edfng cho c\u00e1c tr\u01b0\u1eddng h\u1ee3p s\u1eed d\u1ee5ng ph\u00e2n t\u00edch th\u1eddi gian th\u1ef1c nh\u01b0 ph\u00e2n t\u00edch ph\u01b0\u01a1ng ti\u1ec7n truy\u1ec1n th\u00f4ng x\u00e3 h\u1ed9i, m\u1ee5c ti\u00eau qu\u1ea3ng c\u00e1o, c\u00e1 nh\u00e2n h\u00f3a v\u00e0 IoT.  </p>"},{"location":"ServerAdministration/DatabaseServer/NoSQL/Redis/ban-va-toi-hieu-gi-ve-redis/#3-cac-kieu-du-lieu-ma-redis-co-the-ho-tro","title":"3. C\u00e1c ki\u1ec3u d\u1eef li\u1ec7u m\u00e0 Redis c\u00f3 th\u1ec3 h\u1ed7 tr\u1ee3.","text":""},{"location":"ServerAdministration/DatabaseServer/NoSQL/Redis/ban-va-toi-hieu-gi-ve-redis/#4-redis-hoat-ong-nhu-the-nao","title":"4. Redis ho\u1ea1t \u0111\u1ed9ng nh\u01b0 th\u1ebf n\u00e0o?","text":"<p>\u1ede \u0111\u00e2y m\u00ecnh s\u1ebd n\u00f3i v\u1ec1 c\u00e1ch ho\u1ea1t \u0111\u1ed9ng c\u1ee7a Redis cache \u0111\u1ec3 m\u1ecdi ng\u01b0\u1eddi c\u00f3 th\u1ec3 d\u1ec5 d\u00e0ng h\u00ecnh dung.  </p> <p>Tr\u01b0\u1eddng h\u1ee3p Redis \u0111\u00e3 l\u01b0u d\u1eef li\u1ec7u ( Hit Cache): Ng\u01b0\u1eddi d\u00f9ng g\u1eedi y\u00eau c\u1ea7u &gt; Redis ki\u1ec3m tra y\u00eau c\u1ea7u &gt; Yes &gt; tr\u1ea3 d\u1eef li\u1ec7u cho ng\u01b0\u1eddi d\u00f9ng. </p> <p>Tr\u01b0\u1eddng h\u1ee3p Redis ch\u01b0a l\u01b0u d\u1eef li\u1ec7u ( Miss Cache): Ng\u01b0\u1eddi d\u00f9ng g\u1eedi y\u00eau c\u1ea7u &gt; Redis ki\u1ec3m tra y\u00eau c\u1ea7u &gt; No &gt; Request \u0111\u1ebfn Database Mysql &gt; Tr\u1ea3 d\u1eef li\u1ec7u cho ng\u01b0\u1eddi d\u00f9ng &gt; Set Cache v\u00e0o Redis.</p> <p></p>"},{"location":"ServerAdministration/DatabaseServer/NoSQL/Redis/ban-va-toi-hieu-gi-ve-redis/#5-redis-persistence-la-gi","title":"5. Redis Persistence l\u00e0 g\u00ec?","text":"<p>B\u1ea1n h\u00e3y th\u1eed t\u01b0\u1ee3ng t\u01b0\u1ee3ng \u0111i\u1ec1u g\u00ec x\u1ea3y ra n\u1ebfu to\u00e0n b\u1ed9 d\u1eef li\u1ec7u quan tr\u1ecdng (m\u1ea5t nhi\u1ec1u th\u1eddi gian compute m\u1edbi c\u00f3 \u0111\u01b0\u1ee3c) b\u1ecb m\u1ea5t s\u1ea1ch sau 1 l\u1ea7n g\u1eb7p s\u1ef1 c\u1ed1 m\u00e0 kh\u00f4ng th\u1ec3 kh\u00f4i ph\u1ee5c \u0111\u01b0\u1ee3c. V\u1edbi Redis th\u00ec \u0111i\u1ec1u n\u00e0y \u0111\u00e3 \u0111\u01b0\u1ee3c l\u01b0\u1eddng tr\u01b0\u1edbc, v\u00e0 Redis Persistence ch\u00ednh l\u00e0 ph\u01b0\u01a1ng ph\u00e1p gi\u00fap ch\u00fang ta kh\u1eafc ph\u1ee5c v\u1ea5n \u0111\u1ec1 n\u00e0y.  </p> <p>Redis Persistence l\u00e0 kh\u1ea3 n\u0103ng l\u01b0u tr\u1eef d\u1eef li\u1ec7u tr\u00ean Disk \u0111\u1ec3 \u0111\u1ea3m b\u1ea3o t\u00ednh b\u1ec1n v\u1eefng (durability) c\u1ee7a Redis. Redis cung c\u1ea5p hai c\u01a1 ch\u1ebf Persistence: RDB (Redis Database) v\u00e0 AOF (Append-Only File).  </p> <ul> <li> <p>RDB l\u00e0 qu\u00e1 tr\u00ecnh t\u1ea1o ra m\u1ed9t b\u1ea3n sao c\u1ee7a d\u1eef li\u1ec7u Redis (snapshot) trong m\u1ed9t th\u1eddi \u0111i\u1ec3m c\u1ee5 th\u1ec3 \u0111\u01b0\u1ee3c c\u1ea5u h\u00ecnh v\u00e0 s\u1ebd l\u01b0u data v\u00e0o Disk. </p> </li> <li> <p>AOF l\u01b0u l\u1ea1i t\u1ea5t c\u1ea3 c\u00e1c ho\u1ea1t \u0111\u1ed9ng Write v\u00e0o Redis, t\u1ea1o ra m\u1ed9t file d\u1ea1ng log \u0111\u1ec3 ph\u1ee5c h\u1ed3i d\u1eef li\u1ec7u sau khi kh\u1edfi \u0111\u1ed9ng l\u1ea1i h\u1ec7 th\u1ed1ng. </p> </li> <li> <p>RDB + AOF: K\u1ebft h\u1ee3p c\u1ea3 2 options tr\u00ean. Nh\u01b0ng khi Redis restart th\u00ec AOF s\u1ebd \u0111\u01b0\u1ee3c \u01b0u ti\u00ean s\u1eed d\u1ee5ng \u0111\u1ec3 init data. </p> </li> </ul> <p>B\u1eb1ng c\u00e1ch s\u1eed d\u1ee5ng Redis Persistence, b\u1ea1n c\u00f3 th\u1ec3 kh\u00f4i ph\u1ee5c d\u1eef li\u1ec7u sau m\u1ed9t s\u1ef1 c\u1ed1 v\u00e0 \u0111\u1ea3m b\u1ea3o r\u1eb1ng d\u1eef li\u1ec7u kh\u00f4ng b\u1ecb m\u1ea5t.</p> <p></p> <p>## 6. Redis Replication l\u00e0 g\u00ec? </p> <p>Redis Replication l\u00e0 qu\u00e1 tr\u00ecnh sao ch\u00e9p d\u1eef li\u1ec7u t\u1eeb m\u1ed9t Redis master (ch\u1ee7) sang nhi\u1ec1u Redis slave (ph\u1ee5). Khi c\u00f3 s\u1ef1 thay \u0111\u1ed5i d\u1eef li\u1ec7u tr\u00ean Redis master, n\u00f3 s\u1ebd \u0111\u01b0\u1ee3c chuy\u1ec3n ti\u1ebfp \u0111\u1ebfn t\u1ea5t c\u1ea3 c\u00e1c Redis slave \u0111\u1ec3 \u0111\u1ea3m b\u1ea3o d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c \u0111\u1ed3ng b\u1ed9 tr\u00ean to\u00e0n b\u1ed9 h\u1ec7 th\u1ed1ng. Redis Replication cung c\u1ea5p kh\u1ea3 n\u0103ng ch\u1ecbu l\u1ed7i (fault-tolerance) v\u00e0 t\u0103ng c\u01b0\u1eddng hi\u1ec7u su\u1ea5t b\u1eb1ng vi\u1ec7c cho ph\u00e9p \u0111\u1ecdc d\u1eef li\u1ec7u t\u1eeb c\u00e1c Redis slave.  </p> <p></p> <p>## 7. Redis Sentinel l\u00e0 g\u00ec? </p> <p>Redis Sentinel l\u00e0 m\u1ed9t c\u00f4ng c\u1ee5 gi\u00e1m s\u00e1t v\u00e0 qu\u1ea3n l\u00fd h\u1ec7 th\u1ed1ng Redis. N\u00f3 gi\u00fap theo d\u00f5i s\u1ef1 ho\u1ea1t \u0111\u1ed9ng c\u1ee7a c\u00e1c Redis master v\u00e0 slave, v\u00e0 t\u1ef1 \u0111\u1ed9ng th\u1ef1c hi\u1ec7n qu\u00e1 tr\u00ecnh failover (chuy\u1ec3n giao) khi m\u1ed9t Redis master kh\u00f4ng ho\u1ea1t \u0111\u1ed9ng. Redis Sentinel gi\u00fap \u0111\u1ea3m b\u1ea3o t\u00ednh s\u1eb5n s\u00e0ng v\u00e0 \u1ed5n \u0111\u1ecbnh c\u1ee7a h\u1ec7 th\u1ed1ng Redis b\u1eb1ng c\u00e1ch t\u1ef1 \u0111\u1ed9ng ch\u1ecdn m\u1ed9t Redis slave m\u1edbi \u0111\u1ec3 thay th\u1ebf Redis master g\u1ed1c.  </p> <p></p> <p>## 8. Redis Cluster l\u00e0 g\u00ec? </p> <p>Redis Cluster l\u00e0 m\u1ed9t c\u00e1ch \u0111\u1ec3 chia nh\u1ecf d\u1eef li\u1ec7u v\u00e0 ph\u00e2n chia n\u00f3 tr\u00ean nhi\u1ec1u n\u00fat Redis \u0111\u1ec3 t\u1ea1o th\u00e0nh m\u1ed9t c\u1ee5m (cluster). C\u1ee5m Redis Cluster \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng \u0111\u1ec3 cung c\u1ea5p kh\u1ea3 n\u0103ng m\u1edf r\u1ed9ng ngang (horizontal scalability) v\u00e0 t\u00ednh s\u1eb5n s\u00e0ng cao. D\u1eef li\u1ec7u \u0111\u01b0\u1ee3c ph\u00e2n ph\u1ed1i tr\u00ean c\u00e1c n\u00fat c\u1ee7a c\u1ee5m Redis Cluster v\u00e0 c\u00e1c n\u00fat n\u00e0y ho\u1ea1t \u0111\u1ed9ng \u0111\u1ed9c l\u1eadp nh\u01b0ng v\u1eabn h\u1ee3p t\u00e1c \u0111\u1ec3 \u0111\u1ea3m b\u1ea3o t\u00ednh nh\u1ea5t qu\u00e1n v\u00e0 hi\u1ec7u su\u1ea5t cao. Redis Cluster cung c\u1ea5p kh\u1ea3 n\u0103ng ch\u1ecbu l\u1ed7i v\u00e0 m\u1edf r\u1ed9ng t\u1ef1 \u0111\u1ed9ng khi th\u00eam ho\u1eb7c x\u00f3a c\u00e1c n\u00fat trong c\u1ee5m.</p> <p></p> <p>## 9. C\u00e1ch c\u00e0i \u0111\u1eb7t v\u00e0 c\u1ea5u h\u00ecnh m\u1ed9t Redis server tr\u00ean Server Linux. </p>"}]}